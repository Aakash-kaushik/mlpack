\doxysection{Recurrent\+Attention$<$ Input\+Data\+Type, Output\+Data\+Type $>$ Class Template Reference}
\label{classmlpack_1_1ann_1_1RecurrentAttention}\index{RecurrentAttention$<$ InputDataType, OutputDataType $>$@{RecurrentAttention$<$ InputDataType, OutputDataType $>$}}


This class implements the \doxyref{Recurrent}{p.}{classmlpack_1_1ann_1_1Recurrent} Model for Visual Attention, using a variety of possible layer implementations.  


\doxysubsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
\textbf{ Recurrent\+Attention} ()
\begin{DoxyCompactList}\small\item\em Default constructor\+: this will not give a usable \doxyref{Recurrent\+Attention}{p.}{classmlpack_1_1ann_1_1RecurrentAttention} object, so be sure to set all the parameters before use. \end{DoxyCompactList}\item 
{\footnotesize template$<$typename RNNModule\+Type , typename Action\+Module\+Type $>$ }\\\textbf{ Recurrent\+Attention} (const size\+\_\+t out\+Size, const RNNModule\+Type \&rnn, const Action\+Module\+Type \&action, const size\+\_\+t rho)
\begin{DoxyCompactList}\small\item\em Create the \doxyref{Recurrent\+Attention}{p.}{classmlpack_1_1ann_1_1RecurrentAttention} object using the specified modules. \end{DoxyCompactList}\item 
{\footnotesize template$<$typename eT $>$ }\\void \textbf{ Backward} (const arma\+::\+Mat$<$ eT $>$ \&, const arma\+::\+Mat$<$ eT $>$ \&gy, arma\+::\+Mat$<$ eT $>$ \&g)
\begin{DoxyCompactList}\small\item\em Ordinary feed backward pass of a neural network, calculating the function f(x) by propagating x backwards trough f. \end{DoxyCompactList}\item 
Output\+Data\+Type \& \textbf{ Delta} ()
\begin{DoxyCompactList}\small\item\em Modify the delta. \end{DoxyCompactList}\item 
Output\+Data\+Type const  \& \textbf{ Delta} () const
\begin{DoxyCompactList}\small\item\em Get the delta. \end{DoxyCompactList}\item 
bool \& \textbf{ Deterministic} ()
\begin{DoxyCompactList}\small\item\em Modify the value of the deterministic parameter. \end{DoxyCompactList}\item 
bool \textbf{ Deterministic} () const
\begin{DoxyCompactList}\small\item\em The value of the deterministic parameter. \end{DoxyCompactList}\item 
{\footnotesize template$<$typename eT $>$ }\\void \textbf{ Forward} (const arma\+::\+Mat$<$ eT $>$ \&input, arma\+::\+Mat$<$ eT $>$ \&output)
\begin{DoxyCompactList}\small\item\em Ordinary feed forward pass of a neural network, evaluating the function f(x) by propagating the activity forward through f. \end{DoxyCompactList}\item 
Output\+Data\+Type \& \textbf{ Gradient} ()
\begin{DoxyCompactList}\small\item\em Modify the gradient. \end{DoxyCompactList}\item 
Output\+Data\+Type const  \& \textbf{ Gradient} () const
\begin{DoxyCompactList}\small\item\em Get the gradient. \end{DoxyCompactList}\item 
{\footnotesize template$<$typename eT $>$ }\\void \textbf{ Gradient} (const arma\+::\+Mat$<$ eT $>$ \&, const arma\+::\+Mat$<$ eT $>$ \&, arma\+::\+Mat$<$ eT $>$ \&)
\item 
std\+::vector$<$ \textbf{ Layer\+Types}$<$$>$ $>$ \& \textbf{ Model} ()
\begin{DoxyCompactList}\small\item\em Get the model modules. \end{DoxyCompactList}\item 
Output\+Data\+Type \& \textbf{ Output\+Parameter} ()
\begin{DoxyCompactList}\small\item\em Modify the output parameter. \end{DoxyCompactList}\item 
Output\+Data\+Type const  \& \textbf{ Output\+Parameter} () const
\begin{DoxyCompactList}\small\item\em Get the output parameter. \end{DoxyCompactList}\item 
size\+\_\+t \textbf{ Out\+Size} () const
\begin{DoxyCompactList}\small\item\em Get the module output size. \end{DoxyCompactList}\item 
Output\+Data\+Type \& \textbf{ Parameters} ()
\begin{DoxyCompactList}\small\item\em Modify the parameters. \end{DoxyCompactList}\item 
Output\+Data\+Type const  \& \textbf{ Parameters} () const
\begin{DoxyCompactList}\small\item\em Get the parameters. \end{DoxyCompactList}\item 
size\+\_\+t const  \& \textbf{ Rho} () const
\begin{DoxyCompactList}\small\item\em Get the number of steps to backpropagate through time. \end{DoxyCompactList}\item 
{\footnotesize template$<$typename Archive $>$ }\\void \textbf{ serialize} (Archive \&ar, const uint32\+\_\+t)
\begin{DoxyCompactList}\small\item\em Serialize the layer. \end{DoxyCompactList}\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
\subsubsection*{template$<$typename Input\+Data\+Type = arma\+::mat, typename Output\+Data\+Type = arma\+::mat$>$\newline
class mlpack\+::ann\+::\+Recurrent\+Attention$<$ Input\+Data\+Type, Output\+Data\+Type $>$}

This class implements the \doxyref{Recurrent}{p.}{classmlpack_1_1ann_1_1Recurrent} Model for Visual Attention, using a variety of possible layer implementations. 

For more information, see the following paper.


\begin{DoxyCode}{0}
\DoxyCodeLine{@article\{MnihHGK14,}
\DoxyCodeLine{  title   = \{Recurrent Models of Visual Attention\},}
\DoxyCodeLine{  author  = \{Volodymyr Mnih, Nicolas Heess, Alex Graves, Koray Kavukcuoglu\},}
\DoxyCodeLine{  journal = \{CoRR\},}
\DoxyCodeLine{  volume  = \{abs/1406.6247\},}
\DoxyCodeLine{  year    = \{2014\},}
\DoxyCodeLine{  url     = \{https:\textcolor{comment}{//arxiv.org/abs/1406.6247\}}}
\DoxyCodeLine{\}}

\end{DoxyCode}



\begin{DoxyTemplParams}{Template Parameters}
{\em Input\+Data\+Type} & Type of the input data (arma\+::colvec, arma\+::mat, arma\+::sp\+\_\+mat or arma\+::cube). \\
\hline
{\em Output\+Data\+Type} & Type of the output data (arma\+::colvec, arma\+::mat, arma\+::sp\+\_\+mat or arma\+::cube). \\
\hline
\end{DoxyTemplParams}


Definition at line 56 of file recurrent\+\_\+attention.\+hpp.



\doxysubsection{Constructor \& Destructor Documentation}
\mbox{\label{classmlpack_1_1ann_1_1RecurrentAttention_a22ed8f5cdffdce22f47f8f3768e19a47}} 
\index{RecurrentAttention$<$ InputDataType, OutputDataType $>$@{RecurrentAttention$<$ InputDataType, OutputDataType $>$}!RecurrentAttention@{RecurrentAttention}}
\index{RecurrentAttention@{RecurrentAttention}!RecurrentAttention$<$ InputDataType, OutputDataType $>$@{RecurrentAttention$<$ InputDataType, OutputDataType $>$}}
\doxysubsubsection{RecurrentAttention()\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily \textbf{ Recurrent\+Attention} (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})}



Default constructor\+: this will not give a usable \doxyref{Recurrent\+Attention}{p.}{classmlpack_1_1ann_1_1RecurrentAttention} object, so be sure to set all the parameters before use. 

\mbox{\label{classmlpack_1_1ann_1_1RecurrentAttention_a8cc232af21a0e198cc6dc88366d86f02}} 
\index{RecurrentAttention$<$ InputDataType, OutputDataType $>$@{RecurrentAttention$<$ InputDataType, OutputDataType $>$}!RecurrentAttention@{RecurrentAttention}}
\index{RecurrentAttention@{RecurrentAttention}!RecurrentAttention$<$ InputDataType, OutputDataType $>$@{RecurrentAttention$<$ InputDataType, OutputDataType $>$}}
\doxysubsubsection{RecurrentAttention()\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily \textbf{ Recurrent\+Attention} (\begin{DoxyParamCaption}\item[{const size\+\_\+t}]{out\+Size,  }\item[{const RNNModule\+Type \&}]{rnn,  }\item[{const Action\+Module\+Type \&}]{action,  }\item[{const size\+\_\+t}]{rho }\end{DoxyParamCaption})}



Create the \doxyref{Recurrent\+Attention}{p.}{classmlpack_1_1ann_1_1RecurrentAttention} object using the specified modules. 


\begin{DoxyParams}{Parameters}
{\em out\+Size} & The module output size. \\
\hline
{\em rnn} & The recurrent neural network module. \\
\hline
{\em action} & The action module. \\
\hline
{\em rho} & Maximum number of steps to backpropagate through time (BPTT). \\
\hline
\end{DoxyParams}


\doxysubsection{Member Function Documentation}
\mbox{\label{classmlpack_1_1ann_1_1RecurrentAttention_ad9ad1a3bdb0f3fff5c839ed155e4bbf8}} 
\index{RecurrentAttention$<$ InputDataType, OutputDataType $>$@{RecurrentAttention$<$ InputDataType, OutputDataType $>$}!Backward@{Backward}}
\index{Backward@{Backward}!RecurrentAttention$<$ InputDataType, OutputDataType $>$@{RecurrentAttention$<$ InputDataType, OutputDataType $>$}}
\doxysubsubsection{Backward()}
{\footnotesize\ttfamily void Backward (\begin{DoxyParamCaption}\item[{const arma\+::\+Mat$<$ eT $>$ \&}]{,  }\item[{const arma\+::\+Mat$<$ eT $>$ \&}]{gy,  }\item[{arma\+::\+Mat$<$ eT $>$ \&}]{g }\end{DoxyParamCaption})}



Ordinary feed backward pass of a neural network, calculating the function f(x) by propagating x backwards trough f. 

Using the results from the feed forward pass.


\begin{DoxyParams}{Parameters}
{\em $\ast$} & (input) The propagated input activation. \\
\hline
{\em gy} & The backpropagated error. \\
\hline
{\em g} & The calculated gradient. \\
\hline
\end{DoxyParams}
\mbox{\label{classmlpack_1_1ann_1_1RecurrentAttention_ad6601342d560219ce951d554e69e5e87}} 
\index{RecurrentAttention$<$ InputDataType, OutputDataType $>$@{RecurrentAttention$<$ InputDataType, OutputDataType $>$}!Delta@{Delta}}
\index{Delta@{Delta}!RecurrentAttention$<$ InputDataType, OutputDataType $>$@{RecurrentAttention$<$ InputDataType, OutputDataType $>$}}
\doxysubsubsection{Delta()\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily Output\+Data\+Type\& Delta (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Modify the delta. 



Definition at line 136 of file recurrent\+\_\+attention.\+hpp.

\mbox{\label{classmlpack_1_1ann_1_1RecurrentAttention_a797f7edb44dd081e5e2b3cc316eef6bd}} 
\index{RecurrentAttention$<$ InputDataType, OutputDataType $>$@{RecurrentAttention$<$ InputDataType, OutputDataType $>$}!Delta@{Delta}}
\index{Delta@{Delta}!RecurrentAttention$<$ InputDataType, OutputDataType $>$@{RecurrentAttention$<$ InputDataType, OutputDataType $>$}}
\doxysubsubsection{Delta()\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily Output\+Data\+Type const\& Delta (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const\hspace{0.3cm}{\ttfamily [inline]}}



Get the delta. 



Definition at line 134 of file recurrent\+\_\+attention.\+hpp.

\mbox{\label{classmlpack_1_1ann_1_1RecurrentAttention_a42d4ee3da432cff20d3a41b8b1ec801c}} 
\index{RecurrentAttention$<$ InputDataType, OutputDataType $>$@{RecurrentAttention$<$ InputDataType, OutputDataType $>$}!Deterministic@{Deterministic}}
\index{Deterministic@{Deterministic}!RecurrentAttention$<$ InputDataType, OutputDataType $>$@{RecurrentAttention$<$ InputDataType, OutputDataType $>$}}
\doxysubsubsection{Deterministic()\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily bool\& Deterministic (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Modify the value of the deterministic parameter. 



Definition at line 121 of file recurrent\+\_\+attention.\+hpp.

\mbox{\label{classmlpack_1_1ann_1_1RecurrentAttention_a9f4103707f4d199ce5594d239b60443e}} 
\index{RecurrentAttention$<$ InputDataType, OutputDataType $>$@{RecurrentAttention$<$ InputDataType, OutputDataType $>$}!Deterministic@{Deterministic}}
\index{Deterministic@{Deterministic}!RecurrentAttention$<$ InputDataType, OutputDataType $>$@{RecurrentAttention$<$ InputDataType, OutputDataType $>$}}
\doxysubsubsection{Deterministic()\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily bool Deterministic (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const\hspace{0.3cm}{\ttfamily [inline]}}



The value of the deterministic parameter. 



Definition at line 119 of file recurrent\+\_\+attention.\+hpp.

\mbox{\label{classmlpack_1_1ann_1_1RecurrentAttention_a461f849bc638c15bec262dc9c3a58abe}} 
\index{RecurrentAttention$<$ InputDataType, OutputDataType $>$@{RecurrentAttention$<$ InputDataType, OutputDataType $>$}!Forward@{Forward}}
\index{Forward@{Forward}!RecurrentAttention$<$ InputDataType, OutputDataType $>$@{RecurrentAttention$<$ InputDataType, OutputDataType $>$}}
\doxysubsubsection{Forward()}
{\footnotesize\ttfamily void Forward (\begin{DoxyParamCaption}\item[{const arma\+::\+Mat$<$ eT $>$ \&}]{input,  }\item[{arma\+::\+Mat$<$ eT $>$ \&}]{output }\end{DoxyParamCaption})}



Ordinary feed forward pass of a neural network, evaluating the function f(x) by propagating the activity forward through f. 


\begin{DoxyParams}{Parameters}
{\em input} & Input data used for evaluating the specified function. \\
\hline
{\em output} & Resulting output activation. \\
\hline
\end{DoxyParams}
\mbox{\label{classmlpack_1_1ann_1_1RecurrentAttention_a19abce4739c3b0b658b612537e21956a}} 
\index{RecurrentAttention$<$ InputDataType, OutputDataType $>$@{RecurrentAttention$<$ InputDataType, OutputDataType $>$}!Gradient@{Gradient}}
\index{Gradient@{Gradient}!RecurrentAttention$<$ InputDataType, OutputDataType $>$@{RecurrentAttention$<$ InputDataType, OutputDataType $>$}}
\doxysubsubsection{Gradient()\hspace{0.1cm}{\footnotesize\ttfamily [1/3]}}
{\footnotesize\ttfamily Output\+Data\+Type\& Gradient (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Modify the gradient. 



Definition at line 141 of file recurrent\+\_\+attention.\+hpp.

\mbox{\label{classmlpack_1_1ann_1_1RecurrentAttention_a0f1f4e6d93472d83852731a96c8c3f59}} 
\index{RecurrentAttention$<$ InputDataType, OutputDataType $>$@{RecurrentAttention$<$ InputDataType, OutputDataType $>$}!Gradient@{Gradient}}
\index{Gradient@{Gradient}!RecurrentAttention$<$ InputDataType, OutputDataType $>$@{RecurrentAttention$<$ InputDataType, OutputDataType $>$}}
\doxysubsubsection{Gradient()\hspace{0.1cm}{\footnotesize\ttfamily [2/3]}}
{\footnotesize\ttfamily Output\+Data\+Type const\& Gradient (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const\hspace{0.3cm}{\ttfamily [inline]}}



Get the gradient. 



Definition at line 139 of file recurrent\+\_\+attention.\+hpp.

\mbox{\label{classmlpack_1_1ann_1_1RecurrentAttention_acb03ff976cc8ebe5bd829ff21f4a4258}} 
\index{RecurrentAttention$<$ InputDataType, OutputDataType $>$@{RecurrentAttention$<$ InputDataType, OutputDataType $>$}!Gradient@{Gradient}}
\index{Gradient@{Gradient}!RecurrentAttention$<$ InputDataType, OutputDataType $>$@{RecurrentAttention$<$ InputDataType, OutputDataType $>$}}
\doxysubsubsection{Gradient()\hspace{0.1cm}{\footnotesize\ttfamily [3/3]}}
{\footnotesize\ttfamily void Gradient (\begin{DoxyParamCaption}\item[{const arma\+::\+Mat$<$ eT $>$ \&}]{,  }\item[{const arma\+::\+Mat$<$ eT $>$ \&}]{,  }\item[{arma\+::\+Mat$<$ eT $>$ \&}]{ }\end{DoxyParamCaption})}

\mbox{\label{classmlpack_1_1ann_1_1RecurrentAttention_a50c707b4f7c009339d8d661539baf38f}} 
\index{RecurrentAttention$<$ InputDataType, OutputDataType $>$@{RecurrentAttention$<$ InputDataType, OutputDataType $>$}!Model@{Model}}
\index{Model@{Model}!RecurrentAttention$<$ InputDataType, OutputDataType $>$@{RecurrentAttention$<$ InputDataType, OutputDataType $>$}}
\doxysubsubsection{Model()}
{\footnotesize\ttfamily std\+::vector$<$\textbf{ Layer\+Types}$<$$>$ $>$\& Model (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Get the model modules. 



Definition at line 116 of file recurrent\+\_\+attention.\+hpp.

\mbox{\label{classmlpack_1_1ann_1_1RecurrentAttention_a21d5f745f02c709625a4ee0907f004a5}} 
\index{RecurrentAttention$<$ InputDataType, OutputDataType $>$@{RecurrentAttention$<$ InputDataType, OutputDataType $>$}!OutputParameter@{OutputParameter}}
\index{OutputParameter@{OutputParameter}!RecurrentAttention$<$ InputDataType, OutputDataType $>$@{RecurrentAttention$<$ InputDataType, OutputDataType $>$}}
\doxysubsubsection{OutputParameter()\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily Output\+Data\+Type\& Output\+Parameter (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Modify the output parameter. 



Definition at line 131 of file recurrent\+\_\+attention.\+hpp.

\mbox{\label{classmlpack_1_1ann_1_1RecurrentAttention_a0ee21c2a36e5abad1e7a9d5dd00849f9}} 
\index{RecurrentAttention$<$ InputDataType, OutputDataType $>$@{RecurrentAttention$<$ InputDataType, OutputDataType $>$}!OutputParameter@{OutputParameter}}
\index{OutputParameter@{OutputParameter}!RecurrentAttention$<$ InputDataType, OutputDataType $>$@{RecurrentAttention$<$ InputDataType, OutputDataType $>$}}
\doxysubsubsection{OutputParameter()\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily Output\+Data\+Type const\& Output\+Parameter (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const\hspace{0.3cm}{\ttfamily [inline]}}



Get the output parameter. 



Definition at line 129 of file recurrent\+\_\+attention.\+hpp.

\mbox{\label{classmlpack_1_1ann_1_1RecurrentAttention_a366243e673681001c6080d84ea644f6a}} 
\index{RecurrentAttention$<$ InputDataType, OutputDataType $>$@{RecurrentAttention$<$ InputDataType, OutputDataType $>$}!OutSize@{OutSize}}
\index{OutSize@{OutSize}!RecurrentAttention$<$ InputDataType, OutputDataType $>$@{RecurrentAttention$<$ InputDataType, OutputDataType $>$}}
\doxysubsubsection{OutSize()}
{\footnotesize\ttfamily size\+\_\+t Out\+Size (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const\hspace{0.3cm}{\ttfamily [inline]}}



Get the module output size. 



Definition at line 144 of file recurrent\+\_\+attention.\+hpp.

\mbox{\label{classmlpack_1_1ann_1_1RecurrentAttention_a9c5c5900772a689d5a6b59778ec67120}} 
\index{RecurrentAttention$<$ InputDataType, OutputDataType $>$@{RecurrentAttention$<$ InputDataType, OutputDataType $>$}!Parameters@{Parameters}}
\index{Parameters@{Parameters}!RecurrentAttention$<$ InputDataType, OutputDataType $>$@{RecurrentAttention$<$ InputDataType, OutputDataType $>$}}
\doxysubsubsection{Parameters()\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily Output\+Data\+Type\& Parameters (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Modify the parameters. 



Definition at line 126 of file recurrent\+\_\+attention.\+hpp.

\mbox{\label{classmlpack_1_1ann_1_1RecurrentAttention_aa530552c7ef915c952fbacc77b965c90}} 
\index{RecurrentAttention$<$ InputDataType, OutputDataType $>$@{RecurrentAttention$<$ InputDataType, OutputDataType $>$}!Parameters@{Parameters}}
\index{Parameters@{Parameters}!RecurrentAttention$<$ InputDataType, OutputDataType $>$@{RecurrentAttention$<$ InputDataType, OutputDataType $>$}}
\doxysubsubsection{Parameters()\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily Output\+Data\+Type const\& Parameters (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const\hspace{0.3cm}{\ttfamily [inline]}}



Get the parameters. 



Definition at line 124 of file recurrent\+\_\+attention.\+hpp.

\mbox{\label{classmlpack_1_1ann_1_1RecurrentAttention_aae052295a422d4030c3eacfc1255b1b6}} 
\index{RecurrentAttention$<$ InputDataType, OutputDataType $>$@{RecurrentAttention$<$ InputDataType, OutputDataType $>$}!Rho@{Rho}}
\index{Rho@{Rho}!RecurrentAttention$<$ InputDataType, OutputDataType $>$@{RecurrentAttention$<$ InputDataType, OutputDataType $>$}}
\doxysubsubsection{Rho()}
{\footnotesize\ttfamily size\+\_\+t const\& Rho (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const\hspace{0.3cm}{\ttfamily [inline]}}



Get the number of steps to backpropagate through time. 



Definition at line 147 of file recurrent\+\_\+attention.\+hpp.

\mbox{\label{classmlpack_1_1ann_1_1RecurrentAttention_a65cba07328997659bec80b9879b15a51}} 
\index{RecurrentAttention$<$ InputDataType, OutputDataType $>$@{RecurrentAttention$<$ InputDataType, OutputDataType $>$}!serialize@{serialize}}
\index{serialize@{serialize}!RecurrentAttention$<$ InputDataType, OutputDataType $>$@{RecurrentAttention$<$ InputDataType, OutputDataType $>$}}
\doxysubsubsection{serialize()}
{\footnotesize\ttfamily void serialize (\begin{DoxyParamCaption}\item[{Archive \&}]{ar,  }\item[{const uint32\+\_\+t}]{ }\end{DoxyParamCaption})}



Serialize the layer. 



The documentation for this class was generated from the following files\+:\begin{DoxyCompactItemize}
\item 
/home/aakash/mlpack/src/mlpack/methods/ann/layer/\textbf{ layer\+\_\+types.\+hpp}\item 
/home/aakash/mlpack/src/mlpack/methods/ann/layer/\textbf{ recurrent\+\_\+attention.\+hpp}\end{DoxyCompactItemize}
