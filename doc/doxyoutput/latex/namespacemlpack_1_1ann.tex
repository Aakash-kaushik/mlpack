\doxysection{mlpack\+::ann Namespace Reference}
\label{namespacemlpack_1_1ann}\index{mlpack::ann@{mlpack::ann}}


Artificial Neural Network.  


\doxysubsection*{Namespaces}
\begin{DoxyCompactItemize}
\item 
 \textbf{ augmented}
\end{DoxyCompactItemize}
\doxysubsection*{Classes}
\begin{DoxyCompactItemize}
\item 
class \textbf{ Adaptive\+Max\+Pooling}
\begin{DoxyCompactList}\small\item\em Implementation of the \doxyref{Adaptive\+Max\+Pooling}{p.}{classmlpack_1_1ann_1_1AdaptiveMaxPooling} layer. \end{DoxyCompactList}\item 
class \textbf{ Adaptive\+Mean\+Pooling}
\begin{DoxyCompactList}\small\item\em Implementation of the \doxyref{Adaptive\+Mean\+Pooling}{p.}{classmlpack_1_1ann_1_1AdaptiveMeanPooling}. \end{DoxyCompactList}\item 
class \textbf{ Add}
\begin{DoxyCompactList}\small\item\em Implementation of the \doxyref{Add}{p.}{classmlpack_1_1ann_1_1Add} module class. \end{DoxyCompactList}\item 
class \textbf{ Add\+Merge}
\begin{DoxyCompactList}\small\item\em Implementation of the \doxyref{Add\+Merge}{p.}{classmlpack_1_1ann_1_1AddMerge} module class. \end{DoxyCompactList}\item 
class \textbf{ Add\+Visitor}
\begin{DoxyCompactList}\small\item\em \doxyref{Add\+Visitor}{p.}{classmlpack_1_1ann_1_1AddVisitor} exposes the Add() method of the given module. \end{DoxyCompactList}\item 
class \textbf{ Alpha\+Dropout}
\begin{DoxyCompactList}\small\item\em The alpha -\/ dropout layer is a regularizer that randomly with probability \textquotesingle{}ratio\textquotesingle{} sets input values to alpha\+Dash. \end{DoxyCompactList}\item 
class \textbf{ Atrous\+Convolution}
\begin{DoxyCompactList}\small\item\em Implementation of the Atrous \doxyref{Convolution}{p.}{classmlpack_1_1ann_1_1Convolution} class. \end{DoxyCompactList}\item 
class \textbf{ Backward\+Visitor}
\begin{DoxyCompactList}\small\item\em \doxyref{Backward\+Visitor}{p.}{classmlpack_1_1ann_1_1BackwardVisitor} executes the Backward() function given the input, error and delta parameter. \end{DoxyCompactList}\item 
class \textbf{ Base\+Layer}
\begin{DoxyCompactList}\small\item\em Implementation of the base layer. \end{DoxyCompactList}\item 
class \textbf{ Batch\+Norm}
\begin{DoxyCompactList}\small\item\em Declaration of the Batch Normalization layer class. \end{DoxyCompactList}\item 
class \textbf{ BCELoss}
\begin{DoxyCompactList}\small\item\em The binary-\/cross-\/entropy performance function measures the Binary Cross Entropy between the target and the output. \end{DoxyCompactList}\item 
class \textbf{ Bernoulli\+Distribution}
\begin{DoxyCompactList}\small\item\em Multiple independent Bernoulli distributions. \end{DoxyCompactList}\item 
class \textbf{ Bias\+Set\+Visitor}
\begin{DoxyCompactList}\small\item\em \doxyref{Bias\+Set\+Visitor}{p.}{classmlpack_1_1ann_1_1BiasSetVisitor} updates the module bias parameters given the parameters set. \end{DoxyCompactList}\item 
class \textbf{ Bilinear\+Interpolation}
\begin{DoxyCompactList}\small\item\em Definition and Implementation of the Bilinear Interpolation Layer. \end{DoxyCompactList}\item 
class \textbf{ Binary\+RBM}
\begin{DoxyCompactList}\small\item\em For more information, see the following paper\+: \end{DoxyCompactList}\item 
class \textbf{ BRNN}
\begin{DoxyCompactList}\small\item\em Implementation of a standard bidirectional recurrent neural network container. \end{DoxyCompactList}\item 
class \textbf{ CELU}
\begin{DoxyCompactList}\small\item\em The \doxyref{CELU}{p.}{classmlpack_1_1ann_1_1CELU} activation function, defined by. \end{DoxyCompactList}\item 
class \textbf{ Concat}
\begin{DoxyCompactList}\small\item\em Implementation of the \doxyref{Concat}{p.}{classmlpack_1_1ann_1_1Concat} class. \end{DoxyCompactList}\item 
class \textbf{ Concatenate}
\begin{DoxyCompactList}\small\item\em Implementation of the \doxyref{Concatenate}{p.}{classmlpack_1_1ann_1_1Concatenate} module class. \end{DoxyCompactList}\item 
class \textbf{ Concat\+Performance}
\begin{DoxyCompactList}\small\item\em Implementation of the concat performance class. \end{DoxyCompactList}\item 
class \textbf{ Constant}
\begin{DoxyCompactList}\small\item\em Implementation of the constant layer. \end{DoxyCompactList}\item 
class \textbf{ Const\+Initialization}
\begin{DoxyCompactList}\small\item\em This class is used to initialize weight matrix with constant values. \end{DoxyCompactList}\item 
class \textbf{ Convolution}
\begin{DoxyCompactList}\small\item\em Implementation of the \doxyref{Convolution}{p.}{classmlpack_1_1ann_1_1Convolution} class. \end{DoxyCompactList}\item 
class \textbf{ Copy\+Visitor}
\begin{DoxyCompactList}\small\item\em This visitor is to support copy constructor for neural network module. \end{DoxyCompactList}\item 
class \textbf{ Cosine\+Embedding\+Loss}
\begin{DoxyCompactList}\small\item\em Cosine Embedding Loss function is used for measuring whether two inputs are similar or dissimilar, using the cosine distance, and is typically used for learning nonlinear embeddings or semi-\/supervised learning. \end{DoxyCompactList}\item 
class \textbf{ CRe\+LU}
\begin{DoxyCompactList}\small\item\em A concatenated Re\+LU has two outputs, one Re\+LU and one negative Re\+LU, concatenated together. \end{DoxyCompactList}\item 
class \textbf{ DCGAN}
\begin{DoxyCompactList}\small\item\em For more information, see the following paper\+: \end{DoxyCompactList}\item 
class \textbf{ Delete\+Visitor}
\begin{DoxyCompactList}\small\item\em \doxyref{Delete\+Visitor}{p.}{classmlpack_1_1ann_1_1DeleteVisitor} executes the destructor of the instantiated object. \end{DoxyCompactList}\item 
class \textbf{ Delta\+Visitor}
\begin{DoxyCompactList}\small\item\em \doxyref{Delta\+Visitor}{p.}{classmlpack_1_1ann_1_1DeltaVisitor} exposes the delta parameter of the given module. \end{DoxyCompactList}\item 
class \textbf{ Deterministic\+Set\+Visitor}
\begin{DoxyCompactList}\small\item\em \doxyref{Deterministic\+Set\+Visitor}{p.}{classmlpack_1_1ann_1_1DeterministicSetVisitor} set the deterministic parameter given the deterministic value. \end{DoxyCompactList}\item 
class \textbf{ Dice\+Loss}
\begin{DoxyCompactList}\small\item\em The dice loss performance function measures the network\textquotesingle{}s performance according to the dice coefficient between the input and target distributions. \end{DoxyCompactList}\item 
class \textbf{ Drop\+Connect}
\begin{DoxyCompactList}\small\item\em The \doxyref{Drop\+Connect}{p.}{classmlpack_1_1ann_1_1DropConnect} layer is a regularizer that randomly with probability ratio sets the connection values to zero and scales the remaining elements by factor 1 /(1 -\/ ratio). \end{DoxyCompactList}\item 
class \textbf{ Dropout}
\begin{DoxyCompactList}\small\item\em The dropout layer is a regularizer that randomly with probability \textquotesingle{}ratio\textquotesingle{} sets input values to zero and scales the remaining elements by factor 1 / (1 -\/ ratio) rather than during test time so as to keep the expected sum same. \end{DoxyCompactList}\item 
class \textbf{ Earth\+Mover\+Distance}
\begin{DoxyCompactList}\small\item\em The earth mover distance function measures the network\textquotesingle{}s performance according to the Kantorovich-\/\+Rubinstein duality approximation. \end{DoxyCompactList}\item 
class \textbf{ Elish\+Function}
\begin{DoxyCompactList}\small\item\em The ELi\+SH function, defined by. \end{DoxyCompactList}\item 
class \textbf{ Elliot\+Function}
\begin{DoxyCompactList}\small\item\em The Elliot function, defined by. \end{DoxyCompactList}\item 
class \textbf{ ELU}
\begin{DoxyCompactList}\small\item\em The \doxyref{ELU}{p.}{classmlpack_1_1ann_1_1ELU} activation function, defined by. \end{DoxyCompactList}\item 
class \textbf{ Empty\+Loss}
\begin{DoxyCompactList}\small\item\em The empty loss does nothing, letting the user calculate the loss outside the model. \end{DoxyCompactList}\item 
class \textbf{ Fast\+LSTM}
\begin{DoxyCompactList}\small\item\em An implementation of a faster version of the Fast \doxyref{LSTM}{p.}{classmlpack_1_1ann_1_1LSTM} network layer. \end{DoxyCompactList}\item 
class \textbf{ FFN}
\begin{DoxyCompactList}\small\item\em Implementation of a standard feed forward network. \end{DoxyCompactList}\item 
class \textbf{ FFTConvolution}
\begin{DoxyCompactList}\small\item\em Computes the two-\/dimensional convolution through fft. \end{DoxyCompactList}\item 
class \textbf{ Flatten\+TSwish}
\begin{DoxyCompactList}\small\item\em The Flatten T Swish activation function, defined by. \end{DoxyCompactList}\item 
class \textbf{ Flexible\+Re\+LU}
\begin{DoxyCompactList}\small\item\em The \doxyref{Flexible\+Re\+LU}{p.}{classmlpack_1_1ann_1_1FlexibleReLU} activation function, defined by. \end{DoxyCompactList}\item 
class \textbf{ Forward\+Visitor}
\begin{DoxyCompactList}\small\item\em \doxyref{Forward\+Visitor}{p.}{classmlpack_1_1ann_1_1ForwardVisitor} executes the Forward() function given the input and output parameter. \end{DoxyCompactList}\item 
class \textbf{ Full\+Convolution}
\item 
class \textbf{ GAN}
\begin{DoxyCompactList}\small\item\em The implementation of the standard \doxyref{GAN}{p.}{classmlpack_1_1ann_1_1GAN} module. \end{DoxyCompactList}\item 
class \textbf{ Gaussian\+Function}
\begin{DoxyCompactList}\small\item\em The gaussian function, defined by. \end{DoxyCompactList}\item 
class \textbf{ Gaussian\+Initialization}
\begin{DoxyCompactList}\small\item\em This class is used to initialize weigth matrix with a gaussian. \end{DoxyCompactList}\item 
class \textbf{ GELUFunction}
\begin{DoxyCompactList}\small\item\em The GELU function, defined by. \end{DoxyCompactList}\item 
class \textbf{ Glimpse}
\begin{DoxyCompactList}\small\item\em The glimpse layer returns a retina-\/like representation (down-\/scaled cropped images) of increasing scale around a given location in a given image. \end{DoxyCompactList}\item 
class \textbf{ Glorot\+Initialization\+Type}
\begin{DoxyCompactList}\small\item\em This class is used to initialize the weight matrix with the Glorot Initialization method. \end{DoxyCompactList}\item 
class \textbf{ Gradient\+Set\+Visitor}
\begin{DoxyCompactList}\small\item\em \doxyref{Gradient\+Set\+Visitor}{p.}{classmlpack_1_1ann_1_1GradientSetVisitor} update the gradient parameter given the gradient set. \end{DoxyCompactList}\item 
class \textbf{ Gradient\+Update\+Visitor}
\begin{DoxyCompactList}\small\item\em \doxyref{Gradient\+Update\+Visitor}{p.}{classmlpack_1_1ann_1_1GradientUpdateVisitor} update the gradient parameter given the gradient set. \end{DoxyCompactList}\item 
class \textbf{ Gradient\+Visitor}
\begin{DoxyCompactList}\small\item\em Search\+Mode\+Visitor executes the Gradient() method of the given module using the input and delta parameter. \end{DoxyCompactList}\item 
class \textbf{ Gradient\+Zero\+Visitor}
\item 
class \textbf{ GRU}
\begin{DoxyCompactList}\small\item\em An implementation of a gru network layer. \end{DoxyCompactList}\item 
class \textbf{ Hard\+Shrink}
\begin{DoxyCompactList}\small\item\em Hard Shrink operator is defined as,. \end{DoxyCompactList}\item 
class \textbf{ Hard\+Sigmoid\+Function}
\begin{DoxyCompactList}\small\item\em The hard sigmoid function, defined by. \end{DoxyCompactList}\item 
class \textbf{ Hard\+Swish\+Function}
\begin{DoxyCompactList}\small\item\em The Hard Swish function, defined by. \end{DoxyCompactList}\item 
class \textbf{ Hard\+TanH}
\begin{DoxyCompactList}\small\item\em The Hard Tanh activation function, defined by. \end{DoxyCompactList}\item 
class \textbf{ He\+Initialization}
\begin{DoxyCompactList}\small\item\em This class is used to initialize weight matrix with the He initialization rule given by He et. \end{DoxyCompactList}\item 
class \textbf{ Highway}
\begin{DoxyCompactList}\small\item\em Implementation of the \doxyref{Highway}{p.}{classmlpack_1_1ann_1_1Highway} layer. \end{DoxyCompactList}\item 
class \textbf{ Hinge\+Embedding\+Loss}
\begin{DoxyCompactList}\small\item\em The Hinge Embedding loss function is often used to compute the loss between y\+\_\+true and y\+\_\+pred. \end{DoxyCompactList}\item 
class \textbf{ Hinge\+Loss}
\begin{DoxyCompactList}\small\item\em Computes the hinge loss between $y_true$ and $y_pred$. \end{DoxyCompactList}\item 
class \textbf{ Huber\+Loss}
\begin{DoxyCompactList}\small\item\em The Huber loss is a loss function used in robust regression, that is less sensitive to outliers in data than the squared error loss. \end{DoxyCompactList}\item 
class \textbf{ Identity\+Function}
\begin{DoxyCompactList}\small\item\em The identity function, defined by. \end{DoxyCompactList}\item 
class \textbf{ Init\+Traits}
\begin{DoxyCompactList}\small\item\em This is a template class that can provide information about various initialization methods. \end{DoxyCompactList}\item 
class \textbf{ Init\+Traits$<$ Kathirvalavakumar\+Subavathi\+Initialization $>$}
\begin{DoxyCompactList}\small\item\em Initialization traits of the kathirvalavakumar subavath initialization rule. \end{DoxyCompactList}\item 
class \textbf{ Init\+Traits$<$ Nguyen\+Widrow\+Initialization $>$}
\begin{DoxyCompactList}\small\item\em Initialization traits of the Nguyen-\/\+Widrow initialization rule. \end{DoxyCompactList}\item 
class \textbf{ In\+Shape\+Visitor}
\begin{DoxyCompactList}\small\item\em \doxyref{In\+Shape\+Visitor}{p.}{classmlpack_1_1ann_1_1InShapeVisitor} returns the input shape a Layer expects. \end{DoxyCompactList}\item 
class \textbf{ Inv\+Quad\+Function}
\begin{DoxyCompactList}\small\item\em The Inverse Quadratic function, defined by. \end{DoxyCompactList}\item 
class \textbf{ ISRLU}
\begin{DoxyCompactList}\small\item\em The \doxyref{ISRLU}{p.}{classmlpack_1_1ann_1_1ISRLU} activation function, defined by. \end{DoxyCompactList}\item 
class \textbf{ Join}
\begin{DoxyCompactList}\small\item\em Implementation of the \doxyref{Join}{p.}{classmlpack_1_1ann_1_1Join} module class. \end{DoxyCompactList}\item 
class \textbf{ Kathirvalavakumar\+Subavathi\+Initialization}
\begin{DoxyCompactList}\small\item\em This class is used to initialize the weight matrix with the method proposed by T. \end{DoxyCompactList}\item 
class \textbf{ KLDivergence}
\begin{DoxyCompactList}\small\item\em The Kullback–\+Leibler divergence is often used for continuous distributions (direct regression). \end{DoxyCompactList}\item 
class \textbf{ L1\+Loss}
\begin{DoxyCompactList}\small\item\em The L1 loss is a loss function that measures the mean absolute error (MAE) between each element in the input x and target y. \end{DoxyCompactList}\item 
class \textbf{ Layer\+Norm}
\begin{DoxyCompactList}\small\item\em Declaration of the Layer Normalization class. \end{DoxyCompactList}\item 
class \textbf{ Layer\+Traits}
\begin{DoxyCompactList}\small\item\em This is a template class that can provide information about various layers. \end{DoxyCompactList}\item 
class \textbf{ Leaky\+Re\+LU}
\begin{DoxyCompactList}\small\item\em The \doxyref{Leaky\+Re\+LU}{p.}{classmlpack_1_1ann_1_1LeakyReLU} activation function, defined by. \end{DoxyCompactList}\item 
class \textbf{ Lecun\+Normal\+Initialization}
\begin{DoxyCompactList}\small\item\em This class is used to initialize weight matrix with the Lecun Normalization initialization rule. \end{DoxyCompactList}\item 
class \textbf{ Linear}
\begin{DoxyCompactList}\small\item\em Implementation of the \doxyref{Linear}{p.}{classmlpack_1_1ann_1_1Linear} layer class. \end{DoxyCompactList}\item 
class \textbf{ Linear3D}
\begin{DoxyCompactList}\small\item\em Implementation of the \doxyref{Linear3D}{p.}{classmlpack_1_1ann_1_1Linear3D} layer class. \end{DoxyCompactList}\item 
class \textbf{ Linear\+No\+Bias}
\begin{DoxyCompactList}\small\item\em Implementation of the \doxyref{Linear\+No\+Bias}{p.}{classmlpack_1_1ann_1_1LinearNoBias} class. \end{DoxyCompactList}\item 
class \textbf{ Li\+SHTFunction}
\begin{DoxyCompactList}\small\item\em The Li\+SHT function, defined by. \end{DoxyCompactList}\item 
class \textbf{ Load\+Output\+Parameter\+Visitor}
\begin{DoxyCompactList}\small\item\em \doxyref{Load\+Output\+Parameter\+Visitor}{p.}{classmlpack_1_1ann_1_1LoadOutputParameterVisitor} restores the output parameter using the given parameter set. \end{DoxyCompactList}\item 
class \textbf{ Log\+Cosh\+Loss}
\begin{DoxyCompactList}\small\item\em The Log-\/\+Hyperbolic-\/\+Cosine loss function is often used to improve variational auto encoder. \end{DoxyCompactList}\item 
class \textbf{ Logistic\+Function}
\begin{DoxyCompactList}\small\item\em The logistic function, defined by. \end{DoxyCompactList}\item 
class \textbf{ Log\+Soft\+Max}
\begin{DoxyCompactList}\small\item\em Implementation of the log softmax layer. \end{DoxyCompactList}\item 
class \textbf{ Lookup}
\begin{DoxyCompactList}\small\item\em The \doxyref{Lookup}{p.}{classmlpack_1_1ann_1_1Lookup} class stores word embeddings and retrieves them using tokens. \end{DoxyCompactList}\item 
class \textbf{ Loss\+Visitor}
\begin{DoxyCompactList}\small\item\em \doxyref{Loss\+Visitor}{p.}{classmlpack_1_1ann_1_1LossVisitor} exposes the Loss() method of the given module. \end{DoxyCompactList}\item 
class \textbf{ Lp\+Pooling}
\begin{DoxyCompactList}\small\item\em Implementation of the LPPooling. \end{DoxyCompactList}\item 
class \textbf{ LRegularizer}
\begin{DoxyCompactList}\small\item\em The L\+\_\+p regularizer for arbitrary integer p. \end{DoxyCompactList}\item 
class \textbf{ LSTM}
\begin{DoxyCompactList}\small\item\em Implementation of the \doxyref{LSTM}{p.}{classmlpack_1_1ann_1_1LSTM} module class. \end{DoxyCompactList}\item 
class \textbf{ Margin\+Ranking\+Loss}
\begin{DoxyCompactList}\small\item\em Margin ranking loss measures the loss given inputs and a label vector with values of 1 or -\/1. \end{DoxyCompactList}\item 
class \textbf{ Max\+Pooling}
\begin{DoxyCompactList}\small\item\em Implementation of the \doxyref{Max\+Pooling}{p.}{classmlpack_1_1ann_1_1MaxPooling} layer. \end{DoxyCompactList}\item 
class \textbf{ Max\+Pooling\+Rule}
\item 
class \textbf{ Mean\+Absolute\+Percentage\+Error}
\begin{DoxyCompactList}\small\item\em The mean absolute percentage error performance function measures the network\textquotesingle{}s performance according to the mean of the absolute difference between input and target divided by target. \end{DoxyCompactList}\item 
class \textbf{ Mean\+Bias\+Error}
\begin{DoxyCompactList}\small\item\em The mean bias error performance function measures the network\textquotesingle{}s performance according to the mean of errors. \end{DoxyCompactList}\item 
class \textbf{ Mean\+Pooling}
\begin{DoxyCompactList}\small\item\em Implementation of the \doxyref{Mean\+Pooling}{p.}{classmlpack_1_1ann_1_1MeanPooling}. \end{DoxyCompactList}\item 
class \textbf{ Mean\+Pooling\+Rule}
\item 
class \textbf{ Mean\+Squared\+Error}
\begin{DoxyCompactList}\small\item\em The mean squared error performance function measures the network\textquotesingle{}s performance according to the mean of squared errors. \end{DoxyCompactList}\item 
class \textbf{ Mean\+Squared\+Logarithmic\+Error}
\begin{DoxyCompactList}\small\item\em The mean squared logarithmic error performance function measures the network\textquotesingle{}s performance according to the mean of squared logarithmic errors. \end{DoxyCompactList}\item 
class \textbf{ Mini\+Batch\+Discrimination}
\begin{DoxyCompactList}\small\item\em Implementation of the \doxyref{Mini\+Batch\+Discrimination}{p.}{classmlpack_1_1ann_1_1MiniBatchDiscrimination} layer. \end{DoxyCompactList}\item 
class \textbf{ Mish\+Function}
\begin{DoxyCompactList}\small\item\em The Mish function, defined by. \end{DoxyCompactList}\item 
class \textbf{ Multihead\+Attention}
\begin{DoxyCompactList}\small\item\em Multihead Attention allows the model to jointly attend to information from different representation subspaces at different positions. \end{DoxyCompactList}\item 
class \textbf{ Multiply\+Constant}
\begin{DoxyCompactList}\small\item\em Implementation of the multiply constant layer. \end{DoxyCompactList}\item 
class \textbf{ Multiply\+Merge}
\begin{DoxyCompactList}\small\item\em Implementation of the \doxyref{Multiply\+Merge}{p.}{classmlpack_1_1ann_1_1MultiplyMerge} module class. \end{DoxyCompactList}\item 
class \textbf{ Multi\+Quad\+Function}
\begin{DoxyCompactList}\small\item\em The Multi Quadratic function, defined by. \end{DoxyCompactList}\item 
class \textbf{ Naive\+Convolution}
\begin{DoxyCompactList}\small\item\em Computes the two-\/dimensional convolution. \end{DoxyCompactList}\item 
class \textbf{ Negative\+Log\+Likelihood}
\begin{DoxyCompactList}\small\item\em Implementation of the negative log likelihood layer. \end{DoxyCompactList}\item 
class \textbf{ Network\+Initialization}
\begin{DoxyCompactList}\small\item\em This class is used to initialize the network with the given initialization rule. \end{DoxyCompactList}\item 
class \textbf{ Nguyen\+Widrow\+Initialization}
\begin{DoxyCompactList}\small\item\em This class is used to initialize the weight matrix with the Nguyen-\/\+Widrow method. \end{DoxyCompactList}\item 
class \textbf{ Noisy\+Linear}
\begin{DoxyCompactList}\small\item\em Implementation of the \doxyref{Noisy\+Linear}{p.}{classmlpack_1_1ann_1_1NoisyLinear} layer class. \end{DoxyCompactList}\item 
class \textbf{ No\+Regularizer}
\begin{DoxyCompactList}\small\item\em Implementation of the \doxyref{No\+Regularizer}{p.}{classmlpack_1_1ann_1_1NoRegularizer}. \end{DoxyCompactList}\item 
class \textbf{ Normal\+Distribution}
\begin{DoxyCompactList}\small\item\em Implementation of the Normal Distribution function. \end{DoxyCompactList}\item 
class \textbf{ Oivs\+Initialization}
\begin{DoxyCompactList}\small\item\em This class is used to initialize the weight matrix with the oivs method. \end{DoxyCompactList}\item 
class \textbf{ Orthogonal\+Initialization}
\begin{DoxyCompactList}\small\item\em This class is used to initialize the weight matrix with the orthogonal matrix initialization. \end{DoxyCompactList}\item 
class \textbf{ Orthogonal\+Regularizer}
\begin{DoxyCompactList}\small\item\em Implementation of the \doxyref{Orthogonal\+Regularizer}{p.}{classmlpack_1_1ann_1_1OrthogonalRegularizer}. \end{DoxyCompactList}\item 
class \textbf{ Output\+Height\+Visitor}
\begin{DoxyCompactList}\small\item\em \doxyref{Output\+Height\+Visitor}{p.}{classmlpack_1_1ann_1_1OutputHeightVisitor} exposes the Output\+Height() method of the given module. \end{DoxyCompactList}\item 
class \textbf{ Output\+Parameter\+Visitor}
\begin{DoxyCompactList}\small\item\em \doxyref{Output\+Parameter\+Visitor}{p.}{classmlpack_1_1ann_1_1OutputParameterVisitor} exposes the output parameter of the given module. \end{DoxyCompactList}\item 
class \textbf{ Output\+Width\+Visitor}
\begin{DoxyCompactList}\small\item\em \doxyref{Output\+Width\+Visitor}{p.}{classmlpack_1_1ann_1_1OutputWidthVisitor} exposes the Output\+Width() method of the given module. \end{DoxyCompactList}\item 
class \textbf{ Padding}
\begin{DoxyCompactList}\small\item\em Implementation of the \doxyref{Padding}{p.}{classmlpack_1_1ann_1_1Padding} module class. \end{DoxyCompactList}\item 
class \textbf{ Parameters\+Set\+Visitor}
\begin{DoxyCompactList}\small\item\em \doxyref{Parameters\+Set\+Visitor}{p.}{classmlpack_1_1ann_1_1ParametersSetVisitor} update the parameters set using the given matrix. \end{DoxyCompactList}\item 
class \textbf{ Parameters\+Visitor}
\begin{DoxyCompactList}\small\item\em \doxyref{Parameters\+Visitor}{p.}{classmlpack_1_1ann_1_1ParametersVisitor} exposes the parameters set of the given module and stores the parameters set into the given matrix. \end{DoxyCompactList}\item 
class \textbf{ Pixel\+Shuffle}
\begin{DoxyCompactList}\small\item\em Implementation of the \doxyref{Pixel\+Shuffle}{p.}{classmlpack_1_1ann_1_1PixelShuffle} layer. \end{DoxyCompactList}\item 
class \textbf{ Poisson1\+Function}
\begin{DoxyCompactList}\small\item\em The Poisson one function, defined by. \end{DoxyCompactList}\item 
class \textbf{ Poisson\+NLLLoss}
\begin{DoxyCompactList}\small\item\em Implementation of the Poisson negative log likelihood loss. \end{DoxyCompactList}\item 
class \textbf{ Positional\+Encoding}
\begin{DoxyCompactList}\small\item\em Positional Encoding injects some information about the relative or absolute position of the tokens in the sequence. \end{DoxyCompactList}\item 
class \textbf{ PRe\+LU}
\begin{DoxyCompactList}\small\item\em The \doxyref{PRe\+LU}{p.}{classmlpack_1_1ann_1_1PReLU} activation function, defined by (where alpha is trainable) \end{DoxyCompactList}\item 
class \textbf{ Quadratic\+Function}
\begin{DoxyCompactList}\small\item\em The Quadratic function, defined by. \end{DoxyCompactList}\item 
class \textbf{ Random\+Initialization}
\begin{DoxyCompactList}\small\item\em This class is used to initialize randomly the weight matrix. \end{DoxyCompactList}\item 
class \textbf{ RBF}
\begin{DoxyCompactList}\small\item\em Implementation of the Radial Basis Function layer. \end{DoxyCompactList}\item 
class \textbf{ RBM}
\begin{DoxyCompactList}\small\item\em The implementation of the \doxyref{RBM}{p.}{classmlpack_1_1ann_1_1RBM} module. \end{DoxyCompactList}\item 
class \textbf{ Reconstruction\+Loss}
\begin{DoxyCompactList}\small\item\em The reconstruction loss performance function measures the network\textquotesingle{}s performance equal to the negative log probability of the target with the input distribution. \end{DoxyCompactList}\item 
class \textbf{ Rectifier\+Function}
\begin{DoxyCompactList}\small\item\em The rectifier function, defined by. \end{DoxyCompactList}\item 
class \textbf{ Recurrent}
\begin{DoxyCompactList}\small\item\em Implementation of the Recurrent\+Layer class. \end{DoxyCompactList}\item 
class \textbf{ Recurrent\+Attention}
\begin{DoxyCompactList}\small\item\em This class implements the \doxyref{Recurrent}{p.}{classmlpack_1_1ann_1_1Recurrent} Model for Visual Attention, using a variety of possible layer implementations. \end{DoxyCompactList}\item 
class \textbf{ Reinforce\+Normal}
\begin{DoxyCompactList}\small\item\em Implementation of the reinforce normal layer. \end{DoxyCompactList}\item 
class \textbf{ Reparametrization}
\begin{DoxyCompactList}\small\item\em Implementation of the \doxyref{Reparametrization}{p.}{classmlpack_1_1ann_1_1Reparametrization} layer class. \end{DoxyCompactList}\item 
class \textbf{ Reset\+Cell\+Visitor}
\begin{DoxyCompactList}\small\item\em \doxyref{Reset\+Cell\+Visitor}{p.}{classmlpack_1_1ann_1_1ResetCellVisitor} executes the Reset\+Cell() function. \end{DoxyCompactList}\item 
class \textbf{ Reset\+Visitor}
\begin{DoxyCompactList}\small\item\em \doxyref{Reset\+Visitor}{p.}{classmlpack_1_1ann_1_1ResetVisitor} executes the Reset() function. \end{DoxyCompactList}\item 
class \textbf{ Reward\+Set\+Visitor}
\begin{DoxyCompactList}\small\item\em \doxyref{Reward\+Set\+Visitor}{p.}{classmlpack_1_1ann_1_1RewardSetVisitor} set the reward parameter given the reward value. \end{DoxyCompactList}\item 
class \textbf{ RNN}
\begin{DoxyCompactList}\small\item\em Implementation of a standard recurrent neural network container. \end{DoxyCompactList}\item 
class \textbf{ Run\+Set\+Visitor}
\begin{DoxyCompactList}\small\item\em \doxyref{Run\+Set\+Visitor}{p.}{classmlpack_1_1ann_1_1RunSetVisitor} set the run parameter given the run value. \end{DoxyCompactList}\item 
class \textbf{ Save\+Output\+Parameter\+Visitor}
\begin{DoxyCompactList}\small\item\em \doxyref{Save\+Output\+Parameter\+Visitor}{p.}{classmlpack_1_1ann_1_1SaveOutputParameterVisitor} saves the output parameter into the given parameter set. \end{DoxyCompactList}\item 
class \textbf{ Select}
\begin{DoxyCompactList}\small\item\em The select module selects the specified column from a given input matrix. \end{DoxyCompactList}\item 
class \textbf{ Sequential}
\begin{DoxyCompactList}\small\item\em Implementation of the \doxyref{Sequential}{p.}{classmlpack_1_1ann_1_1Sequential} class. \end{DoxyCompactList}\item 
class \textbf{ Set\+Input\+Height\+Visitor}
\begin{DoxyCompactList}\small\item\em \doxyref{Set\+Input\+Height\+Visitor}{p.}{classmlpack_1_1ann_1_1SetInputHeightVisitor} updates the input height parameter with the given input height. \end{DoxyCompactList}\item 
class \textbf{ Set\+Input\+Width\+Visitor}
\begin{DoxyCompactList}\small\item\em \doxyref{Set\+Input\+Width\+Visitor}{p.}{classmlpack_1_1ann_1_1SetInputWidthVisitor} updates the input width parameter with the given input width. \end{DoxyCompactList}\item 
class \textbf{ Sigmoid\+Cross\+Entropy\+Error}
\begin{DoxyCompactList}\small\item\em The \doxyref{Sigmoid\+Cross\+Entropy\+Error}{p.}{classmlpack_1_1ann_1_1SigmoidCrossEntropyError} performance function measures the network\textquotesingle{}s performance according to the cross-\/entropy function between the input and target distributions. \end{DoxyCompactList}\item 
class \textbf{ SILUFunction}
\begin{DoxyCompactList}\small\item\em The SILU function, defined by. \end{DoxyCompactList}\item 
class \textbf{ Soft\+Margin\+Loss}
\item 
class \textbf{ Softmax}
\begin{DoxyCompactList}\small\item\em Implementation of the \doxyref{Softmax}{p.}{classmlpack_1_1ann_1_1Softmax} layer. \end{DoxyCompactList}\item 
class \textbf{ Softmin}
\begin{DoxyCompactList}\small\item\em Implementation of the \doxyref{Softmin}{p.}{classmlpack_1_1ann_1_1Softmin} layer. \end{DoxyCompactList}\item 
class \textbf{ Softplus\+Function}
\begin{DoxyCompactList}\small\item\em The softplus function, defined by. \end{DoxyCompactList}\item 
class \textbf{ Soft\+Shrink}
\begin{DoxyCompactList}\small\item\em Soft Shrink operator is defined as,. \end{DoxyCompactList}\item 
class \textbf{ Softsign\+Function}
\begin{DoxyCompactList}\small\item\em The softsign function, defined by. \end{DoxyCompactList}\item 
class \textbf{ Spatial\+Dropout}
\begin{DoxyCompactList}\small\item\em Implementation of the \doxyref{Spatial\+Dropout}{p.}{classmlpack_1_1ann_1_1SpatialDropout} layer. \end{DoxyCompactList}\item 
class \textbf{ Spike\+Slab\+RBM}
\begin{DoxyCompactList}\small\item\em For more information, see the following paper\+: \end{DoxyCompactList}\item 
class \textbf{ Spline\+Function}
\begin{DoxyCompactList}\small\item\em The Spline function, defined by. \end{DoxyCompactList}\item 
class \textbf{ Standard\+GAN}
\begin{DoxyCompactList}\small\item\em For more information, see the following paper\+: \end{DoxyCompactList}\item 
class \textbf{ Subview}
\begin{DoxyCompactList}\small\item\em Implementation of the subview layer. \end{DoxyCompactList}\item 
class \textbf{ SVDConvolution}
\begin{DoxyCompactList}\small\item\em Computes the two-\/dimensional convolution using singular value decomposition. \end{DoxyCompactList}\item 
class \textbf{ Swish\+Function}
\begin{DoxyCompactList}\small\item\em The swish function, defined by. \end{DoxyCompactList}\item 
class \textbf{ Tanh\+Exp\+Function}
\begin{DoxyCompactList}\small\item\em The Tanh\+Exp function, defined by. \end{DoxyCompactList}\item 
class \textbf{ Tanh\+Function}
\begin{DoxyCompactList}\small\item\em The tanh function, defined by. \end{DoxyCompactList}\item 
class \textbf{ Transposed\+Convolution}
\begin{DoxyCompactList}\small\item\em Implementation of the Transposed \doxyref{Convolution}{p.}{classmlpack_1_1ann_1_1Convolution} class. \end{DoxyCompactList}\item 
class \textbf{ Triplet\+Margin\+Loss}
\begin{DoxyCompactList}\small\item\em The Triplet Margin Loss performance function measures the network\textquotesingle{}s performance according to the relative distance from the anchor input of the positive (truthy) and negative (falsy) inputs. \end{DoxyCompactList}\item 
class \textbf{ Valid\+Convolution}
\item 
class \textbf{ Virtual\+Batch\+Norm}
\begin{DoxyCompactList}\small\item\em Declaration of the \doxyref{Virtual\+Batch\+Norm}{p.}{classmlpack_1_1ann_1_1VirtualBatchNorm} layer class. \end{DoxyCompactList}\item 
class \textbf{ VRClass\+Reward}
\begin{DoxyCompactList}\small\item\em Implementation of the variance reduced classification reinforcement layer. \end{DoxyCompactList}\item 
class \textbf{ Weight\+Norm}
\begin{DoxyCompactList}\small\item\em Declaration of the \doxyref{Weight\+Norm}{p.}{classmlpack_1_1ann_1_1WeightNorm} layer class. \end{DoxyCompactList}\item 
class \textbf{ Weight\+Set\+Visitor}
\begin{DoxyCompactList}\small\item\em \doxyref{Weight\+Set\+Visitor}{p.}{classmlpack_1_1ann_1_1WeightSetVisitor} update the module parameters given the parameters set. \end{DoxyCompactList}\item 
class \textbf{ Weight\+Size\+Visitor}
\begin{DoxyCompactList}\small\item\em \doxyref{Weight\+Size\+Visitor}{p.}{classmlpack_1_1ann_1_1WeightSizeVisitor} returns the number of weights of the given module. \end{DoxyCompactList}\item 
class \textbf{ WGAN}
\begin{DoxyCompactList}\small\item\em For more information, see the following paper\+: \end{DoxyCompactList}\item 
class \textbf{ WGANGP}
\begin{DoxyCompactList}\small\item\em For more information, see the following paper\+: \end{DoxyCompactList}\end{DoxyCompactItemize}
\doxysubsection*{Typedefs}
\begin{DoxyCompactItemize}
\item 
{\footnotesize template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ }\\using \textbf{ Cross\+Entropy\+Error} = \textbf{ BCELoss}$<$ Input\+Data\+Type, Output\+Data\+Type $>$
\begin{DoxyCompactList}\small\item\em Adding alias of \doxyref{BCELoss}{p.}{classmlpack_1_1ann_1_1BCELoss}. \end{DoxyCompactList}\item 
{\footnotesize template$<$class Activation\+Function  = Logistic\+Function, typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ }\\using \textbf{ Custom\+Layer} = \textbf{ Base\+Layer}$<$ Activation\+Function, Input\+Data\+Type, Output\+Data\+Type $>$
\begin{DoxyCompactList}\small\item\em Standard Sigmoid layer. \end{DoxyCompactList}\item 
{\footnotesize template$<$class Activation\+Function  = Elish\+Function, typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ }\\using \textbf{ Elish\+Function\+Layer} = \textbf{ Base\+Layer}$<$ Activation\+Function, Input\+Data\+Type, Output\+Data\+Type $>$
\begin{DoxyCompactList}\small\item\em Standard ELi\+SH-\/\+Layer using the ELi\+SH activation function. \end{DoxyCompactList}\item 
{\footnotesize template$<$class Activation\+Function  = Elliot\+Function, typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ }\\using \textbf{ Elliot\+Function\+Layer} = \textbf{ Base\+Layer}$<$ Activation\+Function, Input\+Data\+Type, Output\+Data\+Type $>$
\begin{DoxyCompactList}\small\item\em Standard Elliot-\/\+Layer using the Elliot activation function. \end{DoxyCompactList}\item 
{\footnotesize template$<$typename Mat\+Type  = arma\+::mat$>$ }\\using \textbf{ Embedding} = \textbf{ Lookup}$<$ Mat\+Type, Mat\+Type $>$
\item 
{\footnotesize template$<$class Activation\+Function  = Gaussian\+Function, typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ }\\using \textbf{ Gaussian\+Function\+Layer} = \textbf{ Base\+Layer}$<$ Activation\+Function, Input\+Data\+Type, Output\+Data\+Type $>$
\begin{DoxyCompactList}\small\item\em Standard Gaussian-\/\+Layer using the Gaussian activation function. \end{DoxyCompactList}\item 
{\footnotesize template$<$class Activation\+Function  = GELUFunction, typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ }\\using \textbf{ GELUFunction\+Layer} = \textbf{ Base\+Layer}$<$ Activation\+Function, Input\+Data\+Type, Output\+Data\+Type $>$
\begin{DoxyCompactList}\small\item\em Standard GELU-\/\+Layer using the GELU activation function. \end{DoxyCompactList}\item 
using \textbf{ Glorot\+Initialization} = \textbf{ Glorot\+Initialization\+Type}$<$ false $>$
\begin{DoxyCompactList}\small\item\em Glorot\+Initialization uses uniform distribution. \end{DoxyCompactList}\item 
{\footnotesize template$<$class Activation\+Function  = Hard\+Sigmoid\+Function, typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ }\\using \textbf{ Hard\+Sigmoid\+Layer} = \textbf{ Base\+Layer}$<$ Activation\+Function, Input\+Data\+Type, Output\+Data\+Type $>$
\begin{DoxyCompactList}\small\item\em Standard Hard\+Sigmoid-\/\+Layer using the Hard\+Sigmoid activation function. \end{DoxyCompactList}\item 
{\footnotesize template$<$class Activation\+Function  = Hard\+Swish\+Function, typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ }\\using \textbf{ Hard\+Swish\+Function\+Layer} = \textbf{ Base\+Layer}$<$ Activation\+Function, Input\+Data\+Type, Output\+Data\+Type $>$
\begin{DoxyCompactList}\small\item\em Standard Hard\+Swish-\/\+Layer using the Hard\+Swish activation function. \end{DoxyCompactList}\item 
{\footnotesize template$<$class Activation\+Function  = Identity\+Function, typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ }\\using \textbf{ Identity\+Layer} = \textbf{ Base\+Layer}$<$ Activation\+Function, Input\+Data\+Type, Output\+Data\+Type $>$
\begin{DoxyCompactList}\small\item\em Standard Identity-\/\+Layer using the identity activation function. \end{DoxyCompactList}\item 
typedef \textbf{ LRegularizer}$<$ 1 $>$ \textbf{ L1\+Regularizer}
\begin{DoxyCompactList}\small\item\em The L1 Regularizer. \end{DoxyCompactList}\item 
typedef \textbf{ LRegularizer}$<$ 2 $>$ \textbf{ L2\+Regularizer}
\begin{DoxyCompactList}\small\item\em The L2 Regularizer. \end{DoxyCompactList}\item 
{\footnotesize template$<$typename... Custom\+Layers$>$ }\\using \textbf{ Layer\+Types} = boost\+::variant$<$ \textbf{ Adaptive\+Max\+Pooling}$<$ arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ Adaptive\+Mean\+Pooling}$<$ arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ Add}$<$ arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ Add\+Merge}$<$ arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ Alpha\+Dropout}$<$ arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ Atrous\+Convolution}$<$ \textbf{ Naive\+Convolution}$<$ \textbf{ Valid\+Convolution} $>$, \textbf{ Naive\+Convolution}$<$ \textbf{ Full\+Convolution} $>$, \textbf{ Naive\+Convolution}$<$ \textbf{ Valid\+Convolution} $>$, arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ Base\+Layer}$<$ \textbf{ Logistic\+Function}, arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ Base\+Layer}$<$ \textbf{ Identity\+Function}, arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ Base\+Layer}$<$ \textbf{ Tanh\+Function}, arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ Base\+Layer}$<$ \textbf{ Softplus\+Function}, arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ Base\+Layer}$<$ \textbf{ Rectifier\+Function}, arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ Batch\+Norm}$<$ arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ Bilinear\+Interpolation}$<$ arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ CELU}$<$ arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ Concat}$<$ arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ Concatenate}$<$ arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ Concat\+Performance}$<$ \textbf{ Negative\+Log\+Likelihood}$<$ arma\+::mat, arma\+::mat $>$, arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ Constant}$<$ arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ Convolution}$<$ \textbf{ Naive\+Convolution}$<$ \textbf{ Valid\+Convolution} $>$, \textbf{ Naive\+Convolution}$<$ \textbf{ Full\+Convolution} $>$, \textbf{ Naive\+Convolution}$<$ \textbf{ Valid\+Convolution} $>$, arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ CRe\+LU}$<$ arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ Drop\+Connect}$<$ arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ Dropout}$<$ arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ ELU}$<$ arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ Fast\+LSTM}$<$ arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ Flexible\+Re\+LU}$<$ arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ GRU}$<$ arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ Hard\+TanH}$<$ arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ Join}$<$ arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ Layer\+Norm}$<$ arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ Leaky\+Re\+LU}$<$ arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ Linear}$<$ arma\+::mat, arma\+::mat, \textbf{ No\+Regularizer} $>$ $\ast$, \textbf{ Linear\+No\+Bias}$<$ arma\+::mat, arma\+::mat, \textbf{ No\+Regularizer} $>$ $\ast$, \textbf{ Log\+Soft\+Max}$<$ arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ Lookup}$<$ arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ LSTM}$<$ arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ Max\+Pooling}$<$ arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ Mean\+Pooling}$<$ arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ Mini\+Batch\+Discrimination}$<$ arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ Multiply\+Constant}$<$ arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ Multiply\+Merge}$<$ arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ Negative\+Log\+Likelihood}$<$ arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ Noisy\+Linear}$<$ arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ Padding}$<$ arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ PRe\+LU}$<$ arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ Softmax}$<$ arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ Spatial\+Dropout}$<$ arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ Transposed\+Convolution}$<$ \textbf{ Naive\+Convolution}$<$ \textbf{ Valid\+Convolution} $>$, \textbf{ Naive\+Convolution}$<$ \textbf{ Valid\+Convolution} $>$, \textbf{ Naive\+Convolution}$<$ \textbf{ Valid\+Convolution} $>$, arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ Weight\+Norm}$<$ arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ More\+Types}, Custom\+Layers $\ast$... $>$
\item 
{\footnotesize template$<$class Activation\+Function  = Li\+SHTFunction, typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ }\\using \textbf{ Li\+SHTFunction\+Layer} = \textbf{ Base\+Layer}$<$ Activation\+Function, Input\+Data\+Type, Output\+Data\+Type $>$
\begin{DoxyCompactList}\small\item\em Standard Li\+SHT-\/\+Layer using the Li\+SHT activation function. \end{DoxyCompactList}\item 
{\footnotesize template$<$class Activation\+Function  = Mish\+Function, typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ }\\using \textbf{ Mish\+Function\+Layer} = \textbf{ Base\+Layer}$<$ Activation\+Function, Input\+Data\+Type, Output\+Data\+Type $>$
\begin{DoxyCompactList}\small\item\em Standard Mish-\/\+Layer using the Mish activation function. \end{DoxyCompactList}\item 
using \textbf{ More\+Types} = boost\+::variant$<$ \textbf{ Linear3D}$<$ arma\+::mat, arma\+::mat, \textbf{ No\+Regularizer} $>$ $\ast$, \textbf{ Lp\+Pooling}$<$ arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ Pixel\+Shuffle}$<$ arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ Glimpse}$<$ arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ Highway}$<$ arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ Multihead\+Attention}$<$ arma\+::mat, arma\+::mat, \textbf{ No\+Regularizer} $>$ $\ast$, \textbf{ Recurrent}$<$ arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ Recurrent\+Attention}$<$ arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ Reinforce\+Normal}$<$ arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ Reparametrization}$<$ arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ Select}$<$ arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ Sequential}$<$ arma\+::mat, arma\+::mat, false $>$ $\ast$, \textbf{ Sequential}$<$ arma\+::mat, arma\+::mat, true $>$ $\ast$, \textbf{ Subview}$<$ arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ VRClass\+Reward}$<$ arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ Virtual\+Batch\+Norm}$<$ arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ RBF}$<$ arma\+::mat, arma\+::mat, \textbf{ Gaussian\+Function} $>$ $\ast$, \textbf{ Base\+Layer}$<$ \textbf{ Gaussian\+Function}, arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ Positional\+Encoding}$<$ arma\+::mat, arma\+::mat $>$ $\ast$, \textbf{ ISRLU}$<$ arma\+::mat, arma\+::mat $>$ $\ast$ $>$
\item 
{\footnotesize template$<$class Activation\+Function  = Rectifier\+Function, typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ }\\using \textbf{ Re\+LULayer} = \textbf{ Base\+Layer}$<$ Activation\+Function, Input\+Data\+Type, Output\+Data\+Type $>$
\begin{DoxyCompactList}\small\item\em Standard rectified linear unit non-\/linearity layer. \end{DoxyCompactList}\item 
{\footnotesize template$<$typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat, typename... Custom\+Layers$>$ }\\using \textbf{ Residual} = \textbf{ Sequential}$<$ Input\+Data\+Type, Output\+Data\+Type, true, Custom\+Layers... $>$
\item 
using \textbf{ SELU} = \textbf{ ELU}$<$ arma\+::mat, arma\+::mat $>$
\item 
{\footnotesize template$<$class Activation\+Function  = Logistic\+Function, typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ }\\using \textbf{ Sigmoid\+Layer} = \textbf{ Base\+Layer}$<$ Activation\+Function, Input\+Data\+Type, Output\+Data\+Type $>$
\begin{DoxyCompactList}\small\item\em Standard Sigmoid-\/\+Layer using the logistic activation function. \end{DoxyCompactList}\item 
{\footnotesize template$<$class Activation\+Function  = SILUFunction, typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ }\\using \textbf{ SILUFunction\+Layer} = \textbf{ Base\+Layer}$<$ Activation\+Function, Input\+Data\+Type, Output\+Data\+Type $>$
\begin{DoxyCompactList}\small\item\em Standard SILU-\/\+Layer using the SILU activation function. \end{DoxyCompactList}\item 
{\footnotesize template$<$class Activation\+Function  = Softplus\+Function, typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ }\\using \textbf{ Soft\+Plus\+Layer} = \textbf{ Base\+Layer}$<$ Activation\+Function, Input\+Data\+Type, Output\+Data\+Type $>$
\begin{DoxyCompactList}\small\item\em Standard Softplus-\/\+Layer using the Softplus activation function. \end{DoxyCompactList}\item 
{\footnotesize template$<$class Activation\+Function  = Swish\+Function, typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ }\\using \textbf{ Swish\+Function\+Layer} = \textbf{ Base\+Layer}$<$ Activation\+Function, Input\+Data\+Type, Output\+Data\+Type $>$
\begin{DoxyCompactList}\small\item\em Standard Swish-\/\+Layer using the Swish activation function. \end{DoxyCompactList}\item 
{\footnotesize template$<$class Activation\+Function  = Tanh\+Exp\+Function, typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ }\\using \textbf{ Tanh\+Exp\+Function\+Layer} = \textbf{ Base\+Layer}$<$ Activation\+Function, Input\+Data\+Type, Output\+Data\+Type $>$
\begin{DoxyCompactList}\small\item\em Standard Tanh\+Exp-\/\+Layer using the Tanh\+Exp activation function. \end{DoxyCompactList}\item 
{\footnotesize template$<$class Activation\+Function  = Tanh\+Function, typename Input\+Data\+Type  = arma\+::mat, typename Output\+Data\+Type  = arma\+::mat$>$ }\\using \textbf{ Tan\+HLayer} = \textbf{ Base\+Layer}$<$ Activation\+Function, Input\+Data\+Type, Output\+Data\+Type $>$
\begin{DoxyCompactList}\small\item\em Standard hyperbolic tangent layer. \end{DoxyCompactList}\item 
using \textbf{ Xavier\+Initialization} = \textbf{ Glorot\+Initialization\+Type}$<$ true $>$
\begin{DoxyCompactList}\small\item\em Xavier\+Initilization is the popular name for this method. \end{DoxyCompactList}\end{DoxyCompactItemize}
\doxysubsection*{Functions}
\begin{DoxyCompactItemize}
\item 
{\footnotesize template$<$typename T $>$ }\\void \textbf{ Check\+Input\+Shape} (const T \&network, const size\+\_\+t input\+Shape, const std\+::string \&function\+Name)
\item 
\textbf{ HAS\+\_\+\+ANY\+\_\+\+METHOD\+\_\+\+FORM} (Input\+Shape, Has\+Input\+Shape\+Check)
\item 
\textbf{ HAS\+\_\+\+ANY\+\_\+\+METHOD\+\_\+\+FORM} (Model, Has\+Model\+Check)
\item 
\textbf{ HAS\+\_\+\+MEM\+\_\+\+FUNC} (\textbf{ Add}, Has\+Add\+Check)
\item 
\textbf{ HAS\+\_\+\+MEM\+\_\+\+FUNC} (Bias, Has\+Bias\+Check)
\item 
\textbf{ HAS\+\_\+\+MEM\+\_\+\+FUNC} (Deterministic, Has\+Deterministic\+Check)
\item 
\textbf{ HAS\+\_\+\+MEM\+\_\+\+FUNC} (Gradient, Has\+Gradient\+Check)
\item 
\textbf{ HAS\+\_\+\+MEM\+\_\+\+FUNC} (Input\+Height, Has\+Input\+Height)
\item 
\textbf{ HAS\+\_\+\+MEM\+\_\+\+FUNC} (Input\+Width, Has\+Input\+Width)
\item 
\textbf{ HAS\+\_\+\+MEM\+\_\+\+FUNC} (Location, Has\+Location\+Check)
\item 
\textbf{ HAS\+\_\+\+MEM\+\_\+\+FUNC} (Loss, Has\+Loss)
\item 
\textbf{ HAS\+\_\+\+MEM\+\_\+\+FUNC} (Max\+Iterations, Has\+Max\+Iterations)
\item 
\textbf{ HAS\+\_\+\+MEM\+\_\+\+FUNC} (Parameters, Has\+Parameters\+Check)
\item 
\textbf{ HAS\+\_\+\+MEM\+\_\+\+FUNC} (Reset, Has\+Reset\+Check)
\item 
\textbf{ HAS\+\_\+\+MEM\+\_\+\+FUNC} (Reset\+Cell, Has\+Reset\+Cell\+Check)
\item 
\textbf{ HAS\+\_\+\+MEM\+\_\+\+FUNC} (Reward, Has\+Reward\+Check)
\item 
\textbf{ HAS\+\_\+\+MEM\+\_\+\+FUNC} (Rho, Has\+Rho)
\item 
\textbf{ HAS\+\_\+\+MEM\+\_\+\+FUNC} (Run, Has\+Run\+Check)
\item 
{\footnotesize template$<$typename Model\+Type $>$ }\\double \textbf{ Inception\+Score} (Model\+Type Model, arma\+::mat images, size\+\_\+t splits=1)
\begin{DoxyCompactList}\small\item\em Function that computes Inception Score for a set of images produced by a \doxyref{GAN}{p.}{classmlpack_1_1ann_1_1GAN}. \end{DoxyCompactList}\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
Artificial Neural Network. 

Artifical Neural Network. 

\doxysubsection{Typedef Documentation}
\mbox{\label{namespacemlpack_1_1ann_ac9d51e01837cbec9de586990aa8123d2}} 
\index{mlpack::ann@{mlpack::ann}!CrossEntropyError@{CrossEntropyError}}
\index{CrossEntropyError@{CrossEntropyError}!mlpack::ann@{mlpack::ann}}
\doxysubsubsection{CrossEntropyError}
{\footnotesize\ttfamily using \textbf{ Cross\+Entropy\+Error} =  \textbf{ BCELoss}$<$ Input\+Data\+Type, Output\+Data\+Type$>$}



Adding alias of \doxyref{BCELoss}{p.}{classmlpack_1_1ann_1_1BCELoss}. 



Definition at line 109 of file binary\+\_\+cross\+\_\+entropy\+\_\+loss.\+hpp.

\mbox{\label{namespacemlpack_1_1ann_ab8ac0d1eb11983be1bc7419ce15e91bf}} 
\index{mlpack::ann@{mlpack::ann}!CustomLayer@{CustomLayer}}
\index{CustomLayer@{CustomLayer}!mlpack::ann@{mlpack::ann}}
\doxysubsubsection{CustomLayer}
{\footnotesize\ttfamily using \textbf{ Custom\+Layer} =  \textbf{ Base\+Layer}$<$ Activation\+Function, Input\+Data\+Type, Output\+Data\+Type$>$}



Standard Sigmoid layer. 



Definition at line 30 of file custom\+\_\+layer.\+hpp.

\mbox{\label{namespacemlpack_1_1ann_aba712c4fa3b49cf06a01ef6867b958fb}} 
\index{mlpack::ann@{mlpack::ann}!ElishFunctionLayer@{ElishFunctionLayer}}
\index{ElishFunctionLayer@{ElishFunctionLayer}!mlpack::ann@{mlpack::ann}}
\doxysubsubsection{ElishFunctionLayer}
{\footnotesize\ttfamily using \textbf{ Elish\+Function\+Layer} =  \textbf{ Base\+Layer}$<$ Activation\+Function, Input\+Data\+Type, Output\+Data\+Type$>$}



Standard ELi\+SH-\/\+Layer using the ELi\+SH activation function. 



Definition at line 272 of file base\+\_\+layer.\+hpp.

\mbox{\label{namespacemlpack_1_1ann_a35264d115715479bbc952816fb070e99}} 
\index{mlpack::ann@{mlpack::ann}!ElliotFunctionLayer@{ElliotFunctionLayer}}
\index{ElliotFunctionLayer@{ElliotFunctionLayer}!mlpack::ann@{mlpack::ann}}
\doxysubsubsection{ElliotFunctionLayer}
{\footnotesize\ttfamily using \textbf{ Elliot\+Function\+Layer} =  \textbf{ Base\+Layer}$<$ Activation\+Function, Input\+Data\+Type, Output\+Data\+Type$>$}



Standard Elliot-\/\+Layer using the Elliot activation function. 



Definition at line 261 of file base\+\_\+layer.\+hpp.

\mbox{\label{namespacemlpack_1_1ann_aaf3ea313e70c222598e17bf4e23dd451}} 
\index{mlpack::ann@{mlpack::ann}!Embedding@{Embedding}}
\index{Embedding@{Embedding}!mlpack::ann@{mlpack::ann}}
\doxysubsubsection{Embedding}
{\footnotesize\ttfamily using \textbf{ Embedding} =  \textbf{ Lookup}$<$Mat\+Type, Mat\+Type$>$}



Definition at line 142 of file lookup.\+hpp.

\mbox{\label{namespacemlpack_1_1ann_a56fb7042c8f1db823cd8ec97b7df6616}} 
\index{mlpack::ann@{mlpack::ann}!GaussianFunctionLayer@{GaussianFunctionLayer}}
\index{GaussianFunctionLayer@{GaussianFunctionLayer}!mlpack::ann@{mlpack::ann}}
\doxysubsubsection{GaussianFunctionLayer}
{\footnotesize\ttfamily using \textbf{ Gaussian\+Function\+Layer} =  \textbf{ Base\+Layer}$<$ Activation\+Function, Input\+Data\+Type, Output\+Data\+Type$>$}



Standard Gaussian-\/\+Layer using the Gaussian activation function. 



Definition at line 283 of file base\+\_\+layer.\+hpp.

\mbox{\label{namespacemlpack_1_1ann_a69253ae519ed598c1bf8b5e3368f6ba4}} 
\index{mlpack::ann@{mlpack::ann}!GELUFunctionLayer@{GELUFunctionLayer}}
\index{GELUFunctionLayer@{GELUFunctionLayer}!mlpack::ann@{mlpack::ann}}
\doxysubsubsection{GELUFunctionLayer}
{\footnotesize\ttfamily using \textbf{ GELUFunction\+Layer} =  \textbf{ Base\+Layer}$<$ Activation\+Function, Input\+Data\+Type, Output\+Data\+Type$>$}



Standard GELU-\/\+Layer using the GELU activation function. 



Definition at line 250 of file base\+\_\+layer.\+hpp.

\mbox{\label{namespacemlpack_1_1ann_a003150a66fa8a2cc2a3650e2384a1dfc}} 
\index{mlpack::ann@{mlpack::ann}!GlorotInitialization@{GlorotInitialization}}
\index{GlorotInitialization@{GlorotInitialization}!mlpack::ann@{mlpack::ann}}
\doxysubsubsection{GlorotInitialization}
{\footnotesize\ttfamily using \textbf{ Glorot\+Initialization} =  \textbf{ Glorot\+Initialization\+Type}$<$false$>$}



Glorot\+Initialization uses uniform distribution. 



Definition at line 200 of file glorot\+\_\+init.\+hpp.

\mbox{\label{namespacemlpack_1_1ann_ae5bb0281a40f808dda254ea8d16d6acf}} 
\index{mlpack::ann@{mlpack::ann}!HardSigmoidLayer@{HardSigmoidLayer}}
\index{HardSigmoidLayer@{HardSigmoidLayer}!mlpack::ann@{mlpack::ann}}
\doxysubsubsection{HardSigmoidLayer}
{\footnotesize\ttfamily using \textbf{ Hard\+Sigmoid\+Layer} =  \textbf{ Base\+Layer}$<$ Activation\+Function, Input\+Data\+Type, Output\+Data\+Type$>$}



Standard Hard\+Sigmoid-\/\+Layer using the Hard\+Sigmoid activation function. 



Definition at line 206 of file base\+\_\+layer.\+hpp.

\mbox{\label{namespacemlpack_1_1ann_a980da46588164257c981febf80bc14ce}} 
\index{mlpack::ann@{mlpack::ann}!HardSwishFunctionLayer@{HardSwishFunctionLayer}}
\index{HardSwishFunctionLayer@{HardSwishFunctionLayer}!mlpack::ann@{mlpack::ann}}
\doxysubsubsection{HardSwishFunctionLayer}
{\footnotesize\ttfamily using \textbf{ Hard\+Swish\+Function\+Layer} =  \textbf{ Base\+Layer}$<$ Activation\+Function, Input\+Data\+Type, Output\+Data\+Type$>$}



Standard Hard\+Swish-\/\+Layer using the Hard\+Swish activation function. 



Definition at line 294 of file base\+\_\+layer.\+hpp.

\mbox{\label{namespacemlpack_1_1ann_a41e9b25c5b33b7de07a0eac6c46dc085}} 
\index{mlpack::ann@{mlpack::ann}!IdentityLayer@{IdentityLayer}}
\index{IdentityLayer@{IdentityLayer}!mlpack::ann@{mlpack::ann}}
\doxysubsubsection{IdentityLayer}
{\footnotesize\ttfamily using \textbf{ Identity\+Layer} =  \textbf{ Base\+Layer}$<$ Activation\+Function, Input\+Data\+Type, Output\+Data\+Type$>$}



Standard Identity-\/\+Layer using the identity activation function. 



Definition at line 162 of file base\+\_\+layer.\+hpp.

\mbox{\label{namespacemlpack_1_1ann_a137169a12d6f400a718d7383f3365112}} 
\index{mlpack::ann@{mlpack::ann}!L1Regularizer@{L1Regularizer}}
\index{L1Regularizer@{L1Regularizer}!mlpack::ann@{mlpack::ann}}
\doxysubsubsection{L1Regularizer}
{\footnotesize\ttfamily typedef \textbf{ LRegularizer}$<$1$>$ \textbf{ L1\+Regularizer}}



The L1 Regularizer. 



Definition at line 62 of file lregularizer.\+hpp.

\mbox{\label{namespacemlpack_1_1ann_ac3b4459576bd0564e145e049ee1549ce}} 
\index{mlpack::ann@{mlpack::ann}!L2Regularizer@{L2Regularizer}}
\index{L2Regularizer@{L2Regularizer}!mlpack::ann@{mlpack::ann}}
\doxysubsubsection{L2Regularizer}
{\footnotesize\ttfamily typedef \textbf{ LRegularizer}$<$2$>$ \textbf{ L2\+Regularizer}}



The L2 Regularizer. 



Definition at line 67 of file lregularizer.\+hpp.

\mbox{\label{namespacemlpack_1_1ann_ac93be13f0d072573d93e2756ef478ab6}} 
\index{mlpack::ann@{mlpack::ann}!LayerTypes@{LayerTypes}}
\index{LayerTypes@{LayerTypes}!mlpack::ann@{mlpack::ann}}
\doxysubsubsection{LayerTypes}
{\footnotesize\ttfamily using \textbf{ Layer\+Types} =  boost\+::variant$<$ \textbf{ Adaptive\+Max\+Pooling}$<$arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ Adaptive\+Mean\+Pooling}$<$arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ Add}$<$arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ Add\+Merge}$<$arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ Alpha\+Dropout}$<$arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ Atrous\+Convolution}$<$\textbf{ Naive\+Convolution}$<$\textbf{ Valid\+Convolution}$>$, \textbf{ Naive\+Convolution}$<$\textbf{ Full\+Convolution}$>$, \textbf{ Naive\+Convolution}$<$\textbf{ Valid\+Convolution}$>$, arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ Base\+Layer}$<$\textbf{ Logistic\+Function}, arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ Base\+Layer}$<$\textbf{ Identity\+Function}, arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ Base\+Layer}$<$\textbf{ Tanh\+Function}, arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ Base\+Layer}$<$\textbf{ Softplus\+Function}, arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ Base\+Layer}$<$\textbf{ Rectifier\+Function}, arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ Batch\+Norm}$<$arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ Bilinear\+Interpolation}$<$arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ CELU}$<$arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ Concat}$<$arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ Concatenate}$<$arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ Concat\+Performance}$<$\textbf{ Negative\+Log\+Likelihood}$<$arma\+::mat, arma\+::mat$>$, arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ Constant}$<$arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ Convolution}$<$\textbf{ Naive\+Convolution}$<$\textbf{ Valid\+Convolution}$>$, \textbf{ Naive\+Convolution}$<$\textbf{ Full\+Convolution}$>$, \textbf{ Naive\+Convolution}$<$\textbf{ Valid\+Convolution}$>$, arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ CRe\+LU}$<$arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ Drop\+Connect}$<$arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ Dropout}$<$arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ ELU}$<$arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ Fast\+LSTM}$<$arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ Flexible\+Re\+LU}$<$arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ GRU}$<$arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ Hard\+TanH}$<$arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ Join}$<$arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ Layer\+Norm}$<$arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ Leaky\+Re\+LU}$<$arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ Linear}$<$arma\+::mat, arma\+::mat, \textbf{ No\+Regularizer}$>$$\ast$, \textbf{ Linear\+No\+Bias}$<$arma\+::mat, arma\+::mat, \textbf{ No\+Regularizer}$>$$\ast$, \textbf{ Log\+Soft\+Max}$<$arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ Lookup}$<$arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ LSTM}$<$arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ Max\+Pooling}$<$arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ Mean\+Pooling}$<$arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ Mini\+Batch\+Discrimination}$<$arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ Multiply\+Constant}$<$arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ Multiply\+Merge}$<$arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ Negative\+Log\+Likelihood}$<$arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ Noisy\+Linear}$<$arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ Padding}$<$arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ PRe\+LU}$<$arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ Softmax}$<$arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ Spatial\+Dropout}$<$arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ Transposed\+Convolution}$<$\textbf{ Naive\+Convolution}$<$\textbf{ Valid\+Convolution}$>$, \textbf{ Naive\+Convolution}$<$\textbf{ Valid\+Convolution}$>$, \textbf{ Naive\+Convolution}$<$\textbf{ Valid\+Convolution}$>$, arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ Weight\+Norm}$<$arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ More\+Types}, Custom\+Layers$\ast$... $>$}



Definition at line 247 of file layer\+\_\+types.\+hpp.

\mbox{\label{namespacemlpack_1_1ann_aebe38e4259931f33c44701ba75d6240d}} 
\index{mlpack::ann@{mlpack::ann}!LiSHTFunctionLayer@{LiSHTFunctionLayer}}
\index{LiSHTFunctionLayer@{LiSHTFunctionLayer}!mlpack::ann@{mlpack::ann}}
\doxysubsubsection{LiSHTFunctionLayer}
{\footnotesize\ttfamily using \textbf{ Li\+SHTFunction\+Layer} =  \textbf{ Base\+Layer}$<$ Activation\+Function, Input\+Data\+Type, Output\+Data\+Type$>$}



Standard Li\+SHT-\/\+Layer using the Li\+SHT activation function. 



Definition at line 239 of file base\+\_\+layer.\+hpp.

\mbox{\label{namespacemlpack_1_1ann_a52aa33cac06fc8dbf5eefcb4e1858fea}} 
\index{mlpack::ann@{mlpack::ann}!MishFunctionLayer@{MishFunctionLayer}}
\index{MishFunctionLayer@{MishFunctionLayer}!mlpack::ann@{mlpack::ann}}
\doxysubsubsection{MishFunctionLayer}
{\footnotesize\ttfamily using \textbf{ Mish\+Function\+Layer} =  \textbf{ Base\+Layer}$<$ Activation\+Function, Input\+Data\+Type, Output\+Data\+Type$>$}



Standard Mish-\/\+Layer using the Mish activation function. 



Definition at line 228 of file base\+\_\+layer.\+hpp.

\mbox{\label{namespacemlpack_1_1ann_a43629d7dff68779cabbd5ff7cb1448d0}} 
\index{mlpack::ann@{mlpack::ann}!MoreTypes@{MoreTypes}}
\index{MoreTypes@{MoreTypes}!mlpack::ann@{mlpack::ann}}
\doxysubsubsection{MoreTypes}
{\footnotesize\ttfamily using \textbf{ More\+Types} =  boost\+::variant$<$ \textbf{ Linear3D}$<$arma\+::mat, arma\+::mat, \textbf{ No\+Regularizer}$>$$\ast$, \textbf{ Lp\+Pooling}$<$arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ Pixel\+Shuffle}$<$arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ Glimpse}$<$arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ Highway}$<$arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ Multihead\+Attention}$<$arma\+::mat, arma\+::mat, \textbf{ No\+Regularizer}$>$$\ast$, \textbf{ Recurrent}$<$arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ Recurrent\+Attention}$<$arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ Reinforce\+Normal}$<$arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ Reparametrization}$<$arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ Select}$<$arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ Sequential}$<$arma\+::mat, arma\+::mat, false$>$$\ast$, \textbf{ Sequential}$<$arma\+::mat, arma\+::mat, true$>$$\ast$, \textbf{ Subview}$<$arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ VRClass\+Reward}$<$arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ Virtual\+Batch\+Norm}$<$arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ RBF}$<$arma\+::mat, arma\+::mat, \textbf{ Gaussian\+Function}$>$$\ast$, \textbf{ Base\+Layer}$<$\textbf{ Gaussian\+Function}, arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ Positional\+Encoding}$<$arma\+::mat, arma\+::mat$>$$\ast$, \textbf{ ISRLU}$<$arma\+::mat, arma\+::mat$>$$\ast$ $>$}



Definition at line 223 of file layer\+\_\+types.\+hpp.

\mbox{\label{namespacemlpack_1_1ann_a06b6e29ab52ee62d0bccbf108d64d1a2}} 
\index{mlpack::ann@{mlpack::ann}!ReLULayer@{ReLULayer}}
\index{ReLULayer@{ReLULayer}!mlpack::ann@{mlpack::ann}}
\doxysubsubsection{ReLULayer}
{\footnotesize\ttfamily using \textbf{ Re\+LULayer} =  \textbf{ Base\+Layer}$<$ Activation\+Function, Input\+Data\+Type, Output\+Data\+Type$>$}



Standard rectified linear unit non-\/linearity layer. 



Definition at line 173 of file base\+\_\+layer.\+hpp.

\mbox{\label{namespacemlpack_1_1ann_ac4f089366ec4066e82d7c4ecae664a46}} 
\index{mlpack::ann@{mlpack::ann}!Residual@{Residual}}
\index{Residual@{Residual}!mlpack::ann@{mlpack::ann}}
\doxysubsubsection{Residual}
{\footnotesize\ttfamily using \textbf{ Residual} =  \textbf{ Sequential}$<$ Input\+Data\+Type, Output\+Data\+Type, true, Custom\+Layers...$>$}



Definition at line 260 of file sequential.\+hpp.

\mbox{\label{namespacemlpack_1_1ann_ac08f9682be904369ec09e68b43b09fad}} 
\index{mlpack::ann@{mlpack::ann}!SELU@{SELU}}
\index{SELU@{SELU}!mlpack::ann@{mlpack::ann}}
\doxysubsubsection{SELU}
{\footnotesize\ttfamily using \textbf{ SELU} =  \textbf{ ELU}$<$arma\+::mat, arma\+::mat$>$}



Definition at line 207 of file elu.\+hpp.

\mbox{\label{namespacemlpack_1_1ann_ad4f35bf0f4f5e2750668e17c2d07a27b}} 
\index{mlpack::ann@{mlpack::ann}!SigmoidLayer@{SigmoidLayer}}
\index{SigmoidLayer@{SigmoidLayer}!mlpack::ann@{mlpack::ann}}
\doxysubsubsection{SigmoidLayer}
{\footnotesize\ttfamily using \textbf{ Sigmoid\+Layer} =  \textbf{ Base\+Layer}$<$ Activation\+Function, Input\+Data\+Type, Output\+Data\+Type$>$}



Standard Sigmoid-\/\+Layer using the logistic activation function. 



Definition at line 151 of file base\+\_\+layer.\+hpp.

\mbox{\label{namespacemlpack_1_1ann_aefb7029510e09a06ddaef3ca52f77ba6}} 
\index{mlpack::ann@{mlpack::ann}!SILUFunctionLayer@{SILUFunctionLayer}}
\index{SILUFunctionLayer@{SILUFunctionLayer}!mlpack::ann@{mlpack::ann}}
\doxysubsubsection{SILUFunctionLayer}
{\footnotesize\ttfamily using \textbf{ SILUFunction\+Layer} =  \textbf{ Base\+Layer}$<$ Activation\+Function, Input\+Data\+Type, Output\+Data\+Type $>$}



Standard SILU-\/\+Layer using the SILU activation function. 



Definition at line 316 of file base\+\_\+layer.\+hpp.

\mbox{\label{namespacemlpack_1_1ann_a3b84f714a815d838e34c11a59480cd1c}} 
\index{mlpack::ann@{mlpack::ann}!SoftPlusLayer@{SoftPlusLayer}}
\index{SoftPlusLayer@{SoftPlusLayer}!mlpack::ann@{mlpack::ann}}
\doxysubsubsection{SoftPlusLayer}
{\footnotesize\ttfamily using \textbf{ Soft\+Plus\+Layer} =  \textbf{ Base\+Layer}$<$ Activation\+Function, Input\+Data\+Type, Output\+Data\+Type$>$}



Standard Softplus-\/\+Layer using the Softplus activation function. 



Definition at line 195 of file base\+\_\+layer.\+hpp.

\mbox{\label{namespacemlpack_1_1ann_a8ea44af9d438cf5fc4098a0edc9e74a4}} 
\index{mlpack::ann@{mlpack::ann}!SwishFunctionLayer@{SwishFunctionLayer}}
\index{SwishFunctionLayer@{SwishFunctionLayer}!mlpack::ann@{mlpack::ann}}
\doxysubsubsection{SwishFunctionLayer}
{\footnotesize\ttfamily using \textbf{ Swish\+Function\+Layer} =  \textbf{ Base\+Layer}$<$ Activation\+Function, Input\+Data\+Type, Output\+Data\+Type$>$}



Standard Swish-\/\+Layer using the Swish activation function. 



Definition at line 217 of file base\+\_\+layer.\+hpp.

\mbox{\label{namespacemlpack_1_1ann_a48b0b3ad6afd2fd3258be418019d9fcb}} 
\index{mlpack::ann@{mlpack::ann}!TanhExpFunctionLayer@{TanhExpFunctionLayer}}
\index{TanhExpFunctionLayer@{TanhExpFunctionLayer}!mlpack::ann@{mlpack::ann}}
\doxysubsubsection{TanhExpFunctionLayer}
{\footnotesize\ttfamily using \textbf{ Tanh\+Exp\+Function\+Layer} =  \textbf{ Base\+Layer}$<$ Activation\+Function, Input\+Data\+Type, Output\+Data\+Type$>$}



Standard Tanh\+Exp-\/\+Layer using the Tanh\+Exp activation function. 



Definition at line 305 of file base\+\_\+layer.\+hpp.

\mbox{\label{namespacemlpack_1_1ann_acc8e7f6b94728a4a6eb310677b5bc532}} 
\index{mlpack::ann@{mlpack::ann}!TanHLayer@{TanHLayer}}
\index{TanHLayer@{TanHLayer}!mlpack::ann@{mlpack::ann}}
\doxysubsubsection{TanHLayer}
{\footnotesize\ttfamily using \textbf{ Tan\+HLayer} =  \textbf{ Base\+Layer}$<$ Activation\+Function, Input\+Data\+Type, Output\+Data\+Type$>$}



Standard hyperbolic tangent layer. 



Definition at line 184 of file base\+\_\+layer.\+hpp.

\mbox{\label{namespacemlpack_1_1ann_a4f99a527ad82262756bb7e3785e8201a}} 
\index{mlpack::ann@{mlpack::ann}!XavierInitialization@{XavierInitialization}}
\index{XavierInitialization@{XavierInitialization}!mlpack::ann@{mlpack::ann}}
\doxysubsubsection{XavierInitialization}
{\footnotesize\ttfamily using \textbf{ Xavier\+Initialization} =  \textbf{ Glorot\+Initialization\+Type}$<$true$>$}



Xavier\+Initilization is the popular name for this method. 



Definition at line 195 of file glorot\+\_\+init.\+hpp.



\doxysubsection{Function Documentation}
\mbox{\label{namespacemlpack_1_1ann_a0812ae5ed2e1ef94937116918ba881a8}} 
\index{mlpack::ann@{mlpack::ann}!CheckInputShape@{CheckInputShape}}
\index{CheckInputShape@{CheckInputShape}!mlpack::ann@{mlpack::ann}}
\doxysubsubsection{CheckInputShape()}
{\footnotesize\ttfamily void mlpack\+::ann\+::\+Check\+Input\+Shape (\begin{DoxyParamCaption}\item[{const T \&}]{network,  }\item[{const size\+\_\+t}]{input\+Shape,  }\item[{const std\+::string \&}]{function\+Name }\end{DoxyParamCaption})}



Definition at line 25 of file check\+\_\+input\+\_\+shape.\+hpp.

\mbox{\label{namespacemlpack_1_1ann_aa333dabed73cee663a9adb47c167570e}} 
\index{mlpack::ann@{mlpack::ann}!HAS\_ANY\_METHOD\_FORM@{HAS\_ANY\_METHOD\_FORM}}
\index{HAS\_ANY\_METHOD\_FORM@{HAS\_ANY\_METHOD\_FORM}!mlpack::ann@{mlpack::ann}}
\doxysubsubsection{HAS\_ANY\_METHOD\_FORM()\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily mlpack\+::ann\+::\+HAS\+\_\+\+ANY\+\_\+\+METHOD\+\_\+\+FORM (\begin{DoxyParamCaption}\item[{Input\+Shape}]{,  }\item[{Has\+Input\+Shape\+Check}]{ }\end{DoxyParamCaption})}

\mbox{\label{namespacemlpack_1_1ann_a5aeaf3e16247ebd569074c32cab63c70}} 
\index{mlpack::ann@{mlpack::ann}!HAS\_ANY\_METHOD\_FORM@{HAS\_ANY\_METHOD\_FORM}}
\index{HAS\_ANY\_METHOD\_FORM@{HAS\_ANY\_METHOD\_FORM}!mlpack::ann@{mlpack::ann}}
\doxysubsubsection{HAS\_ANY\_METHOD\_FORM()\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily mlpack\+::ann\+::\+HAS\+\_\+\+ANY\+\_\+\+METHOD\+\_\+\+FORM (\begin{DoxyParamCaption}\item[{Model}]{,  }\item[{Has\+Model\+Check}]{ }\end{DoxyParamCaption})}

\mbox{\label{namespacemlpack_1_1ann_a923497f92d9b28cfe7143d40e00c6bfc}} 
\index{mlpack::ann@{mlpack::ann}!HAS\_MEM\_FUNC@{HAS\_MEM\_FUNC}}
\index{HAS\_MEM\_FUNC@{HAS\_MEM\_FUNC}!mlpack::ann@{mlpack::ann}}
\doxysubsubsection{HAS\_MEM\_FUNC()\hspace{0.1cm}{\footnotesize\ttfamily [1/15]}}
{\footnotesize\ttfamily mlpack\+::ann\+::\+HAS\+\_\+\+MEM\+\_\+\+FUNC (\begin{DoxyParamCaption}\item[{\textbf{ Add}}]{,  }\item[{Has\+Add\+Check}]{ }\end{DoxyParamCaption})}

\mbox{\label{namespacemlpack_1_1ann_aa339cfab0c5987cfec78736f19e50373}} 
\index{mlpack::ann@{mlpack::ann}!HAS\_MEM\_FUNC@{HAS\_MEM\_FUNC}}
\index{HAS\_MEM\_FUNC@{HAS\_MEM\_FUNC}!mlpack::ann@{mlpack::ann}}
\doxysubsubsection{HAS\_MEM\_FUNC()\hspace{0.1cm}{\footnotesize\ttfamily [2/15]}}
{\footnotesize\ttfamily mlpack\+::ann\+::\+HAS\+\_\+\+MEM\+\_\+\+FUNC (\begin{DoxyParamCaption}\item[{Bias}]{,  }\item[{Has\+Bias\+Check}]{ }\end{DoxyParamCaption})}

\mbox{\label{namespacemlpack_1_1ann_ae95d86bb222cc89639472577da586357}} 
\index{mlpack::ann@{mlpack::ann}!HAS\_MEM\_FUNC@{HAS\_MEM\_FUNC}}
\index{HAS\_MEM\_FUNC@{HAS\_MEM\_FUNC}!mlpack::ann@{mlpack::ann}}
\doxysubsubsection{HAS\_MEM\_FUNC()\hspace{0.1cm}{\footnotesize\ttfamily [3/15]}}
{\footnotesize\ttfamily mlpack\+::ann\+::\+HAS\+\_\+\+MEM\+\_\+\+FUNC (\begin{DoxyParamCaption}\item[{Deterministic}]{,  }\item[{Has\+Deterministic\+Check}]{ }\end{DoxyParamCaption})}

\mbox{\label{namespacemlpack_1_1ann_ac1b6745deedbcee048f2387da27389d4}} 
\index{mlpack::ann@{mlpack::ann}!HAS\_MEM\_FUNC@{HAS\_MEM\_FUNC}}
\index{HAS\_MEM\_FUNC@{HAS\_MEM\_FUNC}!mlpack::ann@{mlpack::ann}}
\doxysubsubsection{HAS\_MEM\_FUNC()\hspace{0.1cm}{\footnotesize\ttfamily [4/15]}}
{\footnotesize\ttfamily mlpack\+::ann\+::\+HAS\+\_\+\+MEM\+\_\+\+FUNC (\begin{DoxyParamCaption}\item[{Gradient}]{,  }\item[{Has\+Gradient\+Check}]{ }\end{DoxyParamCaption})}

\mbox{\label{namespacemlpack_1_1ann_addfd94f5ac2aa2225484ddddb06b8320}} 
\index{mlpack::ann@{mlpack::ann}!HAS\_MEM\_FUNC@{HAS\_MEM\_FUNC}}
\index{HAS\_MEM\_FUNC@{HAS\_MEM\_FUNC}!mlpack::ann@{mlpack::ann}}
\doxysubsubsection{HAS\_MEM\_FUNC()\hspace{0.1cm}{\footnotesize\ttfamily [5/15]}}
{\footnotesize\ttfamily mlpack\+::ann\+::\+HAS\+\_\+\+MEM\+\_\+\+FUNC (\begin{DoxyParamCaption}\item[{Input\+Height}]{,  }\item[{Has\+Input\+Height}]{ }\end{DoxyParamCaption})}

\mbox{\label{namespacemlpack_1_1ann_a4dfcd41ff0d3c6ea37dda6c9a35c832f}} 
\index{mlpack::ann@{mlpack::ann}!HAS\_MEM\_FUNC@{HAS\_MEM\_FUNC}}
\index{HAS\_MEM\_FUNC@{HAS\_MEM\_FUNC}!mlpack::ann@{mlpack::ann}}
\doxysubsubsection{HAS\_MEM\_FUNC()\hspace{0.1cm}{\footnotesize\ttfamily [6/15]}}
{\footnotesize\ttfamily mlpack\+::ann\+::\+HAS\+\_\+\+MEM\+\_\+\+FUNC (\begin{DoxyParamCaption}\item[{Input\+Width}]{,  }\item[{Has\+Input\+Width}]{ }\end{DoxyParamCaption})}

\mbox{\label{namespacemlpack_1_1ann_a9ddaef84cd236998b57624b9b4d2eebe}} 
\index{mlpack::ann@{mlpack::ann}!HAS\_MEM\_FUNC@{HAS\_MEM\_FUNC}}
\index{HAS\_MEM\_FUNC@{HAS\_MEM\_FUNC}!mlpack::ann@{mlpack::ann}}
\doxysubsubsection{HAS\_MEM\_FUNC()\hspace{0.1cm}{\footnotesize\ttfamily [7/15]}}
{\footnotesize\ttfamily mlpack\+::ann\+::\+HAS\+\_\+\+MEM\+\_\+\+FUNC (\begin{DoxyParamCaption}\item[{Location}]{,  }\item[{Has\+Location\+Check}]{ }\end{DoxyParamCaption})}

\mbox{\label{namespacemlpack_1_1ann_af302c82cfb8bb5c0871c8a876e70adcc}} 
\index{mlpack::ann@{mlpack::ann}!HAS\_MEM\_FUNC@{HAS\_MEM\_FUNC}}
\index{HAS\_MEM\_FUNC@{HAS\_MEM\_FUNC}!mlpack::ann@{mlpack::ann}}
\doxysubsubsection{HAS\_MEM\_FUNC()\hspace{0.1cm}{\footnotesize\ttfamily [8/15]}}
{\footnotesize\ttfamily mlpack\+::ann\+::\+HAS\+\_\+\+MEM\+\_\+\+FUNC (\begin{DoxyParamCaption}\item[{Loss}]{,  }\item[{Has\+Loss}]{ }\end{DoxyParamCaption})}

\mbox{\label{namespacemlpack_1_1ann_a8e7950714181dc8adf55752f45467dd8}} 
\index{mlpack::ann@{mlpack::ann}!HAS\_MEM\_FUNC@{HAS\_MEM\_FUNC}}
\index{HAS\_MEM\_FUNC@{HAS\_MEM\_FUNC}!mlpack::ann@{mlpack::ann}}
\doxysubsubsection{HAS\_MEM\_FUNC()\hspace{0.1cm}{\footnotesize\ttfamily [9/15]}}
{\footnotesize\ttfamily mlpack\+::ann\+::\+HAS\+\_\+\+MEM\+\_\+\+FUNC (\begin{DoxyParamCaption}\item[{Max\+Iterations}]{,  }\item[{Has\+Max\+Iterations}]{ }\end{DoxyParamCaption})}

\mbox{\label{namespacemlpack_1_1ann_add5ad48dbc07b098c8df806a7d100de7}} 
\index{mlpack::ann@{mlpack::ann}!HAS\_MEM\_FUNC@{HAS\_MEM\_FUNC}}
\index{HAS\_MEM\_FUNC@{HAS\_MEM\_FUNC}!mlpack::ann@{mlpack::ann}}
\doxysubsubsection{HAS\_MEM\_FUNC()\hspace{0.1cm}{\footnotesize\ttfamily [10/15]}}
{\footnotesize\ttfamily mlpack\+::ann\+::\+HAS\+\_\+\+MEM\+\_\+\+FUNC (\begin{DoxyParamCaption}\item[{Parameters}]{,  }\item[{Has\+Parameters\+Check}]{ }\end{DoxyParamCaption})}

\mbox{\label{namespacemlpack_1_1ann_a7af914cacab417f183e2fc0051a5345a}} 
\index{mlpack::ann@{mlpack::ann}!HAS\_MEM\_FUNC@{HAS\_MEM\_FUNC}}
\index{HAS\_MEM\_FUNC@{HAS\_MEM\_FUNC}!mlpack::ann@{mlpack::ann}}
\doxysubsubsection{HAS\_MEM\_FUNC()\hspace{0.1cm}{\footnotesize\ttfamily [11/15]}}
{\footnotesize\ttfamily mlpack\+::ann\+::\+HAS\+\_\+\+MEM\+\_\+\+FUNC (\begin{DoxyParamCaption}\item[{Reset}]{,  }\item[{Has\+Reset\+Check}]{ }\end{DoxyParamCaption})}

\mbox{\label{namespacemlpack_1_1ann_a5278fc5426da6ac56df6540dabd508e8}} 
\index{mlpack::ann@{mlpack::ann}!HAS\_MEM\_FUNC@{HAS\_MEM\_FUNC}}
\index{HAS\_MEM\_FUNC@{HAS\_MEM\_FUNC}!mlpack::ann@{mlpack::ann}}
\doxysubsubsection{HAS\_MEM\_FUNC()\hspace{0.1cm}{\footnotesize\ttfamily [12/15]}}
{\footnotesize\ttfamily mlpack\+::ann\+::\+HAS\+\_\+\+MEM\+\_\+\+FUNC (\begin{DoxyParamCaption}\item[{Reset\+Cell}]{,  }\item[{Has\+Reset\+Cell\+Check}]{ }\end{DoxyParamCaption})}

\mbox{\label{namespacemlpack_1_1ann_ad4b16a6a10d1b1d3999d177f03b1f4a0}} 
\index{mlpack::ann@{mlpack::ann}!HAS\_MEM\_FUNC@{HAS\_MEM\_FUNC}}
\index{HAS\_MEM\_FUNC@{HAS\_MEM\_FUNC}!mlpack::ann@{mlpack::ann}}
\doxysubsubsection{HAS\_MEM\_FUNC()\hspace{0.1cm}{\footnotesize\ttfamily [13/15]}}
{\footnotesize\ttfamily mlpack\+::ann\+::\+HAS\+\_\+\+MEM\+\_\+\+FUNC (\begin{DoxyParamCaption}\item[{Reward}]{,  }\item[{Has\+Reward\+Check}]{ }\end{DoxyParamCaption})}

\mbox{\label{namespacemlpack_1_1ann_a1e25664538ca94074ff5636b85902c8a}} 
\index{mlpack::ann@{mlpack::ann}!HAS\_MEM\_FUNC@{HAS\_MEM\_FUNC}}
\index{HAS\_MEM\_FUNC@{HAS\_MEM\_FUNC}!mlpack::ann@{mlpack::ann}}
\doxysubsubsection{HAS\_MEM\_FUNC()\hspace{0.1cm}{\footnotesize\ttfamily [14/15]}}
{\footnotesize\ttfamily mlpack\+::ann\+::\+HAS\+\_\+\+MEM\+\_\+\+FUNC (\begin{DoxyParamCaption}\item[{Rho}]{,  }\item[{Has\+Rho}]{ }\end{DoxyParamCaption})}

\mbox{\label{namespacemlpack_1_1ann_af4586b834d3c6bd15695550314404738}} 
\index{mlpack::ann@{mlpack::ann}!HAS\_MEM\_FUNC@{HAS\_MEM\_FUNC}}
\index{HAS\_MEM\_FUNC@{HAS\_MEM\_FUNC}!mlpack::ann@{mlpack::ann}}
\doxysubsubsection{HAS\_MEM\_FUNC()\hspace{0.1cm}{\footnotesize\ttfamily [15/15]}}
{\footnotesize\ttfamily mlpack\+::ann\+::\+HAS\+\_\+\+MEM\+\_\+\+FUNC (\begin{DoxyParamCaption}\item[{Run}]{,  }\item[{Has\+Run\+Check}]{ }\end{DoxyParamCaption})}

\mbox{\label{namespacemlpack_1_1ann_ad1c987f983baef10e712bde7a3f36c98}} 
\index{mlpack::ann@{mlpack::ann}!InceptionScore@{InceptionScore}}
\index{InceptionScore@{InceptionScore}!mlpack::ann@{mlpack::ann}}
\doxysubsubsection{InceptionScore()}
{\footnotesize\ttfamily double mlpack\+::ann\+::\+Inception\+Score (\begin{DoxyParamCaption}\item[{Model\+Type}]{Model,  }\item[{arma\+::mat}]{images,  }\item[{size\+\_\+t}]{splits = {\ttfamily 1} }\end{DoxyParamCaption})}



Function that computes Inception Score for a set of images produced by a \doxyref{GAN}{p.}{classmlpack_1_1ann_1_1GAN}. 

For more information, see the following.


\begin{DoxyCode}{0}
\DoxyCodeLine{@article\{Goodfellow2016,}
\DoxyCodeLine{  author  = \{Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung,}
\DoxyCodeLine{             Alec Radford, Xi Chen\},}
\DoxyCodeLine{  title   = \{Improved Techniques \textcolor{keywordflow}{for} Training GANs\},}
\DoxyCodeLine{  year    = \{2016\},}
\DoxyCodeLine{  url     = \{https:\textcolor{comment}{//arxiv.org/abs/1606.03498\},}}
\DoxyCodeLine{\}}

\end{DoxyCode}



\begin{DoxyParams}{Parameters}
{\em Model} & Model for evaluating the quality of images. \\
\hline
{\em images} & Images generated by \doxyref{GAN}{p.}{classmlpack_1_1ann_1_1GAN}. \\
\hline
{\em splits} & Number of splits to perform (default\+: 1). \\
\hline
\end{DoxyParams}
