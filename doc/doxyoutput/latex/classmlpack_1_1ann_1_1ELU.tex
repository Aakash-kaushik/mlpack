\doxysection{ELU$<$ Input\+Data\+Type, Output\+Data\+Type $>$ Class Template Reference}
\label{classmlpack_1_1ann_1_1ELU}\index{ELU$<$ InputDataType, OutputDataType $>$@{ELU$<$ InputDataType, OutputDataType $>$}}


The \doxyref{ELU}{p.}{classmlpack_1_1ann_1_1ELU} activation function, defined by.  


\doxysubsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
\textbf{ ELU} ()
\begin{DoxyCompactList}\small\item\em Create the \doxyref{ELU}{p.}{classmlpack_1_1ann_1_1ELU} object. \end{DoxyCompactList}\item 
\textbf{ ELU} (const double alpha)
\begin{DoxyCompactList}\small\item\em Create the \doxyref{ELU}{p.}{classmlpack_1_1ann_1_1ELU} object using the specified parameter. \end{DoxyCompactList}\item 
double \& \textbf{ Alpha} ()
\begin{DoxyCompactList}\small\item\em Modify the non zero gradient. \end{DoxyCompactList}\item 
double const  \& \textbf{ Alpha} () const
\begin{DoxyCompactList}\small\item\em Get the non zero gradient. \end{DoxyCompactList}\item 
{\footnotesize template$<$typename Data\+Type $>$ }\\void \textbf{ Backward} (const Data\+Type \&input, const Data\+Type \&gy, Data\+Type \&g)
\begin{DoxyCompactList}\small\item\em Ordinary feed backward pass of a neural network, calculating the function f(x) by propagating x backwards through f. \end{DoxyCompactList}\item 
Output\+Data\+Type \& \textbf{ Delta} ()
\begin{DoxyCompactList}\small\item\em Modify the delta. \end{DoxyCompactList}\item 
Output\+Data\+Type const  \& \textbf{ Delta} () const
\begin{DoxyCompactList}\small\item\em Get the delta. \end{DoxyCompactList}\item 
bool \& \textbf{ Deterministic} ()
\begin{DoxyCompactList}\small\item\em Modify the value of deterministic parameter. \end{DoxyCompactList}\item 
bool \textbf{ Deterministic} () const
\begin{DoxyCompactList}\small\item\em Get the value of deterministic parameter. \end{DoxyCompactList}\item 
{\footnotesize template$<$typename Input\+Type , typename Output\+Type $>$ }\\void \textbf{ Forward} (const Input\+Type \&input, Output\+Type \&output)
\begin{DoxyCompactList}\small\item\em Ordinary feed forward pass of a neural network, evaluating the function f(x) by propagating the activity forward through f. \end{DoxyCompactList}\item 
double const  \& \textbf{ Lambda} () const
\begin{DoxyCompactList}\small\item\em Get the lambda parameter. \end{DoxyCompactList}\item 
Output\+Data\+Type \& \textbf{ Output\+Parameter} ()
\begin{DoxyCompactList}\small\item\em Modify the output parameter. \end{DoxyCompactList}\item 
Output\+Data\+Type const  \& \textbf{ Output\+Parameter} () const
\begin{DoxyCompactList}\small\item\em Get the output parameter. \end{DoxyCompactList}\item 
{\footnotesize template$<$typename Archive $>$ }\\void \textbf{ serialize} (Archive \&ar, const uint32\+\_\+t)
\begin{DoxyCompactList}\small\item\em Serialize the layer. \end{DoxyCompactList}\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
\subsubsection*{template$<$typename Input\+Data\+Type = arma\+::mat, typename Output\+Data\+Type = arma\+::mat$>$\newline
class mlpack\+::ann\+::\+ELU$<$ Input\+Data\+Type, Output\+Data\+Type $>$}

The \doxyref{ELU}{p.}{classmlpack_1_1ann_1_1ELU} activation function, defined by. 

\begin{eqnarray*} f(x) &=& \left\{ \begin{array}{lr} x & : x > 0 \\ \alpha(e^x - 1) & : x \le 0 \end{array} \right. \\ f\textnormal{\textquotesingle}(x) &=& \left\{ \begin{array}{lr} 1 & : x > 0 \\ f(x) + \alpha & : x \le 0 \end{array} \right. \end{eqnarray*}

For more information, read the following paper\+:


\begin{DoxyCode}{0}
\DoxyCodeLine{@article\{Clevert2015,}
\DoxyCodeLine{  author  = \{Djork\{-\/\}Arn\{\(\backslash\)\textcolor{stringliteral}{'\{e\}\} Clevert and Thomas Unterthiner and}}
\DoxyCodeLine{\textcolor{stringliteral}{             Sepp Hochreiter\},}}
\DoxyCodeLine{\textcolor{stringliteral}{  title   = \{Fast and Accurate Deep Network Learning by Exponential Linear}}
\DoxyCodeLine{\textcolor{stringliteral}{             Units (ELUs)\},}}
\DoxyCodeLine{\textcolor{stringliteral}{  journal = \{CoRR\},}}
\DoxyCodeLine{\textcolor{stringliteral}{  year    = \{2015\},}}
\DoxyCodeLine{\textcolor{stringliteral}{  url     = \{https://arxiv.org/abs/1511.07289\}}}
\DoxyCodeLine{\textcolor{stringliteral}{\}}}

\end{DoxyCode}


The SELU activation function is defined by

\begin{eqnarray*} f(x) &=& \left\{ \begin{array}{lr} \lambda * x & : x > 0 \\ \lambda * \alpha(e^x - 1) & : x \le 0 \end{array} \right. \\ f\textnormal{\textquotesingle}(x) &=& \left\{ \begin{array}{lr} \lambda & : x > 0 \\ f(x) + \lambda * \alpha & : x \le 0 \end{array} \right. \end{eqnarray*}

For more information, read the following paper\+:


\begin{DoxyCode}{0}
\DoxyCodeLine{@article\{Klambauer2017,}
\DoxyCodeLine{  author  = \{Gunter Klambauer and Thomas Unterthiner and}
\DoxyCodeLine{             Andreas Mayr\},}
\DoxyCodeLine{  title   = \{Self-\/Normalizing Neural Networks\},}
\DoxyCodeLine{  journal = \{Advances in Neural Information Processing Systems\},}
\DoxyCodeLine{  year    = \{2017\},}
\DoxyCodeLine{  url = \{https:\textcolor{comment}{//arxiv.org/abs/1706.02515\}}}
\DoxyCodeLine{\}}

\end{DoxyCode}


In the deterministic mode, there is no computation of the derivative.

\begin{DoxyNote}{Note}
During training deterministic should be set to false and during testing/inference deterministic should be set to true. 

Make sure to use SELU activation function with normalized inputs and weights initialized with Lecun Normal Initialization.
\end{DoxyNote}

\begin{DoxyTemplParams}{Template Parameters}
{\em Input\+Data\+Type} & Type of the input data (arma\+::colvec, arma\+::mat, arma\+::sp\+\_\+mat or arma\+::cube). \\
\hline
{\em Output\+Data\+Type} & Type of the output data (arma\+::colvec, arma\+::mat, arma\+::sp\+\_\+mat or arma\+::cube). \\
\hline
\end{DoxyTemplParams}


Definition at line 111 of file elu.\+hpp.



\doxysubsection{Constructor \& Destructor Documentation}
\mbox{\label{classmlpack_1_1ann_1_1ELU_ac74bf903a3d27e91aaf80e59e3812624}} 
\index{ELU$<$ InputDataType, OutputDataType $>$@{ELU$<$ InputDataType, OutputDataType $>$}!ELU@{ELU}}
\index{ELU@{ELU}!ELU$<$ InputDataType, OutputDataType $>$@{ELU$<$ InputDataType, OutputDataType $>$}}
\doxysubsubsection{ELU()\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily \textbf{ ELU} (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})}



Create the \doxyref{ELU}{p.}{classmlpack_1_1ann_1_1ELU} object. 

NOTE\+: Use this constructor for SELU activation function. \mbox{\label{classmlpack_1_1ann_1_1ELU_a2be97a9ea26474ca497b24ac1b1a9323}} 
\index{ELU$<$ InputDataType, OutputDataType $>$@{ELU$<$ InputDataType, OutputDataType $>$}!ELU@{ELU}}
\index{ELU@{ELU}!ELU$<$ InputDataType, OutputDataType $>$@{ELU$<$ InputDataType, OutputDataType $>$}}
\doxysubsubsection{ELU()\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily \textbf{ ELU} (\begin{DoxyParamCaption}\item[{const double}]{alpha }\end{DoxyParamCaption})}



Create the \doxyref{ELU}{p.}{classmlpack_1_1ann_1_1ELU} object using the specified parameter. 

The non zero gradient for negative inputs can be adjusted by specifying the \doxyref{ELU}{p.}{classmlpack_1_1ann_1_1ELU} hyperparameter alpha (alpha $>$ 0).

\begin{DoxyNote}{Note}
Use this constructor for \doxyref{ELU}{p.}{classmlpack_1_1ann_1_1ELU} activation function. 
\end{DoxyNote}

\begin{DoxyParams}{Parameters}
{\em alpha} & Scale parameter for the negative factor. \\
\hline
\end{DoxyParams}


\doxysubsection{Member Function Documentation}
\mbox{\label{classmlpack_1_1ann_1_1ELU_acbb0e4747a3a307bee88bad71e5eeaf1}} 
\index{ELU$<$ InputDataType, OutputDataType $>$@{ELU$<$ InputDataType, OutputDataType $>$}!Alpha@{Alpha}}
\index{Alpha@{Alpha}!ELU$<$ InputDataType, OutputDataType $>$@{ELU$<$ InputDataType, OutputDataType $>$}}
\doxysubsubsection{Alpha()\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily double\& Alpha (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Modify the non zero gradient. 



Definition at line 166 of file elu.\+hpp.

\mbox{\label{classmlpack_1_1ann_1_1ELU_a21679485637bdec3078ec74d71572980}} 
\index{ELU$<$ InputDataType, OutputDataType $>$@{ELU$<$ InputDataType, OutputDataType $>$}!Alpha@{Alpha}}
\index{Alpha@{Alpha}!ELU$<$ InputDataType, OutputDataType $>$@{ELU$<$ InputDataType, OutputDataType $>$}}
\doxysubsubsection{Alpha()\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily double const\& Alpha (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const\hspace{0.3cm}{\ttfamily [inline]}}



Get the non zero gradient. 



Definition at line 164 of file elu.\+hpp.

\mbox{\label{classmlpack_1_1ann_1_1ELU_aef8c56f1f8624bd006afec8b3bcda9d6}} 
\index{ELU$<$ InputDataType, OutputDataType $>$@{ELU$<$ InputDataType, OutputDataType $>$}!Backward@{Backward}}
\index{Backward@{Backward}!ELU$<$ InputDataType, OutputDataType $>$@{ELU$<$ InputDataType, OutputDataType $>$}}
\doxysubsubsection{Backward()}
{\footnotesize\ttfamily void Backward (\begin{DoxyParamCaption}\item[{const Data\+Type \&}]{input,  }\item[{const Data\+Type \&}]{gy,  }\item[{Data\+Type \&}]{g }\end{DoxyParamCaption})}



Ordinary feed backward pass of a neural network, calculating the function f(x) by propagating x backwards through f. 

Using the results from the feed forward pass.


\begin{DoxyParams}{Parameters}
{\em input} & The propagated input activation f(x). \\
\hline
{\em gy} & The backpropagated error. \\
\hline
{\em g} & The calculated gradient. \\
\hline
\end{DoxyParams}
\mbox{\label{classmlpack_1_1ann_1_1ELU_ad6601342d560219ce951d554e69e5e87}} 
\index{ELU$<$ InputDataType, OutputDataType $>$@{ELU$<$ InputDataType, OutputDataType $>$}!Delta@{Delta}}
\index{Delta@{Delta}!ELU$<$ InputDataType, OutputDataType $>$@{ELU$<$ InputDataType, OutputDataType $>$}}
\doxysubsubsection{Delta()\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily Output\+Data\+Type\& Delta (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Modify the delta. 



Definition at line 161 of file elu.\+hpp.

\mbox{\label{classmlpack_1_1ann_1_1ELU_a797f7edb44dd081e5e2b3cc316eef6bd}} 
\index{ELU$<$ InputDataType, OutputDataType $>$@{ELU$<$ InputDataType, OutputDataType $>$}!Delta@{Delta}}
\index{Delta@{Delta}!ELU$<$ InputDataType, OutputDataType $>$@{ELU$<$ InputDataType, OutputDataType $>$}}
\doxysubsubsection{Delta()\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily Output\+Data\+Type const\& Delta (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const\hspace{0.3cm}{\ttfamily [inline]}}



Get the delta. 



Definition at line 159 of file elu.\+hpp.

\mbox{\label{classmlpack_1_1ann_1_1ELU_a42d4ee3da432cff20d3a41b8b1ec801c}} 
\index{ELU$<$ InputDataType, OutputDataType $>$@{ELU$<$ InputDataType, OutputDataType $>$}!Deterministic@{Deterministic}}
\index{Deterministic@{Deterministic}!ELU$<$ InputDataType, OutputDataType $>$@{ELU$<$ InputDataType, OutputDataType $>$}}
\doxysubsubsection{Deterministic()\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily bool\& Deterministic (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Modify the value of deterministic parameter. 



Definition at line 171 of file elu.\+hpp.

\mbox{\label{classmlpack_1_1ann_1_1ELU_a9f4103707f4d199ce5594d239b60443e}} 
\index{ELU$<$ InputDataType, OutputDataType $>$@{ELU$<$ InputDataType, OutputDataType $>$}!Deterministic@{Deterministic}}
\index{Deterministic@{Deterministic}!ELU$<$ InputDataType, OutputDataType $>$@{ELU$<$ InputDataType, OutputDataType $>$}}
\doxysubsubsection{Deterministic()\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily bool Deterministic (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const\hspace{0.3cm}{\ttfamily [inline]}}



Get the value of deterministic parameter. 



Definition at line 169 of file elu.\+hpp.

\mbox{\label{classmlpack_1_1ann_1_1ELU_a09440df0a90bdcc766e56e097d91205b}} 
\index{ELU$<$ InputDataType, OutputDataType $>$@{ELU$<$ InputDataType, OutputDataType $>$}!Forward@{Forward}}
\index{Forward@{Forward}!ELU$<$ InputDataType, OutputDataType $>$@{ELU$<$ InputDataType, OutputDataType $>$}}
\doxysubsubsection{Forward()}
{\footnotesize\ttfamily void Forward (\begin{DoxyParamCaption}\item[{const Input\+Type \&}]{input,  }\item[{Output\+Type \&}]{output }\end{DoxyParamCaption})}



Ordinary feed forward pass of a neural network, evaluating the function f(x) by propagating the activity forward through f. 


\begin{DoxyParams}{Parameters}
{\em input} & Input data used for evaluating the specified function. \\
\hline
{\em output} & Resulting output activation. \\
\hline
\end{DoxyParams}
\mbox{\label{classmlpack_1_1ann_1_1ELU_acb669457ad59e62d0fccc5bae3a6c35e}} 
\index{ELU$<$ InputDataType, OutputDataType $>$@{ELU$<$ InputDataType, OutputDataType $>$}!Lambda@{Lambda}}
\index{Lambda@{Lambda}!ELU$<$ InputDataType, OutputDataType $>$@{ELU$<$ InputDataType, OutputDataType $>$}}
\doxysubsubsection{Lambda()}
{\footnotesize\ttfamily double const\& Lambda (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const\hspace{0.3cm}{\ttfamily [inline]}}



Get the lambda parameter. 



Definition at line 174 of file elu.\+hpp.

\mbox{\label{classmlpack_1_1ann_1_1ELU_a21d5f745f02c709625a4ee0907f004a5}} 
\index{ELU$<$ InputDataType, OutputDataType $>$@{ELU$<$ InputDataType, OutputDataType $>$}!OutputParameter@{OutputParameter}}
\index{OutputParameter@{OutputParameter}!ELU$<$ InputDataType, OutputDataType $>$@{ELU$<$ InputDataType, OutputDataType $>$}}
\doxysubsubsection{OutputParameter()\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily Output\+Data\+Type\& Output\+Parameter (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Modify the output parameter. 



Definition at line 156 of file elu.\+hpp.

\mbox{\label{classmlpack_1_1ann_1_1ELU_a0ee21c2a36e5abad1e7a9d5dd00849f9}} 
\index{ELU$<$ InputDataType, OutputDataType $>$@{ELU$<$ InputDataType, OutputDataType $>$}!OutputParameter@{OutputParameter}}
\index{OutputParameter@{OutputParameter}!ELU$<$ InputDataType, OutputDataType $>$@{ELU$<$ InputDataType, OutputDataType $>$}}
\doxysubsubsection{OutputParameter()\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily Output\+Data\+Type const\& Output\+Parameter (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const\hspace{0.3cm}{\ttfamily [inline]}}



Get the output parameter. 



Definition at line 154 of file elu.\+hpp.

\mbox{\label{classmlpack_1_1ann_1_1ELU_a65cba07328997659bec80b9879b15a51}} 
\index{ELU$<$ InputDataType, OutputDataType $>$@{ELU$<$ InputDataType, OutputDataType $>$}!serialize@{serialize}}
\index{serialize@{serialize}!ELU$<$ InputDataType, OutputDataType $>$@{ELU$<$ InputDataType, OutputDataType $>$}}
\doxysubsubsection{serialize()}
{\footnotesize\ttfamily void serialize (\begin{DoxyParamCaption}\item[{Archive \&}]{ar,  }\item[{const uint32\+\_\+t}]{ }\end{DoxyParamCaption})}



Serialize the layer. 



The documentation for this class was generated from the following file\+:\begin{DoxyCompactItemize}
\item 
/home/aakash/mlpack/src/mlpack/methods/ann/layer/\textbf{ elu.\+hpp}\end{DoxyCompactItemize}
