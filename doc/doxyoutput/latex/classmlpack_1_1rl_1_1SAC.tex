\doxysection{SAC$<$ Environment\+Type, QNetwork\+Type, Policy\+Network\+Type, Updater\+Type, Replay\+Type $>$ Class Template Reference}
\label{classmlpack_1_1rl_1_1SAC}\index{SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$@{SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$}}


Implementation of Soft Actor-\/\+Critic, a model-\/free off-\/policy actor-\/critic based deep reinforcement learning algorithm.  


\doxysubsection*{Public Types}
\begin{DoxyCompactItemize}
\item 
using \textbf{ Action\+Type} = typename Environment\+Type\+::\+Action
\begin{DoxyCompactList}\small\item\em Convenient typedef for action. \end{DoxyCompactList}\item 
using \textbf{ State\+Type} = typename Environment\+Type\+::\+State
\begin{DoxyCompactList}\small\item\em Convenient typedef for state. \end{DoxyCompactList}\end{DoxyCompactItemize}
\doxysubsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
\textbf{ SAC} (\textbf{ Training\+Config} \&config, QNetwork\+Type \&learning\+Q1\+Network, Policy\+Network\+Type \&policy\+Network, Replay\+Type \&replay\+Method, Updater\+Type q\+Network\+Updater=Updater\+Type(), Updater\+Type policy\+Network\+Updater=Updater\+Type(), Environment\+Type environment=Environment\+Type())
\begin{DoxyCompactList}\small\item\em Create the \doxyref{SAC}{p.}{classmlpack_1_1rl_1_1SAC} object with given settings. \end{DoxyCompactList}\item 
\textbf{ $\sim$\+SAC} ()
\begin{DoxyCompactList}\small\item\em Clean memory. \end{DoxyCompactList}\item 
const \textbf{ Action\+Type} \& \textbf{ Action} () const
\begin{DoxyCompactList}\small\item\em Get the action of the agent. \end{DoxyCompactList}\item 
bool \& \textbf{ Deterministic} ()
\begin{DoxyCompactList}\small\item\em Modify the training mode / test mode indicator. \end{DoxyCompactList}\item 
const bool \& \textbf{ Deterministic} () const
\begin{DoxyCompactList}\small\item\em Get the indicator of training mode / test mode. \end{DoxyCompactList}\item 
double \textbf{ Episode} ()
\begin{DoxyCompactList}\small\item\em Execute an episode. \end{DoxyCompactList}\item 
void \textbf{ Select\+Action} ()
\begin{DoxyCompactList}\small\item\em Select an action, given an agent. \end{DoxyCompactList}\item 
void \textbf{ Soft\+Update} (double rho)
\begin{DoxyCompactList}\small\item\em Softly update the learning Q network parameters to the target Q network parameters. \end{DoxyCompactList}\item 
\textbf{ State\+Type} \& \textbf{ State} ()
\begin{DoxyCompactList}\small\item\em Modify the state of the agent. \end{DoxyCompactList}\item 
const \textbf{ State\+Type} \& \textbf{ State} () const
\begin{DoxyCompactList}\small\item\em Get the state of the agent. \end{DoxyCompactList}\item 
size\+\_\+t \& \textbf{ Total\+Steps} ()
\begin{DoxyCompactList}\small\item\em Modify total steps from beginning. \end{DoxyCompactList}\item 
const size\+\_\+t \& \textbf{ Total\+Steps} () const
\begin{DoxyCompactList}\small\item\em Get total steps from beginning. \end{DoxyCompactList}\item 
void \textbf{ Update} ()
\begin{DoxyCompactList}\small\item\em Update the Q and policy networks. \end{DoxyCompactList}\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
\subsubsection*{template$<$typename Environment\+Type, typename QNetwork\+Type, typename Policy\+Network\+Type, typename Updater\+Type, typename Replay\+Type = Random\+Replay$<$\+Environment\+Type$>$$>$\newline
class mlpack\+::rl\+::\+SAC$<$ Environment\+Type, QNetwork\+Type, Policy\+Network\+Type, Updater\+Type, Replay\+Type $>$}

Implementation of Soft Actor-\/\+Critic, a model-\/free off-\/policy actor-\/critic based deep reinforcement learning algorithm. 

For more details, see the following\+: 
\begin{DoxyCode}{0}
\DoxyCodeLine{@misc\{haarnoja2018soft,}
\DoxyCodeLine{ author    = \{Tuomas Haarnoja and}
\DoxyCodeLine{              Aurick Zhou and}
\DoxyCodeLine{              Kristian Hartikainen and}
\DoxyCodeLine{              George Tucker and}
\DoxyCodeLine{              Sehoon Ha and}
\DoxyCodeLine{              Jie Tan and}
\DoxyCodeLine{              Vikash Kumar and}
\DoxyCodeLine{              Henry Zhu and}
\DoxyCodeLine{              Abhishek Gupta and}
\DoxyCodeLine{              Pieter Abbeel and}
\DoxyCodeLine{              Sergey Levine\},}
\DoxyCodeLine{ title     = \{Soft Actor-\/Critic Algorithms and Applications\},}
\DoxyCodeLine{ year      = \{2018\},}
\DoxyCodeLine{ url       = \{https:\textcolor{comment}{//arxiv.org/abs/1812.05905\}}}
\DoxyCodeLine{\}}

\end{DoxyCode}



\begin{DoxyTemplParams}{Template Parameters}
{\em Environment\+Type} & The environment of the reinforcement learning task. \\
\hline
{\em Network\+Type} & The network to compute action value. \\
\hline
{\em Updater\+Type} & How to apply gradients when training. \\
\hline
{\em Replay\+Type} & Experience replay method. \\
\hline
\end{DoxyTemplParams}


Definition at line 63 of file sac.\+hpp.



\doxysubsection{Member Typedef Documentation}
\mbox{\label{classmlpack_1_1rl_1_1SAC_aaf7b2dc5d49d01961601c7c16be76777}} 
\index{SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$@{SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$}!ActionType@{ActionType}}
\index{ActionType@{ActionType}!SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$@{SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$}}
\doxysubsubsection{ActionType}
{\footnotesize\ttfamily using \textbf{ Action\+Type} =  typename Environment\+Type\+::\+Action}



Convenient typedef for action. 



Definition at line 70 of file sac.\+hpp.

\mbox{\label{classmlpack_1_1rl_1_1SAC_ada68ef405b7c331a2bee337614f00088}} 
\index{SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$@{SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$}!StateType@{StateType}}
\index{StateType@{StateType}!SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$@{SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$}}
\doxysubsubsection{StateType}
{\footnotesize\ttfamily using \textbf{ State\+Type} =  typename Environment\+Type\+::\+State}



Convenient typedef for state. 



Definition at line 67 of file sac.\+hpp.



\doxysubsection{Constructor \& Destructor Documentation}
\mbox{\label{classmlpack_1_1rl_1_1SAC_a382013c48f00c0cd5e682edb92a01f16}} 
\index{SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$@{SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$}!SAC@{SAC}}
\index{SAC@{SAC}!SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$@{SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$}}
\doxysubsubsection{SAC()}
{\footnotesize\ttfamily \textbf{ SAC} (\begin{DoxyParamCaption}\item[{\textbf{ Training\+Config} \&}]{config,  }\item[{QNetwork\+Type \&}]{learning\+Q1\+Network,  }\item[{Policy\+Network\+Type \&}]{policy\+Network,  }\item[{Replay\+Type \&}]{replay\+Method,  }\item[{Updater\+Type}]{q\+Network\+Updater = {\ttfamily UpdaterType()},  }\item[{Updater\+Type}]{policy\+Network\+Updater = {\ttfamily UpdaterType()},  }\item[{Environment\+Type}]{environment = {\ttfamily EnvironmentType()} }\end{DoxyParamCaption})}



Create the \doxyref{SAC}{p.}{classmlpack_1_1rl_1_1SAC} object with given settings. 

If you want to pass in a parameter and discard the original parameter object, you can directly pass the parameter, as the constructor takes a reference. This avoids unnecessary copy.


\begin{DoxyParams}{Parameters}
{\em config} & Hyper-\/parameters for training. \\
\hline
{\em learning\+Q1\+Network} & The network to compute action value. \\
\hline
{\em policy\+Network} & The network to produce an action given a state. \\
\hline
{\em replay\+Method} & Experience replay method. \\
\hline
{\em q\+Network\+Updater} & How to apply gradients to Q network when training. \\
\hline
{\em policy\+Network\+Updater} & How to apply gradients to policy network when training. \\
\hline
{\em environment} & Reinforcement learning task. \\
\hline
\end{DoxyParams}
\mbox{\label{classmlpack_1_1rl_1_1SAC_abf0c5202e5e579fc47c332f2490fbb8f}} 
\index{SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$@{SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$}!````~SAC@{$\sim$SAC}}
\index{````~SAC@{$\sim$SAC}!SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$@{SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$}}
\doxysubsubsection{$\sim$SAC()}
{\footnotesize\ttfamily $\sim$\textbf{ SAC} (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})}



Clean memory. 



\doxysubsection{Member Function Documentation}
\mbox{\label{classmlpack_1_1rl_1_1SAC_a0d32caed9517e5d2014238a22f78352d}} 
\index{SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$@{SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$}!Action@{Action}}
\index{Action@{Action}!SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$@{SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$}}
\doxysubsubsection{Action()}
{\footnotesize\ttfamily const \textbf{ Action\+Type}\& Action (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const\hspace{0.3cm}{\ttfamily [inline]}}



Get the action of the agent. 



Definition at line 136 of file sac.\+hpp.

\mbox{\label{classmlpack_1_1rl_1_1SAC_a42d4ee3da432cff20d3a41b8b1ec801c}} 
\index{SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$@{SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$}!Deterministic@{Deterministic}}
\index{Deterministic@{Deterministic}!SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$@{SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$}}
\doxysubsubsection{Deterministic()\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily bool\& Deterministic (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Modify the training mode / test mode indicator. 



Definition at line 139 of file sac.\+hpp.

\mbox{\label{classmlpack_1_1rl_1_1SAC_a5d262f7871c5cc8b532971fb644f0abf}} 
\index{SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$@{SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$}!Deterministic@{Deterministic}}
\index{Deterministic@{Deterministic}!SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$@{SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$}}
\doxysubsubsection{Deterministic()\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily const bool\& Deterministic (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const\hspace{0.3cm}{\ttfamily [inline]}}



Get the indicator of training mode / test mode. 



Definition at line 141 of file sac.\+hpp.

\mbox{\label{classmlpack_1_1rl_1_1SAC_a1fb26736f2d90010f882f9628cd26612}} 
\index{SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$@{SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$}!Episode@{Episode}}
\index{Episode@{Episode}!SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$@{SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$}}
\doxysubsubsection{Episode()}
{\footnotesize\ttfamily double Episode (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})}



Execute an episode. 

\begin{DoxyReturn}{Returns}
Return of the episode. 
\end{DoxyReturn}
\mbox{\label{classmlpack_1_1rl_1_1SAC_abd126acd7f564c8326dc765232624ae4}} 
\index{SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$@{SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$}!SelectAction@{SelectAction}}
\index{SelectAction@{SelectAction}!SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$@{SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$}}
\doxysubsubsection{SelectAction()}
{\footnotesize\ttfamily void Select\+Action (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})}



Select an action, given an agent. 

\mbox{\label{classmlpack_1_1rl_1_1SAC_a0e8b07b1eb04d72c36e95f795340d5c6}} 
\index{SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$@{SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$}!SoftUpdate@{SoftUpdate}}
\index{SoftUpdate@{SoftUpdate}!SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$@{SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$}}
\doxysubsubsection{SoftUpdate()}
{\footnotesize\ttfamily void Soft\+Update (\begin{DoxyParamCaption}\item[{double}]{rho }\end{DoxyParamCaption})}



Softly update the learning Q network parameters to the target Q network parameters. 


\begin{DoxyParams}{Parameters}
{\em rho} & How \char`\"{}softly\char`\"{} should the parameters be copied. \\
\hline
\end{DoxyParams}
\mbox{\label{classmlpack_1_1rl_1_1SAC_ad7a595de4a1a67da528603c20f80315f}} 
\index{SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$@{SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$}!State@{State}}
\index{State@{State}!SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$@{SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$}}
\doxysubsubsection{State()\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily \textbf{ State\+Type}\& State (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Modify the state of the agent. 



Definition at line 131 of file sac.\+hpp.

\mbox{\label{classmlpack_1_1rl_1_1SAC_afa3e388ae5e024c8ec49fd4d1ef725ad}} 
\index{SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$@{SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$}!State@{State}}
\index{State@{State}!SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$@{SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$}}
\doxysubsubsection{State()\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily const \textbf{ State\+Type}\& State (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const\hspace{0.3cm}{\ttfamily [inline]}}



Get the state of the agent. 



Definition at line 133 of file sac.\+hpp.

\mbox{\label{classmlpack_1_1rl_1_1SAC_abaf0bb243c2e643c57654b8e65058fa0}} 
\index{SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$@{SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$}!TotalSteps@{TotalSteps}}
\index{TotalSteps@{TotalSteps}!SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$@{SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$}}
\doxysubsubsection{TotalSteps()\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily size\+\_\+t\& Total\+Steps (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Modify total steps from beginning. 



Definition at line 126 of file sac.\+hpp.

\mbox{\label{classmlpack_1_1rl_1_1SAC_a689af4e6e564ab01f40e6ec49638bdaf}} 
\index{SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$@{SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$}!TotalSteps@{TotalSteps}}
\index{TotalSteps@{TotalSteps}!SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$@{SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$}}
\doxysubsubsection{TotalSteps()\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily const size\+\_\+t\& Total\+Steps (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const\hspace{0.3cm}{\ttfamily [inline]}}



Get total steps from beginning. 



Definition at line 128 of file sac.\+hpp.

\mbox{\label{classmlpack_1_1rl_1_1SAC_aec0783b5a136e042adcc47bae4fe5291}} 
\index{SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$@{SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$}!Update@{Update}}
\index{Update@{Update}!SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$@{SAC$<$ EnvironmentType, QNetworkType, PolicyNetworkType, UpdaterType, ReplayType $>$}}
\doxysubsubsection{Update()}
{\footnotesize\ttfamily void Update (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})}



Update the Q and policy networks. 



The documentation for this class was generated from the following file\+:\begin{DoxyCompactItemize}
\item 
/home/aakash/mlpack/src/mlpack/methods/reinforcement\+\_\+learning/\textbf{ sac.\+hpp}\end{DoxyCompactItemize}
