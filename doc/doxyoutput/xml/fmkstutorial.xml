<?xml version='1.0' encoding='UTF-8' standalone='no'?>
<doxygen xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="compound.xsd" version="1.9.1" xml:lang="en-US">
  <compounddef id="fmkstutorial" kind="page">
    <compoundname>fmkstutorial</compoundname>
    <title>Fast max-kernel search tutorial (fastmks)</title>
    <briefdescription>
    </briefdescription>
    <detaileddescription>
<sect1 id="fmkstutorial_1intro_fmkstut">
<title>Introduction</title>
<para>The FastMKS algorithm (fast exact max-kernel search) is a recent algorithm proposed in the following papers:</para>
<para><programlisting><codeline><highlight class="normal">@inproceedings{curtin2013fast,</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>title={Fast<sp/>Exact<sp/>Max-Kernel<sp/>Search},</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>author={Curtin,<sp/>Ryan<sp/>R.<sp/>and<sp/>Ram,<sp/>Parikshit<sp/>and<sp/>Gray,<sp/>Alexander<sp/>G.},</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>booktitle={Proceedings<sp/>of<sp/>the<sp/>2013<sp/>SIAM<sp/>International<sp/>Conference<sp/>on<sp/>Data</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/>Mining<sp/>(SDM<sp/>&apos;13)},</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>year={2013},</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>pages={1--9}</highlight></codeline>
<codeline><highlight class="normal">}</highlight></codeline>
<codeline></codeline>
<codeline><highlight class="normal">@article{curtin2014dual,</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>author<sp/>=<sp/>{Curtin,<sp/>Ryan<sp/>R.<sp/>and<sp/>Ram,<sp/>Parikshit},</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>title<sp/>=<sp/>{Dual-tree<sp/>fast<sp/>exact<sp/>max-kernel<sp/>search},</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>journal<sp/>=<sp/>{Statistical<sp/>Analysis<sp/>and<sp/>Data<sp/>Mining},</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>volume<sp/>=<sp/>{7},</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>number<sp/>=<sp/>{4},</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>publisher<sp/>=<sp/>{Wiley<sp/>Subscription<sp/>Services,<sp/>Inc.,<sp/>A<sp/>Wiley<sp/>Company},</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>issn<sp/>=<sp/>{1932-1872},</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>url<sp/>=<sp/>{http://dx.doi.org/10.1002/sam.11218},</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>doi<sp/>=<sp/>{10.1002/sam.11218},</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>pages<sp/>=<sp/>{229--253},</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>year<sp/>=<sp/>{2014},</highlight></codeline>
<codeline><highlight class="normal">}</highlight></codeline>
</programlisting></para>
<para>Given a set of query points <formula id="178">$Q$</formula> and a set of reference points <formula id="177">$R$</formula>, the FastMKS algorithm is a fast dual-tree (or single-tree) algorithm which finds</para>
<para><formula id="198">\[ \arg\max_{p_r \in R} K(p_q, p_r) \]</formula></para>
<para>for all points <formula id="199">$p_q \in Q$</formula> and for some Mercer kernel <formula id="200">$K(\cdot, \cdot)$</formula>. A Mercer kernel is a kernel that is positive semidefinite; these are the classes of kernels that can be used with the kernel trick. In short, the positive semidefiniteness of a Mercer kernel means that any kernel matrix (or Gram matrix) created on a dataset must be positive semidefinite.</para>
<para>The FastMKS algorithm builds trees on the datasets <formula id="178">$Q$</formula> and <formula id="177">$R$</formula> in such a way that explicit representation of the points in the kernel space is unnecessary, by using cover trees (<ref refid="classmlpack_1_1tree_1_1CoverTree" kindref="compound">mlpack::tree::CoverTree</ref>). This allows the algorithm to be run, for instance, on string kernels, where there is no sensible explicit representation. The <bold>mlpack</bold> implementation allows any type of tree that does not require an explicit representation to be used. For more details, see the paper.</para>
<para>At the time of this writing there is no other fast algorithm for exact max-kernel search. <bold>mlpack</bold> implements both single-tree and dual-tree fast max-kernel search.</para>
<para><bold>mlpack</bold> provides:</para>
<para><itemizedlist>
<listitem><para>a <ref refid="fmkstutorial_1cli_fmkstut" kindref="member">simple command-line executable</ref> to run FastMKS</para>
</listitem><listitem><para>a <ref refid="fmkstutorial_1fastmks_fmkstut" kindref="member">C++ interface</ref> to run FastMKS</para>
</listitem></itemizedlist>
</para>
</sect1>
<sect1 id="fmkstutorial_1toc_fmkstut">
<title>Table of Contents</title>
<para>A list of all the sections this tutorial contains.</para>
<para><itemizedlist>
<listitem><para><ref refid="fmkstutorial_1intro_fmkstut" kindref="member">Introduction</ref></para>
</listitem><listitem><para><ref refid="fmkstutorial_1toc_fmkstut" kindref="member">Table of Contents</ref></para>
</listitem><listitem><para><ref refid="fmkstutorial_1cli_fmkstut" kindref="member">Command-line FastMKS (mlpack_fastmks)</ref><itemizedlist>
<listitem><para><ref refid="fmkstutorial_1cli_ex1_fmkstut" kindref="member">FastMKS with a linear kernel on one dataset</ref></para>
</listitem><listitem><para><ref refid="fmkstutorial_1cli_ex2_fmkstut" kindref="member">FastMKS on a reference and query dataset</ref></para>
</listitem><listitem><para><ref refid="fmkstutorial_1cli_ex3_fmkstut" kindref="member">FastMKS with a different kernel</ref></para>
</listitem><listitem><para><ref refid="fmkstutorial_1cli_ex4_fmkstut" kindref="member">Using single-tree search or naive search</ref></para>
</listitem><listitem><para><ref refid="fmkstutorial_1cli_ex5_fmkstut" kindref="member">Parameters for alternate kernels</ref></para>
</listitem><listitem><para><ref refid="fmkstutorial_1cli_ex6_fmkstut" kindref="member">Saving a FastMKS model/tree</ref></para>
</listitem><listitem><para><ref refid="fmkstutorial_1cli_ex7_fmkstut" kindref="member">Loading a FastMKS model for further searches</ref></para>
</listitem></itemizedlist>
</para>
</listitem><listitem><para><ref refid="fmkstutorial_1fastmks_fmkstut" kindref="member">The &apos;FastMKS&apos; class</ref><itemizedlist>
<listitem><para><ref refid="fmkstutorial_1fastmks_ex1_fmkstut" kindref="member">FastMKS on one dataset</ref></para>
</listitem><listitem><para><ref refid="fmkstutorial_1fastmks_ex2_fmkstut" kindref="member">FastMKS with a query and reference dataset</ref></para>
</listitem><listitem><para><ref refid="fmkstutorial_1fastmks_ex3_fmkstut" kindref="member">FastMKS with an initialized kernel</ref></para>
</listitem><listitem><para><ref refid="fmkstutorial_1fastmks_ex4_fmkstut" kindref="member">FastMKS with an already-created tree</ref></para>
</listitem></itemizedlist>
</para>
</listitem><listitem><para><ref refid="fmkstutorial_1writing_kernel_fmkstut" kindref="member">Writing a custom kernel for FastMKS</ref></para>
</listitem><listitem><para><ref refid="fmkstutorial_1custom_tree_fmkstut" kindref="member">Using other tree types for FastMKS</ref></para>
</listitem><listitem><para><ref refid="fmkstutorial_1objects_fmkstut" kindref="member">Running FastMKS on objects</ref></para>
</listitem><listitem><para><ref refid="fmkstutorial_1further_doc_fmkstut" kindref="member">Further documentation</ref></para>
</listitem></itemizedlist>
</para>
</sect1>
<sect1 id="fmkstutorial_1cli_fmkstut">
<title>Command-line FastMKS (mlpack_fastmks)</title>
<para><bold>mlpack</bold> provides a command-line program, <computeroutput>mlpack_fastmks</computeroutput>, which is used to perform FastMKS on a given query and reference dataset. It supports numerous different types of kernels:</para>
<para><itemizedlist>
<listitem><para><ref refid="classmlpack_1_1kernel_1_1LinearKernel" kindref="compound">linear kernel</ref></para>
</listitem><listitem><para><ref refid="classmlpack_1_1kernel_1_1PolynomialKernel" kindref="compound">polynomial kernel</ref></para>
</listitem><listitem><para><ref refid="classmlpack_1_1kernel_1_1CosineDistance" kindref="compound">cosine distance</ref></para>
</listitem><listitem><para><ref refid="classmlpack_1_1kernel_1_1GaussianKernel" kindref="compound">Gaussian kernel</ref></para>
</listitem><listitem><para><ref refid="classmlpack_1_1kernel_1_1EpanechnikovKernel" kindref="compound">Epanechnikov kernel</ref></para>
</listitem><listitem><para><ref refid="classmlpack_1_1kernel_1_1TriangularKernel" kindref="compound">triangular kernel</ref></para>
</listitem><listitem><para><ref refid="classmlpack_1_1kernel_1_1HyperbolicTangentKernel" kindref="compound">hyperbolic tangent kernel</ref></para>
</listitem></itemizedlist>
</para>
<para>Note that when a shift-invariant kernel is used, the results will be the same as nearest neighbor search, so <ref refid="nstutorial" kindref="compound">KNN</ref> may be a better option. A shift-invariant kernel is a kernel that depends only on the distance between the two input points. The <ref refid="classmlpack_1_1kernel_1_1GaussianKernel" kindref="compound">Gaussian kernel</ref>, <ref refid="classmlpack_1_1kernel_1_1EpanechnikovKernel" kindref="compound">Epanechnikov kernel</ref>, and <ref refid="classmlpack_1_1kernel_1_1TriangularKernel" kindref="compound">triangular kernel</ref> are instances of shift-invariant kernels. The paper contains more details on this situation. The <computeroutput>mlpack_fastmks</computeroutput> executable still provides these kernels as options, though.</para>
<para>The following examples detail usage of the <computeroutput>mlpack_fastmks</computeroutput> program. Note that you can get documentation on all the possible parameters by typing:</para>
<para><programlisting><codeline><highlight class="normal">$<sp/>mlpack_fastmks<sp/>--help</highlight></codeline>
</programlisting></para>
<sect2 id="fmkstutorial_1cli_ex1_fmkstut">
<title>FastMKS with a linear kernel on one dataset</title>
<para>If only one dataset is specified (with <computeroutput>-r</computeroutput> or <computeroutput><ndash/>reference_file</computeroutput>), the reference dataset is taken to be both the query and reference datasets. The example below finds the 4 maximum kernels of each point in dataset.csv, using the default linear kernel.</para>
<para><programlisting><codeline><highlight class="normal">$<sp/>mlpack_fastmks<sp/>-r<sp/>dataset.csv<sp/>-k<sp/>4<sp/>-v<sp/>-p<sp/>products.csv<sp/>-i<sp/>indices.csv</highlight></codeline>
</programlisting></para>
<para>When the operation completes, the values of the kernels are saved in products.csv and the indices of the points which give the maximum kernels are saved in indices.csv.</para>
<para><programlisting><codeline><highlight class="normal">$<sp/>head<sp/>indices.csv</highlight></codeline>
<codeline><highlight class="normal">762,910,863,890</highlight></codeline>
<codeline><highlight class="normal">762,910,426,568</highlight></codeline>
<codeline><highlight class="normal">910,762,863,426</highlight></codeline>
<codeline><highlight class="normal">762,910,863,426</highlight></codeline>
<codeline><highlight class="normal">863,910,614,762</highlight></codeline>
<codeline><highlight class="normal">762,863,910,614</highlight></codeline>
<codeline><highlight class="normal">762,910,488,568</highlight></codeline>
<codeline><highlight class="normal">762,910,863,426</highlight></codeline>
<codeline><highlight class="normal">910,762,863,426</highlight></codeline>
<codeline><highlight class="normal">863,762,910,614</highlight></codeline>
</programlisting></para>
<para><programlisting><codeline><highlight class="normal">$<sp/>head<sp/>products.csv</highlight></codeline>
<codeline><highlight class="normal">1.6221652894e+00,1.5998743443e+00,1.5898890769e+00,1.5406789753e+00</highlight></codeline>
<codeline><highlight class="normal">1.3387953449e+00,1.3317349486e+00,1.2966613184e+00,1.2774493620e+00</highlight></codeline>
<codeline><highlight class="normal">1.6386110476e+00,1.6332029753e+00,1.5952629124e+00,1.5887195330e+00</highlight></codeline>
<codeline><highlight class="normal">1.0917545803e+00,1.0820878726e+00,1.0668992636e+00,1.0419838050e+00</highlight></codeline>
<codeline><highlight class="normal">1.2272441028e+00,1.2169643942e+00,1.2104597963e+00,1.2067780154e+00</highlight></codeline>
<codeline><highlight class="normal">1.5720962456e+00,1.5618504956e+00,1.5609069923e+00,1.5235605095e+00</highlight></codeline>
<codeline><highlight class="normal">1.3655478674e+00,1.3548593212e+00,1.3311547298e+00,1.3250728881e+00</highlight></codeline>
<codeline><highlight class="normal">2.0119149744e+00,2.0043668067e+00,1.9847289214e+00,1.9298280046e+00</highlight></codeline>
<codeline><highlight class="normal">1.1586923205e+00,1.1494586097e+00,1.1274872962e+00,1.1248172766e+00</highlight></codeline>
<codeline><highlight class="normal">4.4789820372e-01,4.4618539778e-01,4.4200024852e-01,4.3989721792e-01</highlight></codeline>
</programlisting></para>
<para>We can see in this example that for point 0, the point with maximum kernel value is point 762, with a kernel value of 1.622165. For point 3, the point with third largest kernel value is point 863, with a kernel value of 1.0669.</para>
</sect2>
<sect2 id="fmkstutorial_1cli_ex2_fmkstut">
<title>FastMKS on a reference and query dataset</title>
<para>The query points may be different than the reference points. To specify a different query set, the <computeroutput>-q</computeroutput> (or <computeroutput><ndash/>query_file</computeroutput>) option is used, as in the example below.</para>
<para><programlisting><codeline><highlight class="normal">$<sp/>mlpack_fastmks<sp/>-q<sp/>query_set.csv<sp/>-r<sp/>reference_set.csv<sp/>-k<sp/>5<sp/>-i<sp/>indices.csv<sp/>\</highlight></codeline>
<codeline><highlight class="normal">&gt;<sp/>-p<sp/>products.csv</highlight></codeline>
</programlisting></para>
</sect2>
<sect2 id="fmkstutorial_1cli_ex3_fmkstut">
<title>FastMKS with a different kernel</title>
<para>The <computeroutput>mlpack_fastmks</computeroutput> program offers more than just the linear kernel. Valid options are <computeroutput>&apos;linear&apos;</computeroutput>, <computeroutput>&apos;polynomial&apos;</computeroutput>, <computeroutput>&apos;cosine&apos;</computeroutput>, <computeroutput>&apos;gaussian&apos;</computeroutput>, <computeroutput>&apos;epanechnikov&apos;</computeroutput>, <computeroutput>&apos;triangular&apos;</computeroutput> and <computeroutput>&apos;hyptan&apos;</computeroutput> (the hyperbolic tangent kernel). Note that the hyperbolic tangent kernel is provably not a Mercer kernel but is positive semidefinite on most datasets and is commonly used as a kernel. Note also that the Gaussian kernel and other shift-invariant kernels give the same results as nearest neighbor search (see <ref refid="nstutorial" kindref="compound">NeighborSearch tutorial (k-nearest-neighbors)</ref>).</para>
<para>The kernel to use is specified with the <computeroutput>-K</computeroutput> (or <computeroutput><ndash/>kernel</computeroutput>) option. The example below uses the cosine similarity as a kernel.</para>
<para><programlisting><codeline><highlight class="normal">$<sp/>mlpack_fastmks<sp/>-r<sp/>dataset.csv<sp/>-k<sp/>5<sp/>-K<sp/>cosine<sp/>-i<sp/>indices.csv<sp/>-p<sp/>products.csv<sp/>-v</highlight></codeline>
</programlisting></para>
</sect2>
<sect2 id="fmkstutorial_1cli_ex4_fmkstut">
<title>Using single-tree search or naive search</title>
<para>In some cases, it may be useful to not use the dual-tree FastMKS algorithm. Instead you can specify the <computeroutput><ndash/>single</computeroutput> option, indicating that a tree should be built only on the reference set, and then the queries should be processed in a linear scan (instead of in a tree). Alternately, the <computeroutput>-N</computeroutput> (or <computeroutput><ndash/>naive</computeroutput>) option makes the program not build trees at all and instead use brute-force search to find the solutions.</para>
<para>The example below uses single-tree search on two datasets with the linear kernel.</para>
<para><programlisting><codeline><highlight class="normal">$<sp/>mlpack_fastmks<sp/>-q<sp/>query_set.csv<sp/>-r<sp/>reference_set.csv<sp/>--single<sp/>-k<sp/>5<sp/>\</highlight></codeline>
<codeline><highlight class="normal">&gt;<sp/>-p<sp/>products.csv<sp/>-i<sp/>indices.csv<sp/>-K<sp/>linear</highlight></codeline>
</programlisting></para>
<para>The example below uses naive search on one dataset.</para>
<para><programlisting><codeline><highlight class="normal">$<sp/>mlpack_fastmks<sp/>-r<sp/>reference_set.csv<sp/>-k<sp/>5<sp/>-N<sp/>-p<sp/>products.csv<sp/>-i<sp/>indices.csv</highlight></codeline>
</programlisting></para>
</sect2>
<sect2 id="fmkstutorial_1cli_ex5_fmkstut">
<title>Parameters for alternate kernels</title>
<para>Many of the alternate kernel choices have parameters which can be chosen; these are detailed in this section.</para>
<para><itemizedlist>
<listitem><para><bold><computeroutput>-w</computeroutput> </bold>(<computeroutput><ndash/>bandwidth</computeroutput>): this sets the bandwidth of the kernel, and is applicable to the <computeroutput>&apos;gaussian&apos;</computeroutput>, <computeroutput>&apos;epanechnikov&apos;</computeroutput>, and <computeroutput>&apos;triangular&apos;</computeroutput> kernels. This is the &quot;spread&quot; of the kernel.</para>
</listitem><listitem><para><bold><computeroutput>-d</computeroutput> </bold>(<computeroutput><ndash/>degree</computeroutput>): this sets the degree of the polynomial kernel (the power to which the result is raised). It is only applicable to the <computeroutput>&apos;polynomial&apos;</computeroutput> kernel.</para>
</listitem><listitem><para><bold><computeroutput>-o</computeroutput> </bold>(<computeroutput><ndash/>offset</computeroutput>): this sets the offset of the kernel, for the <computeroutput>&apos;polynomial&apos;</computeroutput> and <computeroutput>&apos;hyptan&apos;</computeroutput> kernel. See <ref refid="classmlpack_1_1kernel_1_1PolynomialKernel" kindref="compound">the polynomial kernel documentation</ref> and <ref refid="classmlpack_1_1kernel_1_1HyperbolicTangentKernel" kindref="compound">the hyperbolic tangent kernel documentation</ref> for more information.</para>
</listitem><listitem><para><bold><computeroutput>-s</computeroutput> </bold>(<computeroutput><ndash/>scale</computeroutput>): this sets the scale of the kernel, and is only applicable to the <computeroutput>&apos;hyptan&apos;</computeroutput> kernel. See <ref refid="classmlpack_1_1kernel_1_1HyperbolicTangentKernel" kindref="compound">the hyperbolic tangent kernel documentation</ref> for more information.</para>
</listitem></itemizedlist>
</para>
</sect2>
<sect2 id="fmkstutorial_1cli_ex6_fmkstut">
<title>Saving a FastMKS model/tree</title>
<para>The <computeroutput>mlpack_fastmks</computeroutput> program also supports saving a model built on a reference dataset (this model includes the tree, the kernel, and the search parameters). The <computeroutput><ndash/>output_model_file</computeroutput> or <computeroutput>-M</computeroutput> option allows one to save these parameters to disk for later usage. An example is below:</para>
<para><programlisting><codeline><highlight class="normal">$<sp/>mlpack_fastmks<sp/>-r<sp/>reference_set.csv<sp/>-K<sp/>cosine<sp/>-M<sp/>fastmks_model.xml</highlight></codeline>
</programlisting></para>
<para>This example builds a tree on the dataset in <computeroutput>reference_set.csv</computeroutput> using the cosine similarity kernel, and saves the resulting model to <computeroutput>fastmks_model.xml</computeroutput>. This model may then be used in later calls to the <computeroutput>mlpack_fastmks</computeroutput> program.</para>
</sect2>
<sect2 id="fmkstutorial_1cli_ex7_fmkstut">
<title>Loading a FastMKS model for further searches</title>
<para>Supposing that a FastMKS model has been saved with the <computeroutput><ndash/>output_model_file</computeroutput> or <computeroutput>-M</computeroutput> parameter, that model can then be later loaded in subsequent calls to the <computeroutput>mlpack_fastmks</computeroutput> program, using the <computeroutput><ndash/>input_model_file</computeroutput> or <computeroutput>-m</computeroutput> option. For instance, with a model saved in <computeroutput>fastmks_model.xml</computeroutput> and a query set in <computeroutput>query_set.csv</computeroutput>, we can find 3 max-kernel candidates, saving to <computeroutput>indices.csv</computeroutput> and <computeroutput>kernels.csv:</computeroutput> </para>
<para><programlisting><codeline><highlight class="normal">$<sp/>mlpack_fastmks<sp/>-m<sp/>fastmks_model.xml<sp/>-k<sp/>3<sp/>-i<sp/>indices.csv<sp/>-p<sp/>kernels.csv</highlight></codeline>
</programlisting></para>
<para>Loading a model as opposed to building a model is advantageous because the reference tree is already built. So, among other situations, this could be useful in the setting where many different query sets (or many different values of k) will be used.</para>
<para>Note that the kernel cannot be changed in a saved model without rebuilding the model entirely.</para>
</sect2>
</sect1>
<sect1 id="fmkstutorial_1fastmks_fmkstut">
<title>The &apos;FastMKS&apos; class</title>
<para>The <computeroutput>FastMKS&lt;&gt;</computeroutput> class offers a simple API for use within C++ applications, and allows further flexibility in kernel choice and tree type choice. However, <computeroutput>FastMKS&lt;&gt;</computeroutput> has no default template parameter for the kernel type <ndash/> that must be manually specified. Choices that <bold>mlpack</bold> provides include:</para>
<para><itemizedlist>
<listitem><para><ref refid="classmlpack_1_1kernel_1_1LinearKernel" kindref="compound">mlpack::kernel::LinearKernel</ref></para>
</listitem><listitem><para><ref refid="classmlpack_1_1kernel_1_1PolynomialKernel" kindref="compound">mlpack::kernel::PolynomialKernel</ref></para>
</listitem><listitem><para><ref refid="classmlpack_1_1kernel_1_1CosineDistance" kindref="compound">mlpack::kernel::CosineDistance</ref></para>
</listitem><listitem><para><ref refid="classmlpack_1_1kernel_1_1GaussianKernel" kindref="compound">mlpack::kernel::GaussianKernel</ref></para>
</listitem><listitem><para><ref refid="classmlpack_1_1kernel_1_1EpanechnikovKernel" kindref="compound">mlpack::kernel::EpanechnikovKernel</ref></para>
</listitem><listitem><para><ref refid="classmlpack_1_1kernel_1_1TriangularKernel" kindref="compound">mlpack::kernel::TriangularKernel</ref></para>
</listitem><listitem><para><ref refid="classmlpack_1_1kernel_1_1HyperbolicTangentKernel" kindref="compound">mlpack::kernel::HyperbolicTangentKernel</ref></para>
</listitem><listitem><para><ref refid="classmlpack_1_1kernel_1_1LaplacianKernel" kindref="compound">mlpack::kernel::LaplacianKernel</ref></para>
</listitem><listitem><para><ref refid="classmlpack_1_1kernel_1_1PSpectrumStringKernel" kindref="compound">mlpack::kernel::PSpectrumStringKernel</ref></para>
</listitem></itemizedlist>
</para>
<para>The following examples use kernels from that list. Writing your own kernel is detailed in <ref refid="fmkstutorial_1writing_kernel_fmkstut" kindref="member">the next section</ref>. Remember that when you are using the C++ interface, the data matrices must be column-major. See <ref refid="matrices" kindref="compound">Matrices in mlpack</ref> for more information.</para>
<sect2 id="fmkstutorial_1fastmks_ex1_fmkstut">
<title>FastMKS on one dataset</title>
<para>Given only a reference dataset, the following code will run FastMKS with k set to 5.</para>
<para><programlisting><codeline><highlight class="normal">#include<sp/>&lt;mlpack/methods/fastmks/fastmks.hpp&gt;</highlight></codeline>
<codeline><highlight class="normal">#include<sp/>&lt;mlpack/core/kernels/linear_kernel.hpp&gt;</highlight></codeline>
<codeline></codeline>
<codeline><highlight class="normal">using<sp/>namespace<sp/>mlpack::fastmks;</highlight></codeline>
<codeline></codeline>
<codeline><highlight class="normal">//<sp/>The<sp/>reference<sp/>dataset,<sp/>which<sp/>is<sp/>column-major.</highlight></codeline>
<codeline><highlight class="normal">extern<sp/>arma::mat<sp/>data;</highlight></codeline>
<codeline></codeline>
<codeline><highlight class="normal">//<sp/>This<sp/>will<sp/>initialize<sp/>the<sp/>FastMKS<sp/>object<sp/>with<sp/>the<sp/>linear<sp/>kernel<sp/>with<sp/>default</highlight></codeline>
<codeline><highlight class="normal">//<sp/>options:<sp/>K(x,<sp/>y)<sp/>=<sp/>x^T<sp/>y.<sp/><sp/>The<sp/>tree<sp/>is<sp/>built<sp/>in<sp/>the<sp/>constructor.</highlight></codeline>
<codeline><highlight class="normal">FastMKS&lt;LinearKernel&gt;<sp/>f(data);</highlight></codeline>
<codeline></codeline>
<codeline><highlight class="normal">//<sp/>The<sp/>results<sp/>will<sp/>be<sp/>stored<sp/>in<sp/>these<sp/>matrices.</highlight></codeline>
<codeline><highlight class="normal">arma::Mat&lt;size_t&gt;<sp/>indices;</highlight></codeline>
<codeline><highlight class="normal">arma::mat<sp/>products;</highlight></codeline>
<codeline></codeline>
<codeline><highlight class="normal">//<sp/>Run<sp/>FastMKS.</highlight></codeline>
<codeline><highlight class="normal">f.Search(5,<sp/>indices,<sp/>products);</highlight></codeline>
</programlisting></para>
</sect2>
<sect2 id="fmkstutorial_1fastmks_ex2_fmkstut">
<title>FastMKS with a query and reference dataset</title>
<para>In this setting we have both a query and reference dataset. We search for 10 maximum kernels.</para>
<para><programlisting><codeline><highlight class="normal">#include<sp/>&lt;mlpack/methods/fastmks/fastmks.hpp&gt;</highlight></codeline>
<codeline><highlight class="normal">#include<sp/>&lt;mlpack/core/kernels/triangular_kernel.hpp&gt;</highlight></codeline>
<codeline></codeline>
<codeline><highlight class="normal">using<sp/>namespace<sp/>mlpack::fastmks;</highlight></codeline>
<codeline><highlight class="normal">using<sp/>namespace<sp/>mlpack::kernel;</highlight></codeline>
<codeline></codeline>
<codeline><highlight class="normal">//<sp/>The<sp/>reference<sp/>and<sp/>query<sp/>datasets,<sp/>which<sp/>are<sp/>column-major.</highlight></codeline>
<codeline><highlight class="normal">extern<sp/>arma::mat<sp/>referenceData;</highlight></codeline>
<codeline><highlight class="normal">extern<sp/>arma::mat<sp/>queryData;</highlight></codeline>
<codeline></codeline>
<codeline><highlight class="normal">//<sp/>This<sp/>will<sp/>initialize<sp/>the<sp/>FastMKS<sp/>object<sp/>with<sp/>the<sp/>triangular<sp/>kernel<sp/>with</highlight></codeline>
<codeline><highlight class="normal">//<sp/>default<sp/>options<sp/>(bandwidth<sp/>of<sp/>1).<sp/><sp/>The<sp/>reference<sp/>tree<sp/>is<sp/>built<sp/>in<sp/>the</highlight></codeline>
<codeline><highlight class="normal">//<sp/>constructor.</highlight></codeline>
<codeline><highlight class="normal">FastMKS&lt;TriangularKernel&gt;<sp/>f(referenceData);</highlight></codeline>
<codeline></codeline>
<codeline><highlight class="normal">//<sp/>The<sp/>results<sp/>will<sp/>be<sp/>stored<sp/>in<sp/>these<sp/>matrices.</highlight></codeline>
<codeline><highlight class="normal">arma::Mat&lt;size_t&gt;<sp/>indices;</highlight></codeline>
<codeline><highlight class="normal">arma::mat<sp/>products;</highlight></codeline>
<codeline></codeline>
<codeline><highlight class="normal">//<sp/>Run<sp/>FastMKS.<sp/><sp/>The<sp/>query<sp/>tree<sp/>is<sp/>built<sp/>during<sp/>the<sp/>call<sp/>to<sp/>Search().</highlight></codeline>
<codeline><highlight class="normal">f.Search(queryData,<sp/>10,<sp/>indices,<sp/>products);</highlight></codeline>
</programlisting></para>
</sect2>
<sect2 id="fmkstutorial_1fastmks_ex3_fmkstut">
<title>FastMKS with an initialized kernel</title>
<para>Often, kernels have parameters which need to be specified. <computeroutput>FastMKS&lt;&gt;</computeroutput> has constructors which take initialized kernels. Note that temporary kernels cannot be passed as an argument. The example below initializes a <computeroutput>PolynomialKernel</computeroutput> object and then runs FastMKS with a query and reference dataset.</para>
<para><programlisting><codeline><highlight class="normal">#include<sp/>&lt;mlpack/methods/fastmks/fastmks.hpp&gt;</highlight></codeline>
<codeline><highlight class="normal">#include<sp/>&lt;mlpack/core/kernels/polynomial_kernel.hpp&gt;</highlight></codeline>
<codeline></codeline>
<codeline><highlight class="normal">using<sp/>namespace<sp/>mlpack::fastmks;</highlight></codeline>
<codeline><highlight class="normal">using<sp/>namespace<sp/>mlpack::kernel;</highlight></codeline>
<codeline></codeline>
<codeline><highlight class="normal">//<sp/>The<sp/>reference<sp/>and<sp/>query<sp/>datasets,<sp/>which<sp/>are<sp/>column-major.</highlight></codeline>
<codeline><highlight class="normal">extern<sp/>arma::mat<sp/>referenceData;</highlight></codeline>
<codeline><highlight class="normal">extern<sp/>arma::mat<sp/>queryData;</highlight></codeline>
<codeline></codeline>
<codeline><highlight class="normal">//<sp/>Initialize<sp/>the<sp/>polynomial<sp/>kernel<sp/>with<sp/>degree<sp/>of<sp/>3<sp/>and<sp/>offset<sp/>of<sp/>2.5.</highlight></codeline>
<codeline><highlight class="normal">PolynomialKernel<sp/>pk(3.0,<sp/>2.5);</highlight></codeline>
<codeline></codeline>
<codeline><highlight class="normal">//<sp/>Create<sp/>the<sp/>FastMKS<sp/>object<sp/>with<sp/>the<sp/>initialized<sp/>kernel.</highlight></codeline>
<codeline><highlight class="normal">FastMKS&lt;PolynomialKernel&gt;<sp/>f(referenceData,<sp/>pk);</highlight></codeline>
<codeline></codeline>
<codeline><highlight class="normal">//<sp/>The<sp/>results<sp/>will<sp/>be<sp/>stored<sp/>in<sp/>these<sp/>matrices.</highlight></codeline>
<codeline><highlight class="normal">arma::Mat&lt;size_t&gt;<sp/>indices;</highlight></codeline>
<codeline><highlight class="normal">arma::mat<sp/>products;</highlight></codeline>
<codeline></codeline>
<codeline><highlight class="normal">//<sp/>Run<sp/>FastMKS.</highlight></codeline>
<codeline><highlight class="normal">f.Search(queryData,<sp/>10,<sp/>indices,<sp/>products);</highlight></codeline>
</programlisting></para>
<para>The syntax for running FastMKS with one dataset and an initialized kernel is very similar:</para>
<para><programlisting><codeline><highlight class="normal">f.Search(10,<sp/>indices,<sp/>products);</highlight></codeline>
</programlisting></para>
</sect2>
<sect2 id="fmkstutorial_1fastmks_ex4_fmkstut">
<title>FastMKS with an already-created tree</title>
<para>By default, <computeroutput>FastMKS&lt;&gt;</computeroutput> uses the cover tree datastructure (see <ref refid="classmlpack_1_1tree_1_1CoverTree" kindref="compound">mlpack::tree::CoverTree</ref>). Sometimes, it is useful to modify the parameters of the cover tree. In this scenario, a tree must be built outside of the constructor, and then passed to the appropriate <computeroutput>FastMKS&lt;&gt;</computeroutput> constructor. An example on just a reference dataset is shown below, where the base of the cover tree is modified.</para>
<para>We also use an instantiated kernel, but because we are building our own tree, we must use <ref refid="classmlpack_1_1metric_1_1IPMetric" kindref="compound">IPMetric</ref> so that our tree is built on the metric induced by our kernel function.</para>
<para><programlisting><codeline><highlight class="normal">#include<sp/>&lt;mlpack/methods/fastmks/fastmks.hpp&gt;</highlight></codeline>
<codeline><highlight class="normal">#include<sp/>&lt;mlpack/core/kernels/polynomial_kernel.hpp&gt;</highlight></codeline>
<codeline></codeline>
<codeline><highlight class="normal">//<sp/>The<sp/>reference<sp/>dataset,<sp/>which<sp/>is<sp/>column-major.</highlight></codeline>
<codeline><highlight class="normal">extern<sp/>arma::mat<sp/>data;</highlight></codeline>
<codeline></codeline>
<codeline><highlight class="normal">//<sp/>Initialize<sp/>the<sp/>polynomial<sp/>kernel<sp/>with<sp/>a<sp/>degree<sp/>of<sp/>4<sp/>and<sp/>offset<sp/>of<sp/>2.0.</highlight></codeline>
<codeline><highlight class="normal">PolynomialKernel<sp/>pk(4.0,<sp/>2.0);</highlight></codeline>
<codeline></codeline>
<codeline><highlight class="normal">//<sp/>Create<sp/>the<sp/>metric<sp/>induced<sp/>by<sp/>this<sp/>kernel<sp/>(because<sp/>a<sp/>kernel<sp/>is<sp/>not<sp/>a<sp/>metric</highlight></codeline>
<codeline><highlight class="normal">//<sp/>and<sp/>we<sp/>can&apos;t<sp/>build<sp/>a<sp/>tree<sp/>on<sp/>a<sp/>kernel<sp/>alone).</highlight></codeline>
<codeline><highlight class="normal">IPMetric&lt;PolynomialKernel&gt;<sp/>metric(pk);</highlight></codeline>
<codeline></codeline>
<codeline><highlight class="normal">//<sp/>Now<sp/>build<sp/>a<sp/>tree<sp/>on<sp/>the<sp/>reference<sp/>dataset<sp/>using<sp/>the<sp/>instantiated<sp/>metric<sp/>and</highlight></codeline>
<codeline><highlight class="normal">//<sp/>the<sp/>custom<sp/>base<sp/>of<sp/>1.5<sp/>(default<sp/>is<sp/>1.3).<sp/><sp/>We<sp/>have<sp/>to<sp/>be<sp/>sure<sp/>to<sp/>use<sp/>the<sp/>right</highlight></codeline>
<codeline><highlight class="normal">//<sp/>type<sp/>here<sp/>--<sp/>FastMKS<sp/>needs<sp/>the<sp/>FastMKSStat<sp/>object<sp/>as<sp/>the<sp/>tree&apos;s</highlight></codeline>
<codeline><highlight class="normal">//<sp/>StatisticType.</highlight></codeline>
<codeline><highlight class="normal">typedef<sp/>tree::CoverTree&lt;IPMetric&lt;PolynomialKernel&gt;,<sp/>tree::FirstPointIsRoot,</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/>FastMKSStat&gt;<sp/>TreeType;<sp/>//<sp/>Convenience<sp/>typedef.</highlight></codeline>
<codeline><highlight class="normal">TreeType*<sp/>tree<sp/>=<sp/>new<sp/>TreeType(data,<sp/>metric,<sp/>1.5);</highlight></codeline>
<codeline></codeline>
<codeline><highlight class="normal">//<sp/>Now<sp/>initialize<sp/>FastMKS<sp/>with<sp/>that<sp/>statistic.<sp/><sp/>We<sp/>don&apos;t<sp/>need<sp/>to<sp/>specify<sp/>the</highlight></codeline>
<codeline><highlight class="normal">//<sp/>TreeType<sp/>template<sp/>parameter<sp/>since<sp/>we<sp/>are<sp/>still<sp/>using<sp/>the<sp/>default.<sp/><sp/>We<sp/>don&apos;t</highlight></codeline>
<codeline><highlight class="normal">//<sp/>need<sp/>to<sp/>pass<sp/>the<sp/>kernel<sp/>because<sp/>that<sp/>is<sp/>contained<sp/>in<sp/>the<sp/>tree.</highlight></codeline>
<codeline><highlight class="normal">FastMKS&lt;PolynomialKernel&gt;<sp/>f(tree);</highlight></codeline>
<codeline></codeline>
<codeline><highlight class="normal">//<sp/>The<sp/>results<sp/>will<sp/>be<sp/>stored<sp/>in<sp/>these<sp/>matrices.</highlight></codeline>
<codeline><highlight class="normal">arma::Mat&lt;size_t&gt;<sp/>indices;</highlight></codeline>
<codeline><highlight class="normal">arma::mat<sp/>products;</highlight></codeline>
<codeline></codeline>
<codeline><highlight class="normal">//<sp/>Run<sp/>FastMKS.</highlight></codeline>
<codeline><highlight class="normal">f.Search(10,<sp/>indices,<sp/>products);</highlight></codeline>
</programlisting></para>
<para>The syntax is similar for the case where different query and reference datasets are given; but trees for both need to be built in the manner specified above. Be sure to build both trees using the same metric (or at least a metric with the exact same parameters).</para>
<para><programlisting><codeline><highlight class="normal">f.Search(queryTree,<sp/>10,<sp/>indices,<sp/>products);</highlight></codeline>
</programlisting></para>
</sect2>
</sect1>
<sect1 id="fmkstutorial_1writing_kernel_fmkstut">
<title>Writing a custom kernel for FastMKS</title>
<para>While <bold>mlpack</bold> provides some number of kernels in the <ref refid="namespacemlpack_1_1kernel" kindref="compound">mlpack::kernel</ref> namespace, it is easy to create a custom kernel. To satisfy the KernelType policy, a class must implement the following methods:</para>
<para><programlisting><codeline><highlight class="normal">//<sp/>Empty<sp/>constructor<sp/>is<sp/>required.</highlight></codeline>
<codeline><highlight class="normal">KernelType();</highlight></codeline>
<codeline></codeline>
<codeline><highlight class="normal">//<sp/>Evaluate<sp/>the<sp/>kernel<sp/>between<sp/>two<sp/>points.</highlight></codeline>
<codeline><highlight class="normal">template&lt;typename<sp/>VecType&gt;</highlight></codeline>
<codeline><highlight class="normal">double<sp/>Evaluate(const<sp/>VecType&amp;<sp/>a,<sp/>const<sp/>VecType&amp;<sp/>b);</highlight></codeline>
</programlisting></para>
<para>The template parameter <computeroutput>VecType</computeroutput> is helpful (but not necessary) so that the kernel can be used with both sparse and dense matrices (<computeroutput>arma::sp_mat</computeroutput> and <computeroutput>arma::mat</computeroutput>).</para>
</sect1>
<sect1 id="fmkstutorial_1custom_tree_fmkstut">
<title>Using other tree types for FastMKS</title>
<para>The use of the cover tree (see <ref refid="classmlpack_1_1tree_1_1CoverTree" kindref="compound">CoverTree</ref>) is not necessary for FastMKS, although it is the default tree type. A different type of tree can be specified with the TreeType template parameter. However, the tree type is required to have <ref refid="classmlpack_1_1fastmks_1_1FastMKSStat" kindref="compound">FastMKSStat</ref> as the StatisticType, and for FastMKS to work, the tree must be built only on kernel evaluations (or distance evaluations in the kernel space via <ref refid="classmlpack_1_1metric_1_1IPMetric" kindref="compound">IPMetric::Evaluate()</ref>).</para>
<para>Below is an example where a custom tree class, <computeroutput>CustomTree</computeroutput>, is used as the tree type for FastMKS. In this example FastMKS is only run on one dataset.</para>
<para><programlisting><codeline><highlight class="normal">#include<sp/>&lt;mlpack/methods/fastmks/fastmks.hpp&gt;</highlight></codeline>
<codeline><highlight class="normal">#include<sp/>&quot;custom_tree.hpp&quot;</highlight></codeline>
<codeline></codeline>
<codeline><highlight class="normal">using<sp/>namespace<sp/>mlpack::fastmks;</highlight></codeline>
<codeline><highlight class="normal">using<sp/>namespace<sp/>mlpack::tree;</highlight></codeline>
<codeline></codeline>
<codeline><highlight class="normal">//<sp/>The<sp/>dataset<sp/>that<sp/>FastMKS<sp/>will<sp/>be<sp/>run<sp/>on.</highlight></codeline>
<codeline><highlight class="normal">extern<sp/>arma::mat<sp/>data;</highlight></codeline>
<codeline></codeline>
<codeline><highlight class="normal">//<sp/>The<sp/>custom<sp/>tree<sp/>type.<sp/><sp/>We&apos;ll<sp/>assume<sp/>that<sp/>the<sp/>first<sp/>template<sp/>parameter<sp/>is<sp/>the</highlight></codeline>
<codeline><highlight class="normal">//<sp/>statistic<sp/>type.</highlight></codeline>
<codeline><highlight class="normal">typedef<sp/>CustomTree&lt;FastMKSStat&gt;<sp/>TreeType;</highlight></codeline>
<codeline></codeline>
<codeline><highlight class="normal">//<sp/>The<sp/>FastMKS<sp/>constructor<sp/>will<sp/>create<sp/>the<sp/>tree.</highlight></codeline>
<codeline><highlight class="normal">FastMKS&lt;LinearKernel,<sp/>arma::mat,<sp/>TreeType&gt;<sp/>f(data);</highlight></codeline>
<codeline></codeline>
<codeline><highlight class="normal">//<sp/>These<sp/>will<sp/>hold<sp/>the<sp/>results.</highlight></codeline>
<codeline><highlight class="normal">arma::Mat&lt;size_t&gt;<sp/>indices;</highlight></codeline>
<codeline><highlight class="normal">arma::mat<sp/>products;</highlight></codeline>
<codeline></codeline>
<codeline><highlight class="normal">//<sp/>Run<sp/>FastMKS.</highlight></codeline>
<codeline><highlight class="normal">f.Search(5,<sp/>indices,<sp/>products);</highlight></codeline>
</programlisting></para>
</sect1>
<sect1 id="fmkstutorial_1objects_fmkstut">
<title>Running FastMKS on objects</title>
<para>FastMKS has a lot of utility on objects which are not representable in some sort of metric space. These objects might be strings, graphs, models, or other objects. For these types of objects, questions based on distance don&apos;t really make sense. One good example is with strings. The question &quot;how far is &apos;dog&apos;
from &apos;Taki Inoue&apos;?&quot; simply doesn&apos;t make sense. We can&apos;t have a centroid of the terms &apos;Fritz&apos;, &apos;E28&apos;, and &apos;popsicle&apos;.</para>
<para>However, what we can do is define some sort of kernel on these objects. These kernels generally correspond to some similarity measure, with one example being the p-spectrum string kernel (see <ref refid="classmlpack_1_1kernel_1_1PSpectrumStringKernel" kindref="compound">mlpack::kernel::PSpectrumStringKernel</ref>). Using that, we can say &quot;how similar is &apos;dog&apos; to &apos;Taki Inoue&apos;?&quot; and get an actual numerical result by evaluating K(&apos;dog&apos;, &apos;Taki Inoue&apos;) (where K is our p-spectrum string kernel).</para>
<para>The only requirement on these kernels is that they are positive definite kernels (or Mercer kernels). For more information on those details, refer to the FastMKS paper.</para>
<para>Remember that FastMKS is a tree-based method. But trees like the binary space tree require centroids <ndash/> and as we said earlier, centroids often don&apos;t make sense with these types of objects. Therefore, we need a type of tree which is built <bold>exclusively</bold> on points in the dataset <ndash/> those are points which we can evaluate our kernel function on. The cover tree is one example of a type of tree satisfying this condition; its construction will only call the kernel function on two points that are in the dataset.</para>
<para>But, we have one more problem. The <computeroutput>CoverTree</computeroutput> class is built on <computeroutput>arma::mat</computeroutput> objects (dense matrices). Our objects, however, are not necessarily representable in a column of a matrix. To use the example we have been using, strings cannot be represented easily in a matrix because they may all have different lengths.</para>
<para>The way to work around this problem is to create a &quot;fake&quot; data matrix which simply holds indices to objects. A good example of how to do this is detailed in the documentation for the <ref refid="classmlpack_1_1kernel_1_1PSpectrumStringKernel" kindref="compound">PSpectrumStringKernel</ref>.</para>
<para>In short, the trick is to make each data matrix one-dimensional and containing linear indices:</para>
<para><programlisting><codeline><highlight class="normal">arma::mat<sp/>data<sp/>=<sp/>&quot;0<sp/>1<sp/>2<sp/>3<sp/>4<sp/>5<sp/>6<sp/>7<sp/>8&quot;;</highlight></codeline>
</programlisting></para>
<para>Then, when <computeroutput>Evaluate()</computeroutput> is called on the kernel function, the parameters will be two one-dimensional vectors that simply contain indices to objects. The example below details the process a little better:</para>
<para><programlisting><codeline><highlight class="normal">//<sp/>This<sp/>function<sp/>evaluates<sp/>the<sp/>kernel<sp/>on<sp/>two<sp/>Objects<sp/>(in<sp/>this<sp/>example,<sp/>its</highlight></codeline>
<codeline><highlight class="normal">//<sp/>implementation<sp/>is<sp/>not<sp/>important;<sp/>the<sp/>only<sp/>important<sp/>thing<sp/>is<sp/>that<sp/>the</highlight></codeline>
<codeline><highlight class="normal">//<sp/>function<sp/>exists).</highlight></codeline>
<codeline><highlight class="normal">double<sp/>ObjectKernel::Evaluate(const<sp/>Object&amp;<sp/>a,<sp/>const<sp/>Object&amp;<sp/>b)<sp/>const;</highlight></codeline>
<codeline></codeline>
<codeline><highlight class="normal">template&lt;typename<sp/>VecType&gt;</highlight></codeline>
<codeline><highlight class="normal">double<sp/>ObjectKernel::Evaluate(const<sp/>VecType&amp;<sp/>a,<sp/>const<sp/>VecType&amp;<sp/>b)<sp/>const</highlight></codeline>
<codeline><highlight class="normal">{</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>//<sp/>Extract<sp/>the<sp/>indices<sp/>from<sp/>the<sp/>vectors.</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>const<sp/>size_t<sp/>indexA<sp/>=<sp/>size_t(a[0]);</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>const<sp/>size_t<sp/>indexB<sp/>=<sp/>size_t(b[0]);</highlight></codeline>
<codeline></codeline>
<codeline><highlight class="normal"><sp/><sp/>//<sp/>Assume<sp/>that<sp/>&apos;objects&apos;<sp/>is<sp/>an<sp/>array<sp/>(or<sp/>std::vector<sp/>or<sp/>other<sp/>container)</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>//<sp/>holding<sp/>Objects.</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>const<sp/>Object&amp;<sp/>objectA<sp/>=<sp/>objects[indexA];</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>const<sp/>Object&amp;<sp/>objectB<sp/>=<sp/>objects[indexB];</highlight></codeline>
<codeline></codeline>
<codeline><highlight class="normal"><sp/><sp/>//<sp/>Now<sp/>call<sp/>the<sp/>function<sp/>that<sp/>does<sp/>the<sp/>actual<sp/>evaluation<sp/>on<sp/>the<sp/>objects<sp/>and</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>//<sp/>return<sp/>its<sp/>result.</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/>return<sp/>Evaluate(objectA,<sp/>objectB);</highlight></codeline>
<codeline><highlight class="normal">}</highlight></codeline>
</programlisting></para>
<para>As written earlier, the documentation for <ref refid="classmlpack_1_1kernel_1_1PSpectrumStringKernel" kindref="compound">PSpectrumStringKernel</ref> is a good place to consult for further reference on this. That kernel uses two dimensional indices; one dimension represents the index of the string, and the other represents whether it is referring to the query set or the reference set. If your kernel is meant to work on separate query and reference sets, that strategy should be considered.</para>
</sect1>
<sect1 id="fmkstutorial_1further_doc_fmkstut">
<title>Further documentation</title>
<para>For further documentation on the FastMKS class, consult the <ref refid="classmlpack_1_1fastmks_1_1FastMKS" kindref="compound">complete API documentation</ref>. </para>
</sect1>
    </detaileddescription>
    <location file="/home/aakash/mlpack/doc/tutorials/fastmks/fastmks.txt"/>
  </compounddef>
</doxygen>
