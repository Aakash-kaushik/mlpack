<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.9.1"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>mlpack: Reinforcement Learning Tutorial</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="extra-stylesheet.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">mlpack
   &#160;<span id="projectnumber">3.4.2</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.1 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search','.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="PageDoc"><div class="header">
  <div class="headertitle">
<div class="title">Reinforcement Learning Tutorial </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h1><a class="anchor" id="intro_rltut"></a>
Introduction</h1>
<p>Reinforcement Learning is one of the hottest topics right now, with interest surging after DeepMind published their article on training deep neural networks to play Atari games to great success. mlpack implements a complete end-to-end framework for Reinforcement Learning, featuring multiple environments, policies and methods. Of course, custom environments and policies can be used and plugged into the existing framework with no runtime overhead.</p>
<p>mlpack implements typical benchmark environments (Acrobot, Mountain car etc.), commonly used policies, replay methods and supports asynchronous learning as well. In addition, it can <a href="https://github.com/zoq/gym_tcp_api">communicate</a> with the OpenAI Gym toolkit for more environments.</p>
<h1><a class="anchor" id="toc_rltut"></a>
Table of Contents</h1>
<p>This tutorial is split into the following sections:</p>
<ul>
<li><a class="el" href="rltutorial.html#intro_rltut">Introduction</a></li>
<li><a class="el" href="rltutorial.html#toc_rltut">Table of Contents</a></li>
<li><a class="el" href="rltutorial.html#environment_rltut">Reinforcement Learning Environments</a></li>
<li><a class="el" href="rltutorial.html#agent_components_rltut">Components of an RL Agent</a></li>
<li><a class="el" href="rltutorial.html#q_learning_rltut">Q-Learning in mlpack</a></li>
<li><a class="el" href="rltutorial.html#async_learning_rltut">async_learning_rltut</a></li>
<li><a class="el" href="rltutorial.html#further_rltut">Further documentation</a></li>
</ul>
<h1><a class="anchor" id="environment_rltut"></a>
Reinforcement Learning Environments</h1>
<p>mlpack implements a number of the most popular environments used for testing RL agents and algorithms. These include the Cart Pole, Acrobot, Mountain Car and their variations. Of course, as mentioned above, you can communicate with OpenAI Gym for other environments, like the Atari video games.</p>
<p>A key component of mlpack is its extensibility. It is a simple process to create your own custom environments, specific to your needs, and use it with mlpack's RL framework. All the environments implement a few specific methods and classes which are used by the agents while learning.</p>
<ul>
<li><code>State:</code> The State class is a representation of the environment. For the CartPole, this would involve storing the position, velocity, angle and angular velocity.</li>
<li><code>Action:</code> For discrete environments, Action is a class with an enum naming all the possible actions the agent can take in the environment. Continuing with the CartPole example, the enum would simply contain the two possible actions, <code>backward</code> and <code>forward</code>. For continuous environments, the Action class contains an array with its size depending on the action space.</li>
<li><code>Sample:</code> This method is perhaps the heart of the environment, providing rewards to the agent depending on the state and the action taken, and updates the state based on the action taken as well.</li>
</ul>
<p>Of course, your custom environment will most likely make use of a number of helper methods, depending on your application, such as the <code>Dsdt</code> method in the <code>Acrobot</code> environment, used in the <code>RK4</code> iterative method (also another helper method) to estimate the next state.</p>
<h1><a class="anchor" id="agent_components_rltut"></a>
Components of an RL Agent</h1>
<p>A Reinforcement Learning agent, in general, takes actions in an environment in order to maximize a cumulative reward. To that end, it requires a way to choose actions (<b>policy</b>) and a way to sample previous experiences (<b>replay</b>).</p>
<p>An example of a simple policy would be an epsilon-greedy policy. Using such a policy, the agent will choose actions greedily with some probability epsilon. This probability is slowly decreased over time, balancing the line between exploration and exploitation.</p>
<p>Similarly, an example of a simple replay would be a random replay. At each time step, the interactions between the agent and the environment are saved to a memory buffer and previous experiences are sampled from the buffer to train the agent.</p>
<p>Instantiating the components of an agent can be easily done by passing the Environment as a templated argument and the parameters of the policy/replay to the constructor.</p>
<p>To create a Greedy Policy and Prioritized Replay for the CartPole environment, we would do the following:</p>
<div class="fragment"><div class="line">GreedyPolicy&lt;CartPole&gt; policy(1.0, 1000, 0.1);</div>
<div class="line">PrioritizedReplay&lt;CartPole&gt; replayMethod(10, 10000, 0.6);</div>
</div><!-- fragment --><p>The arguments to <code>policy</code> are the initial epsilon values, the interval of decrease in its value and the value at which epsilon bottoms out and won't be reduced further. The arguments to <code>replayMethod</code> are size of the batch returned, the number of examples stored in memory, and the degree of prioritization.</p>
<p>In addition to the above components, an RL agent requires many hyperparameters to be tuned during it's training period. These parameters include everything from the discount rate of the future reward to whether Double Q-learning should be used or not. The <code>TrainingConfig</code> class can be instantiated and configured as follows:</p>
<div class="fragment"><div class="line">TrainingConfig config;</div>
<div class="line">config.StepSize() = 0.01;</div>
<div class="line">config.Discount() = 0.9;</div>
<div class="line">config.TargetNetworkSyncInterval() = 100;</div>
<div class="line">config.ExplorationSteps() = 100;</div>
<div class="line">config.DoubleQLearning() = <span class="keyword">false</span>;</div>
<div class="line">config.StepLimit() = 200;</div>
</div><!-- fragment --><p>The object <code>config</code> describes an RL agent, using a step size of 0.01 for the optimization process, a discount factor of 0.9, sync interval of 200 episodes. This agent only starts learning after storing 100 exploration steps, has a step limit of 200, and does not utilize double q-learning.</p>
<p>In this way, we can easily configure an RL agent with the desired hyperparameters.</p>
<h1><a class="anchor" id="q_learning_rltut"></a>
Q-Learning in mlpack</h1>
<p>Here, we demonstrate Q-Learning in mlpack through the use of a simple example, the training of a Q-Learning agent on the CartPole environment. The code has been broken into chunks for easy understanding.</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="core_8hpp.html">mlpack/core.hpp</a>&gt;</span></div>
<div class="line"><span class="preprocessor">#include &lt;<a class="code" href="ffn_8hpp.html">mlpack/methods/ann/ffn.hpp</a>&gt;</span></div>
<div class="line"><span class="preprocessor">#include &lt;<a class="code" href="gaussian__init_8hpp.html">mlpack/methods/ann/init_rules/gaussian_init.hpp</a>&gt;</span></div>
<div class="line"><span class="preprocessor">#include &lt;<a class="code" href="layer_8hpp.html">mlpack/methods/ann/layer/layer.hpp</a>&gt;</span></div>
<div class="line"><span class="preprocessor">#include &lt;<a class="code" href="mean__squared__error_8hpp.html">mlpack/methods/ann/loss_functions/mean_squared_error.hpp</a>&gt;</span></div>
<div class="line"><span class="preprocessor">#include &lt;<a class="code" href="q__learning_8hpp.html">mlpack/methods/reinforcement_learning/q_learning.hpp</a>&gt;</span></div>
<div class="line"><span class="preprocessor">#include &lt;<a class="code" href="simple__dqn_8hpp.html">mlpack/methods/reinforcement_learning/q_networks/simple_dqn.hpp</a>&gt;</span></div>
<div class="line"><span class="preprocessor">#include &lt;<a class="code" href="cart__pole_8hpp.html">mlpack/methods/reinforcement_learning/environment/cart_pole.hpp</a>&gt;</span></div>
<div class="line"><span class="preprocessor">#include &lt;<a class="code" href="greedy__policy_8hpp.html">mlpack/methods/reinforcement_learning/policy/greedy_policy.hpp</a>&gt;</span></div>
<div class="line"><span class="preprocessor">#include &lt;<a class="code" href="training__config_8hpp.html">mlpack/methods/reinforcement_learning/training_config.hpp</a>&gt;</span></div>
<div class="line"><span class="preprocessor">#include &lt;ensmallen.hpp&gt;</span></div>
<div class="line"> </div>
<div class="line"><span class="keyword">using namespace </span><a class="code" href="namespacemlpack.html">mlpack</a>;</div>
<div class="line"><span class="keyword">using namespace </span><a class="code" href="namespacemlpack_1_1ann.html">mlpack::ann</a>;</div>
<div class="line"><span class="keyword">using namespace </span>ens;</div>
<div class="line"><span class="keyword">using namespace </span><a class="code" href="namespacemlpack_1_1rl.html">mlpack::rl</a>;</div>
<div class="ttc" id="acart__pole_8hpp_html"><div class="ttname"><a href="cart__pole_8hpp.html">cart_pole.hpp</a></div></div>
<div class="ttc" id="acore_8hpp_html"><div class="ttname"><a href="core_8hpp.html">core.hpp</a></div><div class="ttdoc">Include all of the base components required to write mlpack methods, and the main mlpack Doxygen docu...</div></div>
<div class="ttc" id="affn_8hpp_html"><div class="ttname"><a href="ffn_8hpp.html">ffn.hpp</a></div></div>
<div class="ttc" id="agaussian__init_8hpp_html"><div class="ttname"><a href="gaussian__init_8hpp.html">gaussian_init.hpp</a></div></div>
<div class="ttc" id="agreedy__policy_8hpp_html"><div class="ttname"><a href="greedy__policy_8hpp.html">greedy_policy.hpp</a></div></div>
<div class="ttc" id="alayer_8hpp_html"><div class="ttname"><a href="layer_8hpp.html">layer.hpp</a></div></div>
<div class="ttc" id="amean__squared__error_8hpp_html"><div class="ttname"><a href="mean__squared__error_8hpp.html">mean_squared_error.hpp</a></div></div>
<div class="ttc" id="anamespacemlpack_1_1ann_html"><div class="ttname"><a href="namespacemlpack_1_1ann.html">mlpack::ann</a></div><div class="ttdoc">Artificial Neural Network.</div><div class="ttdef"><b>Definition:</b> <a href="elish__function_8hpp_source.html#l00032">elish_function.hpp:32</a></div></div>
<div class="ttc" id="anamespacemlpack_1_1rl_html"><div class="ttname"><a href="namespacemlpack_1_1rl.html">mlpack::rl</a></div><div class="ttdef"><b>Definition:</b> <a href="async__learning_8hpp_source.html#l00024">async_learning.hpp:24</a></div></div>
<div class="ttc" id="anamespacemlpack_html"><div class="ttname"><a href="namespacemlpack.html">mlpack</a></div><div class="ttdoc">Linear algebra utility functions, generally performed on matrices or vectors.</div><div class="ttdef"><b>Definition:</b> <a href="add__to__cli11_8hpp_source.html#l00021">add_to_cli11.hpp:21</a></div></div>
<div class="ttc" id="aq__learning_8hpp_html"><div class="ttname"><a href="q__learning_8hpp.html">q_learning.hpp</a></div></div>
<div class="ttc" id="asimple__dqn_8hpp_html"><div class="ttname"><a href="simple__dqn_8hpp.html">simple_dqn.hpp</a></div></div>
<div class="ttc" id="atraining__config_8hpp_html"><div class="ttname"><a href="training__config_8hpp.html">training_config.hpp</a></div></div>
</div><!-- fragment --><p>We include all the necessary components of our toy example and declare namespaces for convenience.</p>
<div class="fragment"><div class="line"><span class="keywordtype">int</span> main()</div>
<div class="line">{</div>
<div class="line">  <span class="comment">// Set up the network.</span></div>
<div class="line">  SimpleDQN&lt;&gt; model(4, 64, 32, 2);</div>
</div><!-- fragment --><p>The first step in setting our Q-learning agent is to setup the network for it to use. SimpleDQN class creates a simple feed forward network with 2 hidden layers. The network constructed here has an input shape of 4 and output shape of 2. This corresponds to the structure of the CartPole environment, where each state is represented as a column vector with 4 data members (position, velocity, angle, angular velocity). Similarly, the output shape is represented by the number of possible actions, which in this case, is only 2 (<code>foward</code> and <code>backward</code>).</p>
<p>We can also use mlpack's ann module to setup a custom FFN network. For example, here we use a single hidden layer. However, the Q-Learning agent expects the object to have a <code>ResetNoise</code> method which <code>SimpleDQN</code> has. We can't pass mlpack's FFN network directly. Instead, we have to wrap it into <code>SimpleDQN</code> object.</p>
<div class="fragment"><div class="line"><span class="keywordtype">int</span> main()</div>
<div class="line">{</div>
<div class="line">  <span class="comment">// Set up the network.</span></div>
<div class="line">  FFN&lt;MeanSquaredError&lt;&gt;, GaussianInitialization&gt; network(MeanSquaredError&lt;&gt;(),</div>
<div class="line">      GaussianInitialization(0, 0.001));</div>
<div class="line">  network.Add&lt;Linear&lt;&gt;&gt;(4, 128);</div>
<div class="line">  network.Add&lt;ReLULayer&lt;&gt;&gt;();</div>
<div class="line">  network.Add&lt;Linear&lt;&gt;&gt;(128, 128);</div>
<div class="line">  network.Add&lt;ReLULayer&lt;&gt;&gt;();</div>
<div class="line">  network.Add&lt;Linear&lt;&gt;&gt;(128, 2);</div>
<div class="line"> </div>
<div class="line">  SimpleDQN&lt;&gt; model(network);</div>
</div><!-- fragment --><p>The next step would be to setup the other components of the Q-learning agent, namely its policy, replay method and hyperparameters.</p>
<div class="fragment"><div class="line"><span class="comment">// Set up the policy and replay method.</span></div>
<div class="line"> GreedyPolicy&lt;CartPole&gt; policy(1.0, 1000, 0.1, 0.99);</div>
<div class="line"> RandomReplay&lt;CartPole&gt; replayMethod(10, 10000);</div>
<div class="line"> </div>
<div class="line"> TrainingConfig config;</div>
<div class="line"> config.StepSize() = 0.01;</div>
<div class="line"> config.Discount() = 0.9;</div>
<div class="line"> config.TargetNetworkSyncInterval() = 100;</div>
<div class="line"> config.ExplorationSteps() = 100;</div>
<div class="line"> config.DoubleQLearning() = <span class="keyword">false</span>;</div>
<div class="line"> config.StepLimit() = 200;</div>
</div><!-- fragment --><p>And now, we get to the heart of the program, declaring a Q-Learning agent.</p>
<div class="fragment"><div class="line">QLearning&lt;CartPole, decltype(model), AdamUpdate, decltype(policy)&gt;</div>
<div class="line">    agent(config, model, policy, replayMethod);</div>
</div><!-- fragment --><p>Here, we call the <code>QLearning</code> constructor, passing in the type of environment, network, updater, policy and replay. We use <code>decltype(var)</code> as a shorthand for the variable, saving us the trouble of copying the lengthy templated type.</p>
<p>We pass references of the objects we created, as parameters to QLearning class.</p>
<p>Now, we have our Q-Learning agent <code>agent</code> ready to be trained on the Cart Pole environment.</p>
<div class="fragment"><div class="line">  arma::running_stat&lt;double&gt; averageReturn;</div>
<div class="line">  <span class="keywordtype">size_t</span> episodes = 0;</div>
<div class="line">  <span class="keywordtype">bool</span> converged = <span class="keyword">true</span>;</div>
<div class="line">  <span class="keywordflow">while</span> (<span class="keyword">true</span>)</div>
<div class="line">  {</div>
<div class="line">    <span class="keywordtype">double</span> episodeReturn = agent.Episode();</div>
<div class="line">    averageReturn(episodeReturn);</div>
<div class="line">    episodes += 1;</div>
<div class="line"> </div>
<div class="line">    <span class="keywordflow">if</span> (episodes &gt; 1000)</div>
<div class="line">    {</div>
<div class="line">      std::cout &lt;&lt; <span class="stringliteral">&quot;Cart Pole with DQN failed.&quot;</span> &lt;&lt; std::endl;</div>
<div class="line">      converged = <span class="keyword">false</span>;</div>
<div class="line">      <span class="keywordflow">break</span>;</div>
<div class="line">    }</div>
<div class="line"> </div>
<div class="line">    std::cout &lt;&lt; <span class="stringliteral">&quot;Average return: &quot;</span> &lt;&lt; averageReturn.mean()</div>
<div class="line">        &lt;&lt; <span class="stringliteral">&quot; Episode return: &quot;</span> &lt;&lt; episodeReturn &lt;&lt; std::endl;</div>
<div class="line">    <span class="keywordflow">if</span> (averageReturn.mean() &gt; 35)</div>
<div class="line">      <span class="keywordflow">break</span>;</div>
<div class="line">  }</div>
<div class="line">  <span class="keywordflow">if</span> (converged)</div>
<div class="line">    std::cout &lt;&lt; <span class="stringliteral">&quot;Hooray! Q-Learning agent successfully trained&quot;</span> &lt;&lt; std::endl;</div>
<div class="line"> </div>
<div class="line">  <span class="keywordflow">return</span> 0;</div>
<div class="line">}</div>
</div><!-- fragment --><p>We set up a loop to train the agent. The exit condition is determined by the average reward which can be computed with <code>arma::running_stat</code>. It is used for storing running statistics of scalars, which in this case is the reward signal. The agent can be said to have converged when the average return reaches a predetermined value (i.e. &gt; 35).</p>
<p>Conversely, if the average return does not go beyond that amount even after a thousand episodes, we can conclude that the agent will not converge and exit the training loop.</p>
<h1><a class="anchor" id="async_learning_rltut"></a>
async_learning_rltut</h1>
<p>In 2016, Researchers at Deepmind and University of Montreal published their paper "Asynchronous Methods for Deep Reinforcement Learning". In it they described asynchronous variants of four standard reinforcement learning algorithms:</p><ul>
<li>One-Step SARSA</li>
<li>One-Step Q-Learning</li>
<li>N-Step Q-Learning</li>
<li>Advantage Actor-Critic(A3C)</li>
</ul>
<p>Online RL algorithms and Deep Neural Networks make an unstable combination because of the non-stationary and correlated nature of online updates. Although this is solved by Experience Replay, it has several drawbacks: it uses more memory and computation per real interaction; and it requires off-policy learning algorithms.</p>
<p>Asynchronous methods, instead of experience replay, asynchronously executes multiple agents in parallel, on multiple instances of the environment, which solves all the above problems.</p>
<p>Here, we demonstrate Asynchronous Learning methods in mlpack through the training of an async agent. Asynchronous learning involves training several agents simultaneously. Here, each of the agents are referred to as "workers". Currently mlpack has One-Step Q-Learning worker, N-Step Q-Learning worker and One-Step SARSA worker.</p>
<p>Let's examine the sample code in chunks.</p>
<p>Apart from the includes used for the q-learning example, two more have to be included:</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="async__learning_8hpp.html">mlpack/methods/reinforcement_learning/async_learning.hpp</a>&gt;</span></div>
<div class="line"><span class="preprocessor">#include &lt;<a class="code" href="aggregated__policy_8hpp.html">mlpack/methods/reinforcement_learning/policy/aggregated_policy.hpp</a>&gt;</span></div>
<div class="ttc" id="aaggregated__policy_8hpp_html"><div class="ttname"><a href="aggregated__policy_8hpp.html">aggregated_policy.hpp</a></div></div>
<div class="ttc" id="aasync__learning_8hpp_html"><div class="ttname"><a href="async__learning_8hpp.html">async_learning.hpp</a></div></div>
</div><!-- fragment --><p>Here we don't use experience replay, and instead of a single policy, we use three different policies, each corresponding to its worker. Number of workers created, depends on the number of policies given in the Aggregated Policy. The column vector contains the probability distribution for each child policy. We should make sure its size is same as the number of policies and the sum of its elements is equal to 1.</p>
<div class="fragment"><div class="line">AggregatedPolicy&lt;GreedyPolicy&lt;CartPole&gt;&gt; policy({GreedyPolicy&lt;CartPole&gt;(0.7, 5000, 0.1),</div>
<div class="line">                                                 GreedyPolicy&lt;CartPole&gt;(0.7, 5000, 0.01),</div>
<div class="line">                                                 GreedyPolicy&lt;CartPole&gt;(0.7, 5000, 0.5)},</div>
<div class="line">                                                 arma::colvec(<span class="stringliteral">&quot;0.4 0.3 0.3&quot;</span>));</div>
</div><!-- fragment --><p>Now, we will create the "OneStepQLearning" agent. We could have used "NStepQLearning" or "OneStepSarsa" here according to our requirement.</p>
<div class="fragment"><div class="line"><a class="code" href="namespacemlpack_1_1rl.html#a3c2fb6897a13583e67422ad12286f6d1">OneStepQLearning</a>&lt;CartPole, decltype(model), ens::AdamUpdate, decltype(policy)&gt;</div>
<div class="line">    agent(std::move(config), std::move(model), std::move(policy));</div>
<div class="ttc" id="anamespacemlpack_1_1rl_html_a3c2fb6897a13583e67422ad12286f6d1"><div class="ttname"><a href="namespacemlpack_1_1rl.html#a3c2fb6897a13583e67422ad12286f6d1">mlpack::rl::OneStepQLearning</a></div><div class="ttdeci">AsyncLearning&lt; OneStepQLearningWorker&lt; EnvironmentType, NetworkType, UpdaterType, PolicyType &gt;, EnvironmentType, NetworkType, UpdaterType, PolicyType &gt; OneStepQLearning</div><div class="ttdoc">Convenient typedef for async one step q-learning.</div><div class="ttdef"><b>Definition:</b> <a href="async__learning_8hpp_source.html#l00195">async_learning.hpp:197</a></div></div>
</div><!-- fragment --><p>Here, unlike the Q-Learning example, instead of the entire while loop, we use the Train method of the Asynchronous Learning class inside a for loop. 100 training episodes will take around 50 seconds.</p>
<div class="fragment"><div class="line"><span class="keywordflow">for</span> (<span class="keywordtype">int</span> i = 0; i &lt; 100; i++)</div>
<div class="line">{</div>
<div class="line">  agent.Train(measure);</div>
<div class="line">}</div>
</div><!-- fragment --><p>What is "measure" here? It is a lambda function which returns a boolean value (indicating the end of training) and accepts the episode return (total reward of a deterministic test episode) as parameter. So, let's create that.</p>
<div class="fragment"><div class="line">arma::vec returns(20, arma::fill::zeros);</div>
<div class="line"><span class="keywordtype">size_t</span> position = 0;</div>
<div class="line"><span class="keywordtype">size_t</span> episode = 0;</div>
<div class="line"> </div>
<div class="line"><span class="keyword">auto</span> measure = [&amp;returns, &amp;position, &amp;episode](<span class="keywordtype">double</span> episodeReturn)</div>
<div class="line">{</div>
<div class="line">  <span class="keywordflow">if</span>(episode &gt; 10000) <span class="keywordflow">return</span> <span class="keyword">true</span>;</div>
<div class="line"> </div>
<div class="line">  returns[position++] = episodeReturn;</div>
<div class="line">  position = position % returns.n_elem;</div>
<div class="line">  episode++;</div>
<div class="line"> </div>
<div class="line">  std::cout &lt;&lt; <span class="stringliteral">&quot;Episode No.: &quot;</span> &lt;&lt; episode</div>
<div class="line">      &lt;&lt; <span class="stringliteral">&quot;; Episode Return: &quot;</span> &lt;&lt; episodeReturn</div>
<div class="line">      &lt;&lt; <span class="stringliteral">&quot;; Average Return: &quot;</span> &lt;&lt; arma::mean(returns) &lt;&lt; std::endl;</div>
<div class="line">};</div>
</div><!-- fragment --><p>This will train three different agents on three CPU threads asynchronously and use this data to update the action value estimate. Voila, thats all there is to it.</p>
<p>Here is the full code to try this right away:</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="core_8hpp.html">mlpack/core.hpp</a>&gt;</span></div>
<div class="line"><span class="preprocessor">#include &lt;<a class="code" href="ffn_8hpp.html">mlpack/methods/ann/ffn.hpp</a>&gt;</span></div>
<div class="line"><span class="preprocessor">#include &lt;<a class="code" href="gaussian__init_8hpp.html">mlpack/methods/ann/init_rules/gaussian_init.hpp</a>&gt;</span></div>
<div class="line"><span class="preprocessor">#include &lt;<a class="code" href="layer_8hpp.html">mlpack/methods/ann/layer/layer.hpp</a>&gt;</span></div>
<div class="line"><span class="preprocessor">#include &lt;<a class="code" href="mean__squared__error_8hpp.html">mlpack/methods/ann/loss_functions/mean_squared_error.hpp</a>&gt;</span></div>
<div class="line"><span class="preprocessor">#include &lt;<a class="code" href="async__learning_8hpp.html">mlpack/methods/reinforcement_learning/async_learning.hpp</a>&gt;</span></div>
<div class="line"><span class="preprocessor">#include &lt;<a class="code" href="cart__pole_8hpp.html">mlpack/methods/reinforcement_learning/environment/cart_pole.hpp</a>&gt;</span></div>
<div class="line"><span class="preprocessor">#include &lt;<a class="code" href="greedy__policy_8hpp.html">mlpack/methods/reinforcement_learning/policy/greedy_policy.hpp</a>&gt;</span></div>
<div class="line"><span class="preprocessor">#include &lt;<a class="code" href="aggregated__policy_8hpp.html">mlpack/methods/reinforcement_learning/policy/aggregated_policy.hpp</a>&gt;</span></div>
<div class="line"><span class="preprocessor">#include &lt;<a class="code" href="training__config_8hpp.html">mlpack/methods/reinforcement_learning/training_config.hpp</a>&gt;</span></div>
<div class="line"><span class="preprocessor">#include &lt;ensmallen.hpp&gt;</span></div>
<div class="line"> </div>
<div class="line"><span class="keyword">using namespace </span><a class="code" href="namespacemlpack.html">mlpack</a>;</div>
<div class="line"><span class="keyword">using namespace </span><a class="code" href="namespacemlpack_1_1ann.html">mlpack::ann</a>;</div>
<div class="line"><span class="keyword">using namespace </span><a class="code" href="namespacemlpack_1_1rl.html">mlpack::rl</a>;</div>
<div class="line"><span class="keywordtype">int</span> main()</div>
<div class="line">{</div>
<div class="line">  <span class="comment">// Set up the network.</span></div>
<div class="line">  FFN&lt;MeanSquaredError&lt;&gt;, GaussianInitialization&gt; model(MeanSquaredError&lt;&gt;(), GaussianInitialization(0, 0.001));</div>
<div class="line">  model.Add&lt;Linear&lt;&gt;&gt;(4, 128);</div>
<div class="line">  model.Add&lt;ReLULayer&lt;&gt;&gt;();</div>
<div class="line">  model.Add&lt;Linear&lt;&gt;&gt;(128, 128);</div>
<div class="line">  model.Add&lt;ReLULayer&lt;&gt;&gt;();</div>
<div class="line">  model.Add&lt;Linear&lt;&gt;&gt;(128, 2);</div>
<div class="line"> </div>
<div class="line">  AggregatedPolicy&lt;GreedyPolicy&lt;CartPole&gt;&gt; policy({GreedyPolicy&lt;CartPole&gt;(0.7, 5000, 0.1),</div>
<div class="line">                                                   GreedyPolicy&lt;CartPole&gt;(0.7, 5000, 0.01),</div>
<div class="line">                                                   GreedyPolicy&lt;CartPole&gt;(0.7, 5000, 0.5)},</div>
<div class="line">                                                   arma::colvec(<span class="stringliteral">&quot;0.4 0.3 0.3&quot;</span>));</div>
<div class="line"> </div>
<div class="line">  TrainingConfig config;</div>
<div class="line">  config.StepSize() = 0.01;</div>
<div class="line">  config.Discount() = 0.9;</div>
<div class="line">  config.TargetNetworkSyncInterval() = 100;</div>
<div class="line">  config.ExplorationSteps() = 100;</div>
<div class="line">  config.DoubleQLearning() = <span class="keyword">false</span>;</div>
<div class="line">  config.StepLimit() = 200;</div>
<div class="line"> </div>
<div class="line">  <a class="code" href="namespacemlpack_1_1rl.html#a3c2fb6897a13583e67422ad12286f6d1">OneStepQLearning</a>&lt;CartPole, decltype(model), ens::VanillaUpdate, decltype(policy)&gt;</div>
<div class="line">      agent(std::move(config), std::move(model), std::move(policy));</div>
<div class="line"> </div>
<div class="line">  arma::vec returns(20, arma::fill::zeros);</div>
<div class="line">  <span class="keywordtype">size_t</span> position = 0;</div>
<div class="line">  <span class="keywordtype">size_t</span> episode = 0;</div>
<div class="line"> </div>
<div class="line">  <span class="keyword">auto</span> measure = [&amp;returns, &amp;position, &amp;episode](<span class="keywordtype">double</span> episodeReturn)</div>
<div class="line">  {</div>
<div class="line">    <span class="keywordflow">if</span>(episode &gt; 10000) <span class="keywordflow">return</span> <span class="keyword">true</span>;</div>
<div class="line"> </div>
<div class="line">    returns[position++] = episodeReturn;</div>
<div class="line">    position = position % returns.n_elem;</div>
<div class="line">    episode++;</div>
<div class="line"> </div>
<div class="line">    std::cout &lt;&lt; <span class="stringliteral">&quot;Episode No.: &quot;</span> &lt;&lt; episode</div>
<div class="line">        &lt;&lt; <span class="stringliteral">&quot;; Episode Return: &quot;</span> &lt;&lt; episodeReturn</div>
<div class="line">        &lt;&lt; <span class="stringliteral">&quot;; Average Return: &quot;</span> &lt;&lt; arma::mean(returns) &lt;&lt; std::endl;</div>
<div class="line">  };</div>
<div class="line"> </div>
<div class="line">  <span class="keywordflow">for</span> (<span class="keywordtype">int</span> i = 0; i &lt; 100; i++)</div>
<div class="line">  {</div>
<div class="line">    agent.Train(measure);</div>
<div class="line">  }</div>
<div class="line">}</div>
</div><!-- fragment --><h1><a class="anchor" id="further_rltut"></a>
Further documentation</h1>
<p>For further documentation on the rl classes, consult the <a class="el" href="namespacemlpack_1_1rl.html">complete API documentation</a>. </p>
</div></div><!-- contents -->
</div><!-- PageDoc -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.9.1
</small></address>
</body>
<script type="text/javascript">
var x = document.querySelectorAll("img.formulaDsp");
var i;
for (i = 0; i < x.length; i++)
{
  x[i].width = x[i].offsetWidth / 4;
}
</script>
</html>
